	Expansion of polyglutamine tracts in proteins results in protein aggregation and is associated with cell death in at least nine neurodegenerative diseases.
	Disease age of onset is correlated with the polyQ insert length above a critical value of 35 40 glutamines.
	The aggregation kinetics of isolated polyQ peptides in vitro also shows a similar critical-length dependence.
	While recent experimental work has provided considerable insights into polyQ aggregation, the molecular mechanism of aggregation is not well understood.
	Here, using computer simulations of isolated polyQ peptides, we show that a mechanism of aggregation is the conformational transition in a single polyQ peptide chain from random coil to a parallel -helix.
	This transition occurs selectively in peptides longer than 37 glutamines.
	In the -helices observed in simulations, all residues adopt -strand backbone dihedral angles, and the polypeptide chain coils around a central helical axis with 18.5 2 residues per turn.
	We also find that mutant polyQ peptides with proline-glycine inserts show formation of antiparallel -hairpins in their ground state, in agreement with experiments.
	The lower stability of mutant -helices explains their lower aggregation rates compared to wild type.
	Our results provide a molecular mechanism for polyQ-mediated aggregation.
	The appearance of polyglutamine -containing aggregates CITATION CITATION is a hallmark of disease progression in all diseases in which CAG-expansions occur in genes CITATION.
	Intranuclear inclusion bodies containing polyQ aggregates have been found in vitro CITATION, CITATION, in cell cultures, animal models, and affected patients CITATION, CITATION.
	The aggregates are known to have a characteristic amyloid topology CITATION.
	The inhibition of oligomerization by the azo-dye Congo red, or by the Hsp70/Hsp40 chaperone system, exerts marked protective effects in vivo and in vitro CITATION, CITATION.
	Aggregation and disease are observed if the number of glutamines in the expansion, n, exceeds a critical value, n C CITATION.
	The nearly universal existence of this criticality in all polyQ-related diseases suggests that when the polyQ insert length exceeds a critical value, a pathological change, largely independent of the host protein, occurs in the polyQ insert itself.
	Therefore, isolated polyQ peptides have been used as model systems for studying polyQ aggregation CITATION, CITATION, CITATION, and it is known that: The nuclear uptake of polyQ peptide aggregates prepared in vitro is cytotoxic in cell cultures CITATION, isolated polyQ peptides have in vitro aggregation properties similar to the corresponding full-length proteins containing the polyQ insert CITATION, CITATION, peptide aggregation follows a nucleated mechanism showing characteristic lag and growth phases CITATION, CITATION, and the glutamine tract-length dependence of the lag-time interval correlates well with the age of onset of disease CITATION.
	Peptides of subcritical lengths have long lag times of aggregation and a corresponding age of onset later than the typical life span of a person.
	Longer peptides have progressively smaller lag times of aggregation, and a correspondingly early age of onset of the disease CITATION .
	Unaggregated polyQ peptides form random coil structures, whereas aggregates are composed of amyloid-like -strands CITATION.
	The conversion of random coil to -strand occurs in an individual polyQ chain CITATION, and fibril formation occurs by addition of other polyQ chains to these monomeric -strand nuclei.
	Therefore, the conformational dynamics of an individual polyQ chain determines both its aggregation mechanism and the structure of the final aggregates.
	The details of the conformational dynamics of polyQ and the length dependence of the dynamics are not well understood CITATION .### abstract ###
 when judging their likelihood of success in competitive tasks, people tend to be overoptimistic for easy tasks and overpessimistic for hard tasks the shared circumstance effect; sce
 previous research has shown that feedback and experience from repeated-play competitions has a limited impact on sces
 however, in this paper, we suggest that competitive situations, in which the shared difficulty or easiness of the task is more transparent, will be more amenable to debiasing via repeated play
 pairs of participants competed in, made predictions about, and received feedback on, multiple rounds of a throwing task involving both easy- and hard-to-aim objects
 participants initially showed robust sces, but they also showed a significant reduction in bias after only one round of feedback
 these and other results support a more positive view than suggested from past research on the potential for sces to be debiased through outcome feedback
 competition abounds in everyday life, where we contend with others for top grades, jobs, trophies, and mates
 when resources are at a premium, it is optimal to enter into competitive environments in which we are certain to fare well and to avoid those in which we are doomed to fail
 however, when people evaluate their likelihood of success in competitions, they are subject to a robust bias: a competitor should consider the strengths and weaknesses of the self and the other competitors  CITATION , but people often give too much weight to evidence related to their own strengths and weaknesses and too little weight to such evidence about the competitor  CITATION
 this egocentrism results in overoptimism when the circumstances of the competition are favorable, such as when competitors in a trivia game learn that the questions will be from an easy category-even though they'll be easy for everyone  CITATION
 egocentrism also results in overpessimism when the circumstances are unfavorable e g , a difficult trivia category
 this phenomenon of being more optimistic when shared competitive circumstances are favorable than when they are not has been dubbed the shared-circumstance effect  CITATION , and it has been replicated across a variety of settings e g , general knowledge tasks, card games, athletic competitions
 in most previous studies on the sce, participants were presented with novel, non-repeated competitive situations
 these situations did not allow people to learn from past experiences or from feedback within the immediate competitive context
 however, in everyday contexts there are often opportunities to learn how a shared circumstance tends to affect the self, others, and outcomes
 for example, when a tennis tournament is played during a string of windy days, players can have several opportunities to observe how the weather affects themselves and their competitors
 to examine whether egocentrism and sces persist in repeated-play contexts, rose and windschitl  CITATION  had pairs of participants compete against each other in multiple rounds of a trivia contest that involved easy and hard categories
 in each round, participants estimated their likelihood of beating their competitor, answered trivia questions, and received feedback about who won
 in initial rounds, there were robust sces; participants were much more optimistic about winning easy categories than hard ones
 if they encountered the same hard and easy categories across rounds, the participants learned from feedback
 that is, the sce shrank-but slowly-across six rounds with the same categories
 the sce was never eliminated, even after six rounds
 also, for a seventh round, participants were told there would be new categories
 the sce for that round dramatically and fully rebounded; it was every bit as large as observed for round 1
 these results provide a bleak view of how well people can learn from feedback and avoid sces
 moreover, results from a study by moore and cain  CITATION , which also used repeated plays with feedback, suggest an even bleaker view
 those researchers also used easy and difficult quizzes as shared-circumstance manipulations, but found virtually no reduction in sces after numerous rounds with feedback
 CITATION are people's abilities to learn to avoid sces-based on feedback-really as bleak as this prior research might suggest
 we argue that some shared circumstances are more transparently shared than others, and this may affect how readily people learn to avoid the bias that creates sces-and how easily they can transfer this learning to a slightly new set of shared circumstances
 by transparently shared, we are referring to how obvious it is that a circumstance that is helpful or hindering to the self will affect others in largely the same way
 in the present study, we examined the influence of repeated feedback on sces
 however, unlike past research, we used a competition in which the difficulty of the shared circumstance relative to competitions, for example, involving easy and hard trivia categories is more transparently shared
 in a multi-round paradigm, participants competed in object-tossing competitions
 in each round, two competitors each had 8 throws per object-attempting to land the object inside a target area
 there was always one easy-to-aim object e g , a beanbag and one hard-to-aim object e g , a paper plate, which constituted our shared-circumstance manipulation
 full feedback was given during and after each round, and predictions were solicited before each round and also before a final round with novel objects
 we suspected that a tossing competition, rather than a trivia competition, would produce less bleak results about the debiasing of sces through repeated play
 in the case of trivia, watching one's competitor fail to answer trivia questions doesn't give any insight about why the category is difficult for that person
 also, knowing that one's competitor struggled on a difficult category does not provide obvious information about why he or she might struggle on another difficult category
 however, in the case of throwing, watching a competitor fail when tossing an object probably illuminates the relevance of specific shared, situational circumstances i e , the properties of the specific objects as well as a more general awareness of the relevance that object properties have on throwing success-for anyone
 for example, seeing a paper plate fly unpredictably will likely give an observer a clear impression that the object will fly unpredictably regardless of who is throwing it
 for participants, this enhances the appreciation that one's struggles are not primarily due to personal characteristics but to properties of the tossed objects; this would then be useful in mitigating egocentrism and sces, even when new objects are introduced
 consequently, we expected that, even though participants might reveal a robust sce at round 1 prior to any feedback or observations regarding their competitor, they would show a pronounced learning effect after the feedback and observations of round 1
 that is, they would show significantly reduced sces starting immediately after round 1
 we also expected that this learning would be generally transferable
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm
 In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties
 However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed
 In many machine learning applications, however, this assumption does not hold
 The observations received by the learning algorithm often have some inherent temporal dependence
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid  processes that implies a dependence between observations weakening over time
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-iid scenarios
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms
 These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-iid scenarios
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, such as the VC-dimension, covering numbers, or Rademacher complexity
 These measures characterize a class of hypotheses, independently of any algorithm
 In contrast, the notion of algorithmic stability can be used to derive bounds that are tailored to specific learning algorithms and exploit their particular properties
 A learning algorithm is stable if the hypothesis it outputs varies in a limited way in response to small changes made to the training set
 Algorithmic stability has been used effectively in the past to derive tight generalization bounds  CITATION
 But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (iid)
 In many machine learning applications, this assumption, however, does not hold; in fact, the iid assumption is not tested or derived from any data analysis
 The observations received by the learning algorithm often have some inherent temporal dependence
 This is clear in system diagnosis or time series prediction problems
 Clearly, prices of different stocks on the same day, or of the same stock on different days, may be dependent
 But, a less apparent time dependency may affect data sampled in many other tasks as well
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid processes that implies a dependence between observations weakening over time  CITATION
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the usefulness of stability-bounds to non-iid scenarios
 Our proofs are based on the independent block technique described by  CITATION  and attributed to  CITATION , which is commonly used in such contexts
 However, our analysis differs from previous uses of this technique in that the blocks of points considered are not of equal size
 For our analysis of stationary  SYMBOL -mixing sequences, we make use of a generalized version of McDiarmid's inequality  CITATION  that holds for  SYMBOL -mixing sequences
 This leads to stability-based generalization bounds with the standard exponential form
 Our generalization bounds for stationary  SYMBOL -mixing sequences cover a more general non-iid scenario and use the standard McDiarmid's inequality, however, unlike the  SYMBOL -mixing case, the  SYMBOL -mixing bound presented here is not a purely exponential bound and contains an additive term depending on the mixing coefficient
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression (SVR)  CITATION , Kernel Ridge Regression  CITATION , and Support Vector Machines (SVMs)  CITATION
 Algorithms such as support vector regression (SVR)  CITATION  have been used in the context of time series prediction in which the iid assumption does not hold, some with good experimental results  CITATION
 To our knowledge, the use of these algorithms in non-iid scenarios has not been previously supported by any theoretical analysis
 The stability bounds we give for SVR, SVMs, and many other kernel regularization-based and relative entropy-based regularization algorithms can thus be viewed as the first theoretical basis for their use in such scenarios
 The following sections are organized as follows
 In Section~, we introduce the necessary definitions for the non-iid problems that we are considering and discuss the learning scenarios in that context
 Section~ gives our main generalization bounds for stationary  SYMBOL -mixing sequences based on stability, as well as the illustration of its applications to general kernel regularization-based algorithms, including SVR, KRR, and SVMs, as well as to relative entropy-based regularization algorithms
 Finally, Section~ presents the first known stability bounds for the more general stationary  SYMBOL -mixing scenario ### abstract ###
	we test in the context of a dictator game the proposition that individuals may experience a self-control conflict between the temptation to act selfishly and the better judgment to act pro-socially
	we manipulated the likelihood that individuals would identify self-control conflict, and we measured their trait ability to implement self-control strategies
	our analysis reveals a positive and significant correlation between trait self-control and pro-social behavior in the treatment where we expected a relatively high likelihood of conflict identification-but not in the treatment where we expected a low likelihood
	the magnitude of the effect is of economic significance
	we conclude that subtle cues might prove sufficient to alter individuals' perception of allocation opportunities, thereby prompting individuals to draw on their own cognitive resources to act pro-socially
	lured by temptation, individuals may find themselves acting against their better judgment
	self-control failure, famously termed akrasia in plato's protagoras  CITATION , persists throughout domains of daily life and represents a central issue of both philosophy and modern-day social sciences
	for example, the dieter faced with the opportunity to indulge in a delicious creamy cake may perceive a conflict between indulging and maintaining a good figure
	the student may feel conflicted between the desire to go to the cinema and her better judgment to stay home and study
	and, similarly, the fashionista might feel conflicted between the temptation to purchase new boots and her better judgment to maintain a responsible budget
	perhaps less intuitively, but no less importantly, the question of pro-social versus selfish behavior may be understood in similar terms
	this conceptualization may help reconcile conflicting notions in economics of selfish and pro-social motivations
	that individuals should care much about their own self-interest seems almost tautological and requires little further exposition, but that individuals also should care about the interest of others-at the expense of that of their own-has attracted considerable attention  CITATION
	for example, many individuals voluntarily contribute to charity or to public goods e g , recycling, and they pay their taxes despite low likelihood of punishment for failing to do so
	nonetheless, one could imagine that even individuals of generally pro-social inclination on occasion may feel tempted to act selfishly and hence underreport income to the authorities
	that is, pro-social preferences potentially fly in the face of basic urges for personal gain-or greed-and the individual may thus experience a self-control conflict between better judgment to act pro-socially and the temptation to act selfishly
	self-control-our capacity to overrule temptation-is no less complex than it is important
	a multitude of conceptualizations exist, many of which are complementary
	typically, and in line with classic ideas of the conflict between reason and passion, authors view self-control as a "cold" executive function that guides behavior in the face of "hot" impulses to act against better judgment  CITATION
	willpower, then, represents the combined resources that the executive function-or the planner, in the parlance of thaler and shefrin  CITATION -brings to bear in a deliberate struggle against temptation  CITATION
	such resources may include cognitive strategies to divert attention away from temptation  CITATION , strategies of pre-commitment  CITATION , or possibly the sheer strength of mind to hold back from the song of the sirens
	our conceptualization of self-control mirrors these
	only recently has the psychological literature started to explore how the question of pro-social versus selfish behavior relates to that of self-control
	loewenstein  CITATION  suggests that selfish behavior may be motivated by visceral urges or drive-states, resembling cravings for relief from hunger, pain, and sexual deprivation
	o'donoghue and loewenstein  CITATION  argue that such selfish urges may conflict with the "colder", more abstract preferences for altruism, as visceral urges for sweets may conflict with more abstract preferences for a fine figure or good health
	at present, there is but indirect evidence for this idea
	for example, pronin et al CITATION  show that decisions about others resemble decisions about "future selves", both classes of which contrast to decisions about less abstract "present selves"
	albrecht et al CITATION  report consistent results; individuals who choose between immediate and delayed rewards for themselves exhibit less patience and more affective involvement activation in the dopaminergic reward system than do individuals who make such choices for others-or for themselves in the future
	moreover, curry et al CITATION  find in a standard public goods game that individuals' discount rates are negatively associated with their contributions to the public good
	that is, more "impatient" individuals contributed less to the public good than did "patient" ones
	arriving at similar results, fehr and leibbrandt  CITATION  report that patient vs impatient fishermen, whose time preferences were elicited in the lab, exhibited more cooperative behavior in a common resource problem and were in the field less likely to over-exploit the common pool resource
	furthermore, burks et al CITATION  find that "short-term" patience-the  beta   in the  beta  -&#x3b4;  model-is positively associated with cooperative behavior in a sequential prisoner's dilemma
	however, duffy and smith  CITATION  report no effect of cognitive load-a manipulation intended to deplete cognitive resources and thereby impair self-control-on outcomes across treatments in a repeated multi-player prisoner's dilemma
	an emerging literature on the "default" response in games of trust and reciprocity lends further credence to the notion that altruistic responses require self-control
	achtziger et al CITATION  subjected players in an ultimatum game to cognitive resource depletion, and show that depleted proposers made lower offers-they became less altruistic
	moreover, depleted responders were more likely to reject offers that were unfair to themselves-they exhibited "altruistic punishment"
	halali et al CITATION  report the same for responders, but with a different depletion task
	crockett et al CITATION  subjected responders to acute tryptophan depletion-a procedure that temporarily reduces serotonin levels in the brain and thereby impairs self-control  CITATION ; reduced serotonin levels raised rejection rates and this reduction is positively correlated with impulsive choice in a delay-discounting task  CITATION
	using a trust game, knoch et al CITATION  subjected trustees' right lateral prefrontal cortex to transcranial magnetic stimulation, which reduces functioning in the targeted brain region
	trustees, though cognizant that returning a share of the investments was both strategic and norm-compliant, were unable to do so under impaired executive functioning; self-control seems necessary to act on the better judgment to resist the temptation to keep the received investment entirely for oneself
	closest to our domain of inquiry, piovesan and wengstrom  CITATION  measured response times of participants in a repeated dictator game, lasting 24 periods
	they find both across and within participants that lower response times are associated with more selfish choices
	one interpretation of their results is that the default behavior is to act selfishly and that pro-social behavior requires the successful resolution of a self-control conflict, which slows the response time
	such successful resolution of conflict would require cognitive resources, but hauge et al CITATION  report no effect of cognitive load on players in one-shot dictator games
	in this paper we attempt a more direct test of the hypothesis that pro-social versus selfish behavior may represent a self-control problem
	we employ a standard measure of pure pro-social behavior, the one-shot dictator game, which invokes neither concerns for strategy nor for reciprocity; and a well-grounded psychometric measure of self-control, the rosenbaum self-control schedule  CITATION
	further, we explore the conditions under which we expect an association between self-control and pro-social behavior
	in so doing, we rely on two conditions necessary for successfully exercising restraint in the face of temptation; myrseth and fishbach  CITATION  propose a two-stage model of self-control, which postulates that an individual in the face of temptation first identifies conflict or not between indulging and pursuing a higher-order goal and, second, that the individual next employs self-control strategies if and only if conflict was identified at the first stage see figure 1
	critically, self-control strategies are relevant to the decision to indulge only when the individual has identified self-control conflict
	therefore, one strategy for investigating whether the problem of pro-social versus selfish behavior resembles one of self-control is to test whether the tendency to apply self-control strategies is positively associated with pro-social behavior when individuals have identified self-control conflict, but less so or not at all when individuals have not
	determinants of conflict identification in the face of temptation have been explored only recently
	in some contexts, the question is almost trivial and identification of conflict virtually obvious
	for example, the diabetic dieter probably knows that having even a single, tempting chocolate may incur major costs
	however, the question of self-control conflict is more ambiguous for the non-diabetic dieter, who faces the same chocolate
	having this one chocolate alone will not incur major costs, but doing so regularly might
	similarly, the good citizen may find that a general failure to act generously would represent a major threat to his self-image, but being stingy on just a couple of occasions is a more ambiguous matter
	myrseth and fishbach  CITATION  use the term epsilon cost temptation to denote tempting opportunities that incur nothing but trivial costs when consumed in small amounts, but potentially serious costs when consumed extensively
	they argue that individuals identify self-control conflict in the face of epsilon cost temptation if and only if two conditions are met: a the focal consumption opportunity must be viewed in relation to multiple additional opportunities, and b the decision maker must assume that similar choices are made for each opportunity  CITATION
	that is, considering the question of whether or not to have a delicious creamy cake will evoke self-control conflict in the dieter if the dining opportunity is viewed in relation to future opportunities for dessert consumption, but not if the dining opportunity is viewed in isolation, as a singular episode
	similarly, the question of whether or not to be generous-to donate to a charitable organization-may elicit self-control conflict if the decision is viewed in relation to future decisions, but not if the decision is viewed in isolation
	if viewed in relation to future decisions, the question of how much to donate on a single occasion may have bearing on the decision maker's self-image; donating now-and in the future-indicates a generous character, whereas keeping the money for oneself does not
	however, if viewed in isolation, the question of how much to donate has little bearing on self-image; the present decision of how much to donate is considered only in light of immediate consequences, leaving self-image out of the equation
	because a consistent self-image represents an important motivator for pro-social behavior  CITATION , we expect that individuals are more likely to identify self-control conflict between selfish and pro-social behavior if the allocation decision is seen in relation to future opportunities than if it is seen in isolation
	myrseth and fishbach  CITATION  show that subtle framing manipulations are sufficient to influence identification of self-control conflict in the face of epsilon cost temptation
	they find that presenting a calendar displaying the current month, with a grid separating the dates, raised participants' subsequent consumption of potato chips relative to that of participants whom were presented a calendar without a grid
	they argue that the gridded calendar activated an isolated versus interrelated frame of the choice opportunity; it made participants more likely to isolate the date in question and thus less likely to see the decision task in relation to similar future opportunities
	consequently, the grid reduced the likelihood that participants would identify a conflict between the temptation to have chips and the better judgment to maintain a fine figure and good health
	indeed, participants who viewed the gridded calendar reported experiencing less conflict during their decision to have chips or not than did those who viewed the non-gridded calendar
	furthermore, participants' trait ability to implement self-control strategies, measured by rosenbaum's  CITATION  psychometric scale, was positively associated with chips consumption for those who viewed the calendar without the grid and who were more likely to identify conflict, but not for others who viewed the calendar with and who were less likely to identify conflict
	that is, participants who viewed the calendar without the grid, more likely than those who viewed the calendar with, identified self-control conflict and therefore leveraged their self-control strategies to resist the tempting chips
	to explore our hypothesis that the problem of pro-social versus selfish behavior may represent one of self-control, we have applied the empirical strategy from myrseth and fishbach  CITATION  in the dictator game-a participant is granted an endowment and asked to split it between herself and a recipient  CITATION , and in our case the red cross featured as recipient  CITATION
	the game thus pits pro-social motivations against self-interest
	if pro-social versus selfish behavior could represent a self-control conflict, we would expect participants' trait self-control, as measured by rosenbaum's  CITATION  scale, to correlate positively with pro-social behavior for participants who have just previously viewed a calendar without a grid, but less so or not at all for participants who have viewed a calendar with
	the graph in figure 2 displays donation, as a function of level of self-control, for two different levels of identification likelihood
	in the case of low likelihood, the slope is expected to be weakly positive
	in the case of the higher likelihood, however, the slope is expected to be strictly greater than that in the case of low likelihood
	this means that for a given level of self-control, one might observe substantially different donation behavior depending on whether conflict was identified or not### abstract ###
	Human Immunodeficiency Virus 1 uses for entry into host cells a receptor and one of two co-receptors.
	Recently, a new class of antiretroviral drugs has entered clinical practice that specifically bind to the co-receptor CCR5, and thus inhibit virus entry.
	Accurate prediction of the co-receptor used by the virus in the patient is important as it allows for personalized selection of effective drugs and prognosis of disease progression.
	We have investigated whether it is possible to predict co-receptor usage accurately by analyzing the amino acid sequence of the main determinant of co-receptor usage, i.e., the third variable loop V3 of the gp120 protein.
	We developed a two-level machine learning approach that in the first level considers two different properties important for protein-protein binding derived from structural models of V3 and V3 sequences.
	The second level combines the two predictions of the first level.
	The two-level method predicts usage of CXCR4 co-receptor for new V3 sequences within seconds, with an area under the ROC curve of 0.937 0.004.
	Moreover, it is relatively robust against insertions and deletions, which frequently occur in V3.
	The approach could help clinicians to find optimal personalized treatments, and it offers new insights into the molecular basis of co-receptor usage.
	For instance, it quantifies the importance for co-receptor usage of a pocket that probably is responsible for binding sulfated tyrosine.
	Specific protein interactions are central to biological processes, and the infection of cells with viruses is no exception there.
	In the case of pathogenic viruses, such protein interactions are potential targets for medical intervention.
	An example of particularly high relevance is Human Immunodeficiency Virus 1.
	HIV-1 enters human cells in a process that comprises several steps, including the binding of the viral gp120 protein to the cellular receptor protein CD4 and a co-receptor protein, usually one of the two chemokine receptors CCR5 and CXCR4 CITATION.
	The type of co-receptor used by the virus, the so-called co-receptor tropism, has a prognostic value, since patients with a CXCR4-tropic virus progress faster to Acquired Immunodeficiency Syndrome compared to patients with a CCR5-tropic virus CITATION.
	In addition to the purely X4- and R5-tropic viruses, there are also dual-tropic strains, able to use both co-receptors.
	Recently, the first drug that binds to CCR5, and thus inhibits productive binding of gp120, has been approved by regulatory authorities in several countries.
	This has made the determination of co-receptor tropism directly relevant to anti-retroviral treatment, as CCR5-inhibitors are of course inactive against X4 virus.
	The standard way of determining co-receptor tropism is by cell-based assays CITATION, CITATION.
	The main drawbacks of these assays are that they are currently only carried out by a handful of specialized laboratories worldwide, and that the overall procedure typically takes several weeks.
	These impediments to the wide application of entry inhibitors could be overcome by an approach similar to genotypic drug resistance testing CITATION, where drug resistance of a viral strain is inferred from comparison of mutational patterns obtained from sequencing parts of the genome of that strain with patterns of validated resistance mutations.
	This is a relatively fast and cheap standard procedure established in many clinics.
	At first glance, genotypic testing for co-receptor tropism seems to be possible since the main molecular determinant of tropism is known to be the third variable loop of the viral glycoprotein gp120 CITATION, a peptide stretch of about 35 amino-acids with a disulfide bridge connecting the terminal cysteins.
	Unfortunately, as suggested by its name, V3 is notorious for its high sequence variability CITATION including also some variability in length, and this has made it difficult to use it as a basis for genotypic co-receptor tropism testing.
	Nevertheless, the relevance of the quest has prompted many groups to develop models that link properties of V3 to co-receptor tropism.
	The importance of electrostatics for co-receptor tropism has been recognized early on, and the best-known model, the so-called 11/25-rule, refers to charges of V3-residues 11 and 25: if one of these is positive, then the virus is CXCR4-tropic CITATION, CITATION.
	This rule has a specificity of more than 0.9, but only a low to moderate sensitivity of about 0.4 0.6, depending on the test data, which is not satisfactory for routine clinical application.
	To improve predictions from sequence, several groups have applied machine learning methods, such as artificial neural networks CITATION, position specific scoring matrices CITATION, decision trees, or support vector machines CITATION.
	Still, prediction accuracies fall short of what seems reasonable for regular clinical use CITATION.
	It is unclear whether the limited accuracies are the footprint of tropism-determinants outside V3, or the consequence of model imperfections.
	A milestone for the understanding of co-receptor tropism was the X-ray structure of gp120 with the V3 loop in a biological context CITATION.
	This paved the way for the development of prediction methods that use, in addition to V3 sequence, structural information.
	To our knowledge, the first of these methods has been that of Sander et al. CITATION, which was mainly based on geometric distances of amino-acid pairs within the structure of V3.
	Although our method, detailed in the following, relies on the same experimental structure by Huang et al. CITATION, it differs from that of Sander et al. in several respects, e.g. it deals with indels, and, perhaps most crucially, it uses as descriptors properties that directly determine interaction of V3 with the co-receptors.
	By the latter we consider a seemingly trivial but fundamental fact that so far has not been thoroughly exploited: although V3 is highly variable, all X4-tropic V3 loops share one property, namely, they preferentially have a physical binding interaction with CXCR4, while R5-tropic V3 loops preferably interacts with CCR5.
	The accuracy of the method makes it attractive as clinical tool for patient tailored decisions on treatment with entry inhibitors, and it suggests that co-receptor tropism can be explained almost exclusively based on V3.### abstract ###
 third-party punishment has recently received attention as an explanation for human altruism
 feelings of anger in response to norm violations are assumed to motivate third-party sanctions, yet there is only sparse and indirect support for this idea
 we investigated the impact of both anger and guilt feelings on third-party sanctions
 in two studies both emotions were independently manipulated
 results show that anger and guilt independently constitute sufficient but not necessary causes of punishment
 low levels of punishment are observed only when neither emotion is elicited
 we discuss the implications of these findings for the functions of altruistic sanctions
 people often defend the interests of others
 they stand up for their friends if someone speaks ill about them in their absence
 they do not tolerate a colleague being bullied at work
 they boycott consumer products that are produced using child labor
 some even come to the aid of a stranger who is being physically harassed, in spite of obvious personal danger
 in general, people retaliate against injustice even if they are not directly victimized
 sanctioning of norm-violations is vital for prosocial behavior to be sustained  CITATION
 however, punishing norm-violations is costly in terms of time and energy
 it may even impose physical risks
 punishing injustice is therefore considered to be a moral act, particularly when it is performed on behalf of others  CITATION
 this begs the question of what incites third-party sanctions, as they usually oppose self-interest### abstract ###
 similar to research on risky choice, the traditional analysis of intertemporal choice takes the view that an individual behaves so as to maximize the discounted sum of all future utilities
 the well-known allais paradox contradicts the fundamental postulates of maximizing the expected value or utility of a risky option
 we describe a violation of the law of diminishing marginal utility as well as an intertemporal version of the allais paradox
 in the field of intertemporal choice, the discounted-utility du theory proposed by paul samuelson in 1937 was presented not only as a valid normative standard but also as a descriptive theory of actual intertemporal choice behavior  CITATION
 in its general form, the du theory proposes that the value of an option, x; t, is the product of its present utility, ux, and an exponential temporal discounting function, ft, where t is the time at which x is acquired
 the overall value of a mixed option, a = {x, t, x, t, }, denoted va, is simply the sum of these products
 that is, va = sigma  ux ft
 an option a will be preferred to an option b if and only if va  greater than  vb
 however, a large body of empirical evidence demonstrates that people systematically violate this theory
 this includes the common difference effect, the magnitude effect, the gain-loss asymmetry, the delay-speedup asymmetry, and so on  CITATION
 this situation has led researchers to consider extensions and modifications of the du theory to reconcile it with the experimental data
 the most prominent idea to account for these anomalies is the hyperbolic discounting model  CITATION
 this model suggests that the discount rate is not dynamically consistent but that the rate is higher between the present and near future and lower between the near and far distant future
 numerous theories have been developed by transforming the discount function to other forms, from one-parameter hyperbolic discounting  CITATION  to generalized hyperbolic discounting  CITATION , to proportional discounting  CITATION , and to quasi-hyperbolic discounting  CITATION
 however, these models focus on intertemporal choice between pairs of single-dated outcomes represented as pure gains or losses
 when these models are applied to intertemporal choice between pairs of multiple-dated outcomes in mixed contexts, there is general agreement on the additive assumption and the independence assumption
 with an apt transformation of the discounting rate, the additive assumption means that preferences for outcome sequences are based on a simple aggregation of their individual components within intertemporal choice  CITATION
 the independence assumption means that the value or utility of an outcome in one period is independent of outcomes in other periods  CITATION
 because risk and delay might be psychologically equivalent, or at least analogous, and because similar psychological processes might underlie risk and intertemporal choice  CITATION , theoretical development in intertemporal choice has progressed steadily along a similar route as that of risky choice  CITATION
 both lines of research have spawned a large number of variant models
 although the functional forms differ, most theories assume a maximization principle; that is, people calculate the mathematical expectation of each outcome and add them together before choosing the option that maximizes overall value or utility
 a minor difference is that the existing models of intertemporal choice are relatively underdeveloped and are less flexible in dealing with empirical challenges
 for example, research on risky decision making does not treat risky choice as limited to pure gains or pure losses but has been extended to include mixed outcomes involving both gains and losses
 examples include the sign-dependent utility model  CITATION , the rank- and sign-dependent utility model  CITATION , and the transfer of attention exchange model  CITATION
 the well-known allais paradox  CITATION  contradicts the fundamental postulates of maximizing the expected utility of a risky option
 the paradox presents a violation of the cancellation axiom, which asserts that, if two options have a common consequence under a particular event, the preference order of the options should be independent of the value of that consequence  CITATION
 since then, many new descriptive theories of risky choice have abandoned the maximization assumption  CITATION
 most models of intertemporal choice have not yet abandoned the additive assumption and the independence assumption
 these two assumptions would lead to the cancellation axiom, which indicates that a preference between two sequences with elements in common does not depend on the nature of the common elements
 table 1 illustrates an example of the multiple-dated outcomes problem, which would be used to test the cancellation axiom
 in problem i, the additive models predict that adding a common element x at time 2 to both option a and option b would not change the preference orderings
 the violation of cancellation would be observed if the preference orderings were different between problem i and problem i
 however, if allais's proposition applies to intertemporal choice, we will eventually encounter an intertemporal version of the allais paradox
 we first illustrate our point with a paradox that is an intertemporal-type violation of the cancellation axiom### abstract ###
 In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model
 Using the exactness of the averages for belief propagation for Gaussian models, a  different way of obtaining the covariances is found,  based on Belief Propagation on cavity graphs
 We discuss the relation of this  loop correction algorithm to Expectation Propagation  algorithms for the case in which the model is no longer  Gaussian, but slightly perturbed by nonlinear terms
 Message passing techniques in graphical models allow for the computation of (approximate)  marginal probabilities in a time interval scaling polynomially in the  model size
 Their discovery has consequently revolutionized several  fields of applications in the past years, of which error correcting codes and vision are probably the most prominent examples
 In many cases, the corresponding graphs are loopy, implying either that the error resulting from the application of loopy belief propagation (BP) is negligible for the particular model, or it  can be tolerated for the particular purpose BP serves
 In other cases  more sophisticated refinements of BP are necessary, taking into account (part of) the loop errors
 Finding the optimal treatment of these ``loop errors''  motivates an active field of research, in which  different solutions applying to different model classes are developed
 For models involving many short loops,  like on regular lattices, CVM type approaches work well  CITATION , or tree EP approaches  CITATION
 The latter may also be  applied to correct for an incidental large loop
 Unifying frameworks like the Region graphs of  CITATION   lead to general strategies for selecting the basic clusters underlying such approaches for general model classes
 A recent analysis has shown that the local update equations of BP may be interpreted as the zero order term of an expansion in ``cavity connected correlations''
 These quantities are parameterizations of the ``cavity distributions'',  i e , the  distribution over neighbor variables of a central variable which has been removed from the graph
 The Bethe approximation and BP are recovered when this  cavity distribution is assumed to factorize, whereas the first order correction to the local update equations is obtained when one takes into account the pair cumulants  CITATION
 Estimation of these pair cumulants is possible with extra runs of BP, allowing for new polynomial time algorithms, reducing errors to order  SYMBOL   when applying algorithms of which running time scales with an extra factor  of  SYMBOL   CITATION
 Although this scaling seems heavy, the large benefit of the approach is that it does not require selection of basic clusters or underlying  tree-structures, since it takes into account the effect of all loops that contribute to nontrivial correlations in the cavity distribution at once
 The above ``loop correction''  strategy is applicable in the class of models where a perturbative expansion around the Bethe approximation makes sense, i e , in models with large loops and relatively weak interactions
 The principal requirement  is that the magnitude of pair variable cumulants of cavity distributions is an order smaller than the  single variable cumulants, and third order cumulants are even smaller, etc
 However, heuristics based on the strategy allow for other good algorithms  performing well outside these parameter regimes  CITATION
 So far the approach has been developed for discrete variable models on a more abstract  CITATION  versus practical level  CITATION
 In this paper we apply the idea to graphical models for continuous variables
 We derive the loop corrected belief propagation equations  for simple tractable Gaussian models,  yielding a message passing scheme that, besides the correct average marginals, also yields the correct variances
 Besides that we discuss some approaches potentially  applicable to cases in which extra function approximations are necessary,  and the relation with expectation propagation
 A by-product of our loop corrected belief propagation equations is an algorithm that calculates exact covariance matrices for Gaussian models like the one discussed in  CITATION , but without explicitly using linear response
 Neuronal activity is mediated through changes in the probability of stochastic transitions between open and closed states of ion channels.
 While differences in morphology define neuronal cell types and may underlie neurological disorders, very little is known about influences of stochastic ion channel gating in neurons with complex morphology.
 We introduce and validate new computational tools that enable efficient generation and simulation of models containing stochastic ion channels distributed across dendritic and axonal membranes.
 Comparison of five morphologically distinct neuronal cell types reveals that when all simulated neurons contain identical densities of stochastic ion channels, the amplitude of stochastic membrane potential fluctuations differs between cell types and depends on sub-cellular location.
 For typical neurons, the amplitude of membrane potential fluctuations depends on channel kinetics as well as open probability.
 Using a detailed model of a hippocampal CA1 pyramidal neuron, we show that when intrinsic ion channels gate stochastically, the probability of initiation of dendritic or somatic spikes by dendritic synaptic input varies continuously between zero and one, whereas when ion channels gate deterministically, the probability is either zero or one.
 At physiological firing rates, stochastic gating of dendritic ion channels almost completely accounts for probabilistic somatic and dendritic spikes generated by the fully stochastic model.
 These results suggest that the consequences of stochastic ion channel gating differ globally between neuronal cell-types and locally between neuronal compartments.
 Whereas dendritic neurons are often assumed to behave deterministically, our simulations suggest that a direct consequence of stochastic gating of intrinsic ion channels is that spike output may instead be a probabilistic function of patterns of synaptic input to dendrites.
 The appropriate level of physical detail required to understand how complex processes such as cognition and behavior emerge from more simple biological structures is unclear CITATION, CITATION.
 For example, while it is possible to account for certain aspects of nervous system function using models that represent each neuron as a simple integrate and fire device, it is increasingly clear that this approach does not capture the full range of computations that many real neurons carry out CITATION, CITATION.
 Dendritic and axonal morphology are defining features of neuronal cell types and have important influences on the computations that a neuron performs CITATION.
 Differences in morphology determine how neurons respond to synaptic input and are sufficient to produce distinct patterns of spontaneous activity CITATION and degrees of action potential back-propagation from the soma into the dendrites CITATION.
 Cable theory and compartmental modeling provide a foundation for predicting the propagation of electrical signals in the dendrites and axons of neurons CITATION, CITATION.
 However, while the assumption that transitions between open and closed states of ion channels can be treated as a deterministic process may be sufficient for some purposes, recent evidence suggests that stochastic transitions between the states of individual ion channels could influence computations carried out by neurons CITATION CITATION.
 Stochastic opening and closing of ion channels causes noisy fluctuations in the current or voltage recorded from a neuron CITATION CITATION.
 While cable theory suggests that fluctuations of this kind might be particularly important in fine structures such as axons and dendrites CITATION, we nevertheless know very little about how neuronal morphology and stochastic gating of ion channels interact to determine how neurons respond to synaptic input.
 Given the difficulty of reducing detailed morphological models to simple analytical forms that could also incorporate stochastic gating of individual ion channels CITATION, experimentally constrained numerical simulations will be important to enable these issues to be explored systematically.
 Investigation of stochastic ion channel gating using numerical simulations has been limited by trades-offs between simulation accuracy and computation time CITATION.
 A simple approach is to add noise sources to deterministic models.
 However, as ion channels have multiple functional states with transitions that often depend on the membrane voltage CITATION, CITATION, CITATION, CITATION, this may not accurately account for the noise introduced by ion channel currents.
 A more accurate alternative is to explicitly model transitions between different functional states for each ion channel on a neuron's membrane.
 However, for neurons with complex axonal or dendritic architectures there are two substantial obstacles to this approach.
 First, typical central neurons express large numbers of ion channels and simulations must be repeated many times to obtain statistically valid descriptions CITATION.
 This is a formidable computational task and even relatively straightforward simulations of the consequences of stochastic channel gating can require substantial computing time.
 Second, each neuronal ion channel occupies a specific location on the extra-cellular membrane, whereas most neuronal models represent the distribution of ion channels as the density of a deterministic conductance across an area of membrane.
 Although this formalism has been successful for simulating many aspects of neuronal activity, it is of less use for models that explore the consequences of the localization of individual ion channels, for example to evaluate the macroscopic effects of short range interactions between ion channels and other signaling molecules CITATION, or the consequences of spatially heterogeneous distributions of ion channels within relatively small sub-cellular structures such as dendritic spines and axon terminals CITATION, CITATION .
 To address the functional consequences of stochastic ion channel gating in neurons with extensive dendritic or axonal arborizations we developed a parallel stochastic ion channel simulator, which enables efficient simulation of the electrical activity of neurons with complex morphologies and arbitrary localization of stochastic ion channels on the extracellular membrane, while also addressing limitations of previous approaches.
 We have also developed an interactive tool for visualization and development of models of neurons containing uniquely located ion channels.
 Here, we illustrate the use of PSICS and ICING, outline the computational strategies used and provide benchmark data for evaluation.
 We then identify previously unappreciated differences between the effects of stochastic ion channel gating on somatic and dendritic membrane potential activity in several different morphological classes of neuron.
 We show that the consequences of stochastic gating depend on dendritic morphology and suggest novel functional roles for the kinetics of ion channel gating.
 Using a previously well-validated realistic model of a CA1 pyramidal neuron we demonstrate that stochastic ion channel gating influences spike output in response to dendritic synaptic input.
 We show that stochastic gating of axonal or dendritic ion channels substantially modifies synaptically driven dendritic and axonal spike output, with stochastic gating of voltage-dependent sodium and potassium channels having the greatest impact and hyperpolarization-activated channels the least.
 By demonstrating that neuronal responses to dendritic synaptic input can be intrinsically probabilistic, these results offer a new and general perspective on synaptic integration by central neurons.
 Full documentation for PSICS/ICING as well as the software, source code and examples are available from the project website .### abstract ###
 Expansion of polyglutamine tracts in proteins results in protein aggregation and is associated with cell death in at least nine neurodegenerative diseases.
 Disease age of onset is correlated with the polyQ insert length above a critical value of 35 40 glutamines.
 The aggregation kinetics of isolated polyQ peptides in vitro also shows a similar critical-length dependence.
 While recent experimental work has provided considerable insights into polyQ aggregation, the molecular mechanism of aggregation is not well understood.
 Here, using computer simulations of isolated polyQ peptides, we show that a mechanism of aggregation is the conformational transition in a single polyQ peptide chain from random coil to a parallel -helix.
 This transition occurs selectively in peptides longer than 37 glutamines.
 In the -helices observed in simulations, all residues adopt -strand backbone dihedral angles, and the polypeptide chain coils around a central helical axis with 18.5 2 residues per turn.
 We also find that mutant polyQ peptides with proline-glycine inserts show formation of antiparallel -hairpins in their ground state, in agreement with experiments.
 The lower stability of mutant -helices explains their lower aggregation rates compared to wild type.
 Our results provide a molecular mechanism for polyQ-mediated aggregation.
 The appearance of polyglutamine -containing aggregates CITATION CITATION is a hallmark of disease progression in all diseases in which CAG-expansions occur in genes CITATION.
 Intranuclear inclusion bodies containing polyQ aggregates have been found in vitro CITATION, CITATION, in cell cultures, animal models, and affected patients CITATION, CITATION.
 The aggregates are known to have a characteristic amyloid topology CITATION.
 The inhibition of oligomerization by the azo-dye Congo red, or by the Hsp70/Hsp40 chaperone system, exerts marked protective effects in vivo and in vitro CITATION, CITATION.
 Aggregation and disease are observed if the number of glutamines in the expansion, n, exceeds a critical value, n C CITATION.
 The nearly universal existence of this criticality in all polyQ-related diseases suggests that when the polyQ insert length exceeds a critical value, a pathological change, largely independent of the host protein, occurs in the polyQ insert itself.
 Therefore, isolated polyQ peptides have been used as model systems for studying polyQ aggregation CITATION, CITATION, CITATION, and it is known that: The nuclear uptake of polyQ peptide aggregates prepared in vitro is cytotoxic in cell cultures CITATION, isolated polyQ peptides have in vitro aggregation properties similar to the corresponding full-length proteins containing the polyQ insert CITATION, CITATION, peptide aggregation follows a nucleated mechanism showing characteristic lag and growth phases CITATION, CITATION, and the glutamine tract-length dependence of the lag-time interval correlates well with the age of onset of disease CITATION.
 Peptides of subcritical lengths have long lag times of aggregation and a corresponding age of onset later than the typical life span of a person.
 Longer peptides have progressively smaller lag times of aggregation, and a correspondingly early age of onset of the disease CITATION .
 Unaggregated polyQ peptides form random coil structures, whereas aggregates are composed of amyloid-like -strands CITATION.
 The conversion of random coil to -strand occurs in an individual polyQ chain CITATION, and fibril formation occurs by addition of other polyQ chains to these monomeric -strand nuclei.
 Therefore, the conformational dynamics of an individual polyQ chain determines both its aggregation mechanism and the structure of the final aggregates.
 The details of the conformational dynamics of polyQ and the length dependence of the dynamics are not well understood CITATION .
	previous research on anchoring has shown this heuristic to be a very robust psychological phenomenon ubiquitous across many domains of human judgment and decision-making.
	despite the prevalence of anchoring effects  researchers have only recently begun to investigate the underlying factors responsible for how and in what ways a person is susceptible to them.
	this paper examines how one such factor  the big-five personality trait of openness-to-experience  influences the effect of previously presented anchors on participants' judgments.
	our findings indicate that participants high in openness-to-experience were significantly more influenced by anchoring cues relative to participants low in this trait.
	these findings were consistent across two different types of anchoring tasks providing convergent evidence for our hypothesis
	the anchoring effect  CITATION  refers to the adjustment of one's assessment  higher or lower  based upon previously presented external information or an  anchor. 
	the anchoring heuristic appears to be prevalent throughout human decision processes and has been shown to reliably influence judgments in a variety of domains including probability estimates  CITATION   negotiation  CITATION   legal judgments  CITATION   and general knowledge  CITATION.
	further  anchoring effects appear viable across most situations for both novices and experts  CITATION  and seem to be effective under conditions of monetary incentives  CITATION  and in real-world settings  CITATION.
	anchoring thus appears to be a very robust psychological phenomenon.
	however  not all individuals may be equally influenced by anchoring cues.
	identification of factors that influence how and in what ways a person is susceptible to this heuristic should further the understanding of the process.
	one avenue of approach is to investigate the role of individual difference factors.
	tversky and kahneman  CITATION  pointed to the important role of  personal characteristics  of the decision maker in risky choice situations.
	later work by stanovich and west  CITATION  suggested that intellectual traits influence decision making and consequential choice preference.
	recently  individual differences have been found in numerical reliance  CITATION   ambiguity  CITATION   preference for actions or inactions  CITATION  and the optimistic bias  CITATION.
	the big-five personality traits  CITATION  have proven to be important individual difference factors for understanding decision choices.
	further  attesting to the importance of individual differences  levin and hart  CITATION  demonstrated that individual differences in preference appear to originate at a very early age.
	taken together  these findings suggest that the impact of individual difference factors on decision-making is both profound and pervasive.
	the purpose of the current study is to investigate how one individual difference factor may influence the strength of the anchoring effect.
	specifically  we are interested in how individual differences in the personality trait of openness-to-experience influences anchoring effects.
	in the last couple of decades the five-factor model of personality has become the most widely tested and well-regarded personality trait model.
	a great deal of research has supported this model's validity and reliability  CITATION.
	while most research has agreed on the nature of the first four factors  the nature of the fifth factor has been controversial; a controversy predominately based upon whether a lexical approach  derived from language frequency within the lexicon of a particular language  CITATION   or a questionnaire approach  CITATION  should be used to measure it. 
	the fifth factor is often labeled openness-to-experience  which refers to a propensity to adjust beliefs and behaviors when exposed to new types of information or ideas  CITATION. 
	individuals scoring high on this dimension are more open to new ideas  CITATION  and motivated to seek variety and external experience.
	individuals scoring low tend to be less inclined to consider alternative opinions and are more steadfast in their own beliefs  CITATION  making them more likely to rely upon information that is familiar and conventional  CITATION.
	a fundamental aspect of the anchoring effect is that individuals are sensitive to information which they have experienced.
	this change in judgment  which is based upon external cues  seems particularly relevant and related to the openness-to-experience personality trait.
	specifically  as research has shown  the openness trait reflects individual propensities to  adjust  one's beliefs  CITATION  and to consider external information  CITATION.
	therefore  based upon the nature of the openness-to-experience trait and the processes involved in the anchoring effect we hypothesize that individual differences in openness-to-experience will influence susceptibility to anchoring effects.
	specifically  we hypothesize that the judgments of those individuals high in this trait will be more influenced by previously presented anchors whereas those individuals low in this trait will be less influenced by the anchor.
	to test this hypothesis  we first measured individual levels of the personality trait of openness-to-experience.
	we then provided participants with an anchoring task involving either the mississippi river study  NUMBER  or african nations in the un study  NUMBER 
	previous research on anchoring has shown this heuristic to be a very robust psychological phenomenon ubiquitous across many domains of human judgment and decision-making
	despite the prevalence of anchoring effects  researchers have only recently begun to investigate the underlying factors responsible for how and in what ways a person is susceptible to them
	this paper examines how one such factor  the big-five personality trait of openness-to-experience  influences the effect of previously presented anchors on participants' judgments
	our findings indicate that participants high in openness-to-experience were significantly more influenced by anchoring cues relative to participants low in this trait
	these findings were consistent across two different types of anchoring tasks providing convergent evidence for our hypothesis
	the anchoring effect  CITATION  refers to the adjustment of one's assessment  higher or lower  based upon previously presented external information or an  anchor
	the anchoring heuristic appears to be prevalent throughout human decision processes and has been shown to reliably influence judgments in a variety of domains including probability estimates  CITATION   negotiation  CITATION   legal judgments  CITATION   and general knowledge  CITATION
	further  anchoring effects appear viable across most situations for both novices and experts  CITATION  and seem to be effective under conditions of monetary incentives  CITATION  and in real-world settings  CITATION
	anchoring thus appears to be a very robust psychological phenomenon
	however  not all individuals may be equally influenced by anchoring cues
	identification of factors that influence how and in what ways a person is susceptible to this heuristic should further the understanding of the process
	one avenue of approach is to investigate the role of individual difference factors
	tversky and kahneman  CITATION  pointed to the important role of  personal characteristics  of the decision maker in risky choice situations
	later work by stanovich and west  CITATION  suggested that intellectual traits influence decision making and consequential choice preference
	recently  individual differences have been found in numerical reliance  CITATION   ambiguity  CITATION   preference for actions or inactions  CITATION  and the optimistic bias  CITATION
	the big-five personality traits  CITATION  have proven to be important individual difference factors for understanding decision choices
	further  attesting to the importance of individual differences  levin and hart  CITATION  demonstrated that individual differences in preference appear to originate at a very early age
	taken together  these findings suggest that the impact of individual difference factors on decision-making is both profound and pervasive
	the purpose of the current study is to investigate how one individual difference factor may influence the strength of the anchoring effect
	specifically  we are interested in how individual differences in the personality trait of openness-to-experience influences anchoring effects
	in the last couple of decades the five-factor model of personality has become the most widely tested and well-regarded personality trait model
	a great deal of research has supported this model's validity and reliability  CITATION
	while most research has agreed on the nature of the first four factors  the nature of the fifth factor has been controversial  a controversy predominately based upon whether a lexical approach  derived from language frequency within the lexicon of a particular language  CITATION   or a questionnaire approach  CITATION  should be used to measure it
	the fifth factor is often labeled openness-to-experience  which refers to a propensity to adjust beliefs and behaviors when exposed to new types of information or ideas  CITATION
	individuals scoring high on this dimension are more open to new ideas  CITATION  and motivated to seek variety and external experience
	individuals scoring low tend to be less inclined to consider alternative opinions and are more steadfast in their own beliefs  CITATION  making them more likely to rely upon information that is familiar and conventional  CITATION
	a fundamental aspect of the anchoring effect is that individuals are sensitive to information which they have experienced
	this change in judgment  which is based upon external cues  seems particularly relevant and related to the openness-to-experience personality trait
	specifically  as research has shown  the openness trait reflects individual propensities to  adjust  one's beliefs  CITATION  and to consider external information  CITATION
	therefore  based upon the nature of the openness-to-experience trait and the processes involved in the anchoring effect  we hypothesize that individual differences in openness-to-experience will influence susceptibility to anchoring effects
	specifically  we hypothesize that the judgments of those individuals high in this trait will be more influenced by previously presented anchors whereas those individuals low in this trait will be less influenced by the anchor
	to test this hypothesis  we first measured individual levels of the personality trait of openness-to-experience
	we then provided participants with an anchoring task involving either the mississippi river study  NUMBER  or african nations in the un study  NUMBER 
 this paper extends previous research showing that experienced difficulty of recall can influence evaluative judgments CITATION to a field study of university students rating a course
 students completed a mid-course evaluation form in which they were asked to list either  NUMBER  ways in which the course could be improved a relatively easy task or  NUMBER  ways in which the course could be improved a relatively difficult task
 respondents who had been asked for  NUMBER  critical comments subsequently rated the course more favorably than respondents who had been asked for  NUMBER  critical comments
 an internal analysis suggests that the number of critiques solicited provides a frame against which accessibility of instances is evaluated
 the paper concludes with a discussion of implications of the present results and possible directions for future research
 according to tversky and kahneman's  CITATION  availability heuristic  people sometimes judge the frequency of events in the world by the ease with which examples come to mind
 this process has generally been demonstrated by asking participants to assess the relative likelihood of two categories in which instances of the first category are more difficult to recall than instances of the second category   despite the fact that instances of the first category are more common in the world
 for instance  kahneman and tversky  CITATION  found that most people think the letter r more often appears in english words as the first letter than the third letter  presumably because the first letter provides a better cue for recalling instances of words than does the third letter
 in fact  it turns out that r appears more often as the third than first letter in english words
 schwarz et al CITATION  observed that the classic studies demonstrating the availability heuristic failed to distinguish an interpretation based on ease of retrieval from an alternative interpretation based on content of retrieval in which an event is judged more common when a larger number of examples come to mind
 to tease apart these accounts  schwarz et al CITATION  asked participants in one   study to list either  NUMBER  or  NUMBER  examples of assertive or unassertive behavior that they have exhibited and then rate themselves on their overall degree of assertiveness
 participants rated themselves as more assertive after they had listed  NUMBER  examples of assertive behavior a relatively easy task rather than  NUMBER  examples a relatively difficult task  similarly  they rated themselves as less assertive i e   more unassertive after they had listed  NUMBER  rather than  NUMBER  examples of unassertive behavior
 similar patterns of results have been observed in many other studies of frequency-related judgments  including the rate at which a particular letter occurs in various positions of words  CITATION   the quality of one's own memory  CITATION   the frequency of one's own past behaviors  CITATION   one's susceptibility to heart disease  CITATION  and one's susceptibility to sexual assault  CITATION
 for a review of this literature see schwarz  CITATION
 thus  an abundance of data supports the original interpretation of the availability heuristic  categories are judged to be more common when instances more easily come to mind  even when a smaller absolute number of instances are generated
 this program has been extended from frequency-based judgments to  evaluative judgments of such targets as public transportation  CITATION   luxury automobiles  CITATION   and one's own childhood  CITATION
 for instance  winkielman and schwarz  CITATION  asked participants to recall either  NUMBER  childhood events an easy task or  NUMBER  childhood events a difficult task
 some participants were then led to believe that memories from pleasant periods tend to fade  while others were led to believe that memories from unpleasant periods tend to fade
 when later asked to evaluate their childhood  participants believed that pleasant memories fade rated their childhood more favorably when they completed the difficult task  NUMBER  events than the easy task  NUMBER  events  participants who believed that unpleasant memories fade rated their childhood more favorably when they completed the easy rather than difficult task
 previous studies of the availability heuristic using the paradigm of schwarz et al CITATION  have turned up impressive and robust results
 however  these demonstrations have been restricted primarily to laboratory surveys in which task of recalling examples then making an overall assessment may seem somewhat artificial to participants and the responses of little consequence
 more important  most participants in previous studies presumably had little prior experience with the particular likert scale that served as the dependent measure e g   most had never before rated their childhood or public transportation on a  NUMBER -point scale
 hence  ratings of respondents may be especially susceptible to superficial cues-such as the accessibility of instances-when mapping their beliefs and attitudes onto an unfamiliar response scale
 the present investigation overcomes these limitations through a  field study  of students evaluating a course
 first  evaluations are a normal facet of most university courses in which students are commonly asked to list specific suggestions and also provide a global assessment
 moreover  course evaluations are consequential  as they can influence future course offerings and course staffing  promotion and tenure decisions  and provide information to future prospective students of the target course
 second  students at universities quickly become familiar with standard course evaluation scales and how ratings are distributed across classes  often relying on these scores in choosing among elective courses
 the study of course evaluations is also interesting in its own right
 a number of recent papers have questioned the validity of these ratings  and a lively debate appeared some years ago in the american psychologist  CITATION
 thus far  questions of discriminant validity have mainly focused on the correlation between teaching ratings and apparently irrelevant factors such as the students' expected grades or the course workload
 to date there have been few published investigations of the relationship between the design of course feedback forms and summary course evaluations
 the present study attempts to answer the following provocative question  can one paradoxically obtain higher course ratings by soliciting a greater number of critical comments from students
	The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied
	If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence speed
	For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures
	We show that this is even the case if the model class contains only Bernoulli distributions
	We derive a new upper bound on the prediction error for countable Bernoulli classes
	This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes
	We discuss the application to Machine Learning tasks such as classification and hypothesis testing, and generalization to countable classes of iid models
	``Bayes mixture", ``Solomonoff induction", ``marginalization", all these terms refer to a central induction principle: Obtain a predictive distribution by integrating the product of prior and evidence over the model class
	In many cases however, the Bayes mixture is computationally infeasible, and even a sophisticated approximation is expensive
	The MDL or MAP (maximum a posteriori) estimator is both a common approximation for the Bayes mixture and interesting for its own sake: Use the model with the largest product of prior and evidence
	In practice, the MDL estimator is usually being approximated too, since only a local maximum is determined
	How good are the predictions by Bayes mixtures and MDL
	This question has attracted much attention
	In many cases, an important quality measure is the  total  or cumulative  expected loss  of a predictor
	In particular the square loss is often considered
	Assume that the outcome space is finite, and the model class is continuously parameterized
	Then for Bayes mixture prediction, the cumulative expected square loss is usually small but unbounded, growing with  SYMBOL , where  SYMBOL  is the sample size  CITATION
	This corresponds to an  instantaneous  loss bound of  SYMBOL
	For the MDL predictor, the losses behave similarly  CITATION  under appropriate conditions, in particular with a specific prior
	Note that in order to do MDL for continuous model classes, one needs to  discretize  the parameter space, see also  CITATION
	On the other hand, if the model class is discrete, then Solomonoff's theorem  CITATION  bounds the cumulative expected square loss for the Bayes mixture predictions finitely, namely by  SYMBOL , where  SYMBOL  is the prior weight of the ``true" model  SYMBOL
	The only necessary assumption is that the true distribution  SYMBOL  is contained in the model class, ie that we are dealing with  proper learning
	It has been demonstrated  CITATION , that for both Bayes mixture and MDL, the proper learning assumption can be essential: If it is violated, then learning may fail very badly
	For MDL predictions in the proper learning case, it has been shown  CITATION  that a bound of  SYMBOL  holds
	This bound is exponentially larger than the Solomonoff bound, and it is sharp in general
	A finite bound on the total expected square loss is particularly interesting:   It implies convergence of the predictive to the true probabilities with probability one
	In contrast, an instantaneous loss bound of  SYMBOL  implies only convergence in probability
	Additionally, it gives a  convergence speed , in the sense that errors of a certain magnitude cannot occur too often
	So for both, Bayes mixtures and MDL, convergence with probability one holds, while the convergence speed is exponentially worse for MDL compared to the Bayes mixture
	We avoid the term ``convergence rate" here, since the order of convergence is identical in both cases
	It is eg  SYMBOL  if we additionally assume that the error is monotonically decreasing, which is not necessarily true in general)
	It is therefore natural to ask if there are model classes where the cumulative loss of MDL is comparable to that of Bayes mixture predictions
	In the present work, we concentrate on the simplest possible stochastic case, namely discrete Bernoulli classes
	Note that then the MDL ``predictor" just becomes an estimator, in that it estimates the true parameter and directly uses that for prediction
	Nevertheless, for consistency of terminology, we keep the term predictor
	It might be surprising to discover that in general the cumulative loss is still exponential
	On the other hand, we will give mild conditions on the prior guaranteeing a small bound
	Moreover, it is well-known that the instantaneous square loss of the Maximum Likelihood estimator decays as  SYMBOL  in the Bernoulli case
	The same holds for MDL, as we will see
	If convergence speed is measured in terms of instantaneous losses, then much more general statements are possible  CITATION , this is briefly discussed in Section
	A particular motivation to consider discrete model classes arises in Algorithmic Information Theory
	From a computational point of view, the largest relevant model class is the class of all computable models on some fixed universal Turing machine, precisely prefix machine  CITATION
	Thus each model corresponds to a program, and there are countably many programs
	Moreover, the models are stochastic, precisely they are  semimeasures  on strings (programs need not halt, otherwise the models were even measures)
	Each model has a natural description length, namely the length of the corresponding program
	If we agree that programs are binary strings, then a prior is defined by two to the negative description length
	By the Kraft inequality, the priors sum up to at most one
	Also the Bernoulli case can be studied in the view of Algorithmic Information Theory
	We call this the  universal setup : Given a universal Turing machine, the related class of Bernoulli distributions is isomorphic to the countable set of computable reals in  SYMBOL
	The description length  SYMBOL  of a parameter  SYMBOL  is then given by the length of its shortest program
	A prior weight may then be defined by  SYMBOL
	If a string  SYMBOL  is generated by a Bernoulli distribution with computable parameter  SYMBOL , then with high probability the two-part complexity of  SYMBOL  with respect to the Bernoulli class does not exceed its algorithmic complexity by more than a constant, as shown by Vovk  CITATION
	That is, the two-part complexity with respect to the Bernoulli class  is  the shortest description, save for an additive constant
	Many Machine Learning tasks are or can be reduced to sequence prediction tasks
	An important example is classification
	The task of classifying a new instance  SYMBOL  after having seen (instance,class) pairs  SYMBOL  can be phrased as to predict the continuation of the sequence  SYMBOL
	Typically the (instance,class) pairs are iid
	Cumulative loss bounds for prediction usually generalize to prediction  conditionalized  to some inputs  CITATION
	Then we can solve classification problems in the standard form
	It is not obvious if and how the proofs in this paper can be conditionalized
	Our main tool for obtaining results is the Kullback-Leibler divergence
	Lemmata for this quantity are stated in Section
	Section  shows that the exponential error bound obtained in  CITATION  is sharp in general
	In Section , we give an upper bound on the instantaneous and the cumulative losses
	The latter bound is small eg under certain conditions on the distribution of the weights, this is the subject of Section
	Section  treats the universal setup
	Finally, in Section  we discuss the results and give conclusions### abstract ###
 the paper extends research on fixed-pie perceptions by suggesting that disputants may prefer proposals that are perceived to be equally attractive to both parties i e , balanced rather than one-sided, because balanced agreements are seen as more likely to be successfully implemented
 we test our predictions using data on israeli support for the geneva accords, an agreement for a two state solution negotiated by unofficial delegations of israel and the palestinian authority in 2003
 the results demonstrate that israelis are more likely to support agreements that are seen favorably by other israelis, but - contrary to fixed-pie predictions - israeli support for the accords does not diminish simply because a majority of palestinians favors rather than opposes the accords
 we show that implementation concerns create a demand among israelis for balance in the degree to which each side favors or opposes the agreement
 the effect of balance is noteworthy in that it creates considerable support for proposals even when a majority of israelis and palestinians oppose the deal
 normative models of bargaining and negotiation suggest that if there is potential for mutual benefit, conflicting parties should be able to achieve it  CITATION
 descriptive accounts and empirical investigations of negotiation behavior  CITATION , however, suggest that a number of psychological barriers to conflict resolution are likely to make efficient deal making difficult  CITATION
 for example, research on cognitive biases associated with egocentric perceptions suggests that negotiators and evaluators of negotiated agreements are likely to exhibit a "fixed-pie bias"  CITATION
 the fixed-pie bias refers to the belief that any gain for one party will be associated with an equivalent loss to the other party
 this belief is a "bias" when it persists even in contexts where there is a possibility of compatible interests or mutual benefit
 a large body of research finds that negotiators are susceptible to the fixed-pie bias prior to, during, and even after negotiations  CITATION
 in the current paper we investigate and extend research on fixed pie bias in the context of protracted intergroup conflict
	defensive forecasting is a method of transforming laws of probability  stated in game theoretic terms as strategies for sceptic  into forecasting algorithms
	there are two known varieties of defensive forecasting    continuous    in which sceptic s moves are assumed to depend on the forecasts in a  semi continuous manner and which produces deterministic forecasts  and   randomized    in which the dependence of sceptic s moves on the forecasts is arbitrary and forecaster s moves are allowed to be randomized
	this note shows that the randomized variety can be obtained from the continuous variety by smearing sceptic s moves to make them continuous  
	textbf new as compared to version  NUMBER    NUMBER  august  NUMBER   of this report   the assumption of version  NUMBER  that the outcome space   is finite is relaxed  and now it is only assumed to be compact
	in the case where   is finite  it is shown that forecaster can choose his randomized forecasts concentrated on a finite set of cardinality at most
	the continuous variety of defensive forecasting was essentially introduced by levin  CITATION   but was later rediscovered by kakade and foster  CITATION  and takemura  et al    CITATION
	the randomized variety was introduced  in the case of von mises s version of the game theoretic approach to probability  by foster and vohra  CITATION  and further developed by  among others  sandroni  et al    CITATION   these papers  however  were only concerned with asymptotic calibration
	non asymptotic versions of the randomized variety were proposed by sandroni  CITATION   based on standard measure theoretic probability  and vovk and shafer  CITATION   based on game theoretic probability 
	kakade and foster  CITATION  noticed that some calibration results require very little randomization  this will be an important aspect of our theorem  
	this note states two simple results about defensive forecasting  theorem  about the continuous variety and theorem  about the randomized variety
	the proof of theorem  is obtained from the proof of theorem  by blurring sceptic s moves
	in our informal discussions we will be assuming that the set   of all possible outcomes is finite  although we will try to make mathematical statements as general as possible
	the reader who is only interested in the main ideas might choose to specialize theorems  and  and their proofs to the case of finite
 The firing rate of single neurons in the mammalian hippocampus has been demonstrated to encode for a range of spatial and non-spatial stimuli.
 It has also been demonstrated that phase of firing, with respect to the theta oscillation that dominates the hippocampal EEG during stereotype learning behaviour, correlates with an animal's spatial location.
 These findings have led to the hypothesis that the hippocampus operates using a dual coding system.
 To investigate the phenomenon of dual coding in the hippocampus, we examine a spiking recurrent network model with theta coded neural dynamics and an STDP rule that mediates rate-coded Hebbian learning when pre- and post-synaptic firing is stochastic.
 We demonstrate that this plasticity rule can generate both symmetric and asymmetric connections between neurons that fire at concurrent or successive theta phase, respectively, and subsequently produce both pattern completion and sequence prediction from partial cues.
 This unifies previously disparate auto- and hetero-associative network models of hippocampal function and provides them with a firmer basis in modern neurobiology.
 Furthermore, the encoding and reactivation of activity in mutually exciting Hebbian cell assemblies demonstrated here is believed to represent a fundamental mechanism of cognitive processing in the brain.
 The hippocampus and surrounding medial temporal lobe are implicated in declarative memory function in humans and other mammals CITATION.
 Electrophysiology studies in a range of species have demonstrated that the activity of single pyramidal cells within this region can encode for the presence of both spatial and non-spatial stimuli CITATION.
 The majority of empirical investigation has focussed on place cells neurons whose firing rate is directly correlated with an animal's spatial location within the corresponding place field CITATION.
 Subsequent research has identified similar single cell responses to a variety of non-spatial cues including odour CITATION, complex visual images CITATION, CITATION, CITATION, running speed CITATION and the concept of a bed or nest CITATION.
 It has also been demonstrated that the exact timing of place cell discharge, relative to the theta oscillation which dominates the hippocampal EEG during learning, correlates with distance travelled through a place field CITATION, CITATION, CITATION CITATION.
 This phase precession mechanism creates a compressed theta coded firing pattern in place cells which corresponds to the sequence of place fields being traversed CITATION.
 These findings have led to the hypothesis that the hippocampus operates using a dual rate and temporal coding system CITATION, CITATION.
 Here we present a spiking neural network model which utilises a dual coding system in order to encode and recall both symmetric and asymmetric connections between neurons that exhibit repeated synchronous and asynchronous firing patterns respectively.
 The postulated mnemonic function of the hippocampus has been extensively modelled using recurrent neural networks, and this approach is supported by empirical data CITATION CITATION.
 The biological correlate of these models is widely believed to be the CA3 region, which exhibits dense recurrent connectivity and wherein synaptic plasticity can be easily and reliably induced.
 Pharmacological and genetic knockout studies have demonstrated that NMDAr-dependent synaptic plasticity in CA3 is critical for the rapid encoding of novel information, and synaptic output from CA3 critical for its retrieval CITATION, CITATION.
 Recurrent neural network models of hippocampal mnemonic function have generally utilised rate-coded Hebbian learning rules to generate reciprocal associations between neurons with concurrently elevated firing rates CITATION, CITATION.
 Hypothetically, this corresponds to the presence of either multiple stimuli or multiple overlapping place fields encountered at a single location CITATION CITATION.
 The hippocampus is also implicated in sequence learning, and temporally asymmetric plasticity rules have subsequently been employed in recurrent network models to generate hetero-associative connections between neurons that fire with repeated temporal correlation CITATION CITATION.
 Hypothetically, this corresponds to a sequence of place fields being traversed or stimuli being encountered on a behavioural timescale CITATION.
 Importantly, previous computational models of hetero-associative learning have typically encoded each successive stage of a learned sequence with the activity of a single neuron, while empirical studies estimate that place fields are typically encoded by an ensemble of several hundred place cells CITATION, CITATION CITATION.
 No computational model has thus far integrated auto- and hetero- associative learning in order to simultaneously generate both bi-directional and asymmetric connections between neurons that are active at the same and successive theta phases respectively using a single temporally asymmetric synaptic plasticity rule.
 Empirical data indicates that changes in the strength of synapses within the hippocampus can depend upon temporal correlations in pre- and post- synaptic firing according to a spike-timing dependent plasticity rule CITATION CITATION.
 It is not yet clear if rate-coded auto-associative network models of hippocampal mnemonic function are compatible with STDP or theta coded neural dynamics.
 Here, we examine the synaptic dynamics generated by several different STDP rules in a spiking recurrent neural network model of CA3 during the encoding of temporal, rate and dual coded activity patterns created by a phenomenological model of phase precession.
 We demonstrate that under certain conditions - the STDP rule can generate both bi-directional connections between neurons which burst at concurrent theta phase and asymmetric connections between neurons which fire at consecutive theta phase.
 Subsequent superthreshold stimulation of a small number of simulated neurons generates putative recall activity, driven by recurrent excitation, that corresponds to pattern completion and/or sequence prediction in auto- and/or hetero- associative connections respectively.
 Interestingly, these neural dynamics are reminiscent of sharp wave ripple activity observed in vivo CITATION CITATION.
 These findings demonstrate that STDP and theta coded neural dynamics are compatible with rate-coded auto-associative network models of hippocampal function.
 Furthermore, the encoding and reactivation of dual coded Hebbian phase sequences of activity in mutually exciting neuronal ensembles demonstrated here has been proposed as a general neural coding mechanism for cognitive processing CITATION, CITATION CITATION .
 when judging their likelihood of success in competitive tasks, people tend to be overoptimistic for easy tasks and overpessimistic for hard tasks the shared circumstance effect; sce
 previous research has shown that feedback and experience from repeated-play competitions has a limited impact on sces
 however, in this paper, we suggest that competitive situations, in which the shared difficulty or easiness of the task is more transparent, will be more amenable to debiasing via repeated play
 pairs of participants competed in, made predictions about, and received feedback on, multiple rounds of a throwing task involving both easy- and hard-to-aim objects
 participants initially showed robust sces, but they also showed a significant reduction in bias after only one round of feedback
 these and other results support a more positive view than suggested from past research on the potential for sces to be debiased through outcome feedback
 competition abounds in everyday life, where we contend with others for top grades, jobs, trophies, and mates
 when resources are at a premium, it is optimal to enter into competitive environments in which we are certain to fare well and to avoid those in which we are doomed to fail
 however, when people evaluate their likelihood of success in competitions, they are subject to a robust bias: a competitor should consider the strengths and weaknesses of the self and the other competitors  CITATION , but people often give too much weight to evidence related to their own strengths and weaknesses and too little weight to such evidence about the competitor  CITATION
 this egocentrism results in overoptimism when the circumstances of the competition are favorable, such as when competitors in a trivia game learn that the questions will be from an easy category-even though they'll be easy for everyone  CITATION
 egocentrism also results in overpessimism when the circumstances are unfavorable e g , a difficult trivia category
 this phenomenon of being more optimistic when shared competitive circumstances are favorable than when they are not has been dubbed the shared-circumstance effect  CITATION , and it has been replicated across a variety of settings e g , general knowledge tasks, card games, athletic competitions
 in most previous studies on the sce, participants were presented with novel, non-repeated competitive situations
 these situations did not allow people to learn from past experiences or from feedback within the immediate competitive context
 however, in everyday contexts there are often opportunities to learn how a shared circumstance tends to affect the self, others, and outcomes
 for example, when a tennis tournament is played during a string of windy days, players can have several opportunities to observe how the weather affects themselves and their competitors
 to examine whether egocentrism and sces persist in repeated-play contexts, rose and windschitl  CITATION  had pairs of participants compete against each other in multiple rounds of a trivia contest that involved easy and hard categories
 in each round, participants estimated their likelihood of beating their competitor, answered trivia questions, and received feedback about who won
 in initial rounds, there were robust sces; participants were much more optimistic about winning easy categories than hard ones
 if they encountered the same hard and easy categories across rounds, the participants learned from feedback
 that is, the sce shrank-but slowly-across six rounds with the same categories
 the sce was never eliminated, even after six rounds
 also, for a seventh round, participants were told there would be new categories
 the sce for that round dramatically and fully rebounded; it was every bit as large as observed for round 1
 these results provide a bleak view of how well people can learn from feedback and avoid sces
 moreover, results from a study by moore and cain  CITATION , which also used repeated plays with feedback, suggest an even bleaker view
 those researchers also used easy and difficult quizzes as shared-circumstance manipulations, but found virtually no reduction in sces after numerous rounds with feedback CITATION
 are people's abilities to learn to avoid sces-based on feedback-really as bleak as this prior research might suggest
 we argue that some shared circumstances are more transparently shared than others, and this may affect how readily people learn to avoid the bias that creates sces-and how easily they can transfer this learning to a slightly new set of shared circumstances
 by transparently shared, we are referring to how obvious it is that a circumstance that is helpful or hindering to the self will affect others in largely the same way
 in the present study, we examined the influence of repeated feedback on sces
 however, unlike past research, we used a competition in which the difficulty of the shared circumstance relative to competitions, for example, involving easy and hard trivia categories is more transparently shared
 in a multi-round paradigm, participants competed in object-tossing competitions
 in each round, two competitors each had 8 throws per object-attempting to land the object inside a target area
 there was always one easy-to-aim object e g , a beanbag and one hard-to-aim object e g , a paper plate, which constituted our shared-circumstance manipulation
 full feedback was given during and after each round, and predictions were solicited before each round and also before a final round with novel objects
 we suspected that a tossing competition, rather than a trivia competition, would produce less bleak results about the debiasing of sces through repeated play
 in the case of trivia, watching one's competitor fail to answer trivia questions doesn't give any insight about why the category is difficult for that person
 also, knowing that one's competitor struggled on a difficult category does not provide obvious information about why he or she might struggle on another difficult category
 however, in the case of throwing, watching a competitor fail when tossing an object probably illuminates the relevance of specific shared, situational circumstances i e , the properties of the specific objects as well as a more general awareness of the relevance that object properties have on throwing success-for anyone
 for example, seeing a paper plate fly unpredictably will likely give an observer a clear impression that the object will fly unpredictably regardless of who is throwing it
 for participants, this enhances the appreciation that one's struggles are not primarily due to personal characteristics but to properties of the tossed objects; this would then be useful in mitigating egocentrism and sces, even when new objects are introduced
 consequently, we expected that, even though participants might reveal a robust sce at round 1 prior to any feedback or observations regarding their competitor, they would show a pronounced learning effect after the feedback and observations of round 1
 that is, they would show significantly reduced sces starting immediately after round 1
 we also expected that this learning would be generally transferable
	this study was designed to assess sex-related differences in the selection of an appropriate strategy when facing novelty
 	a simple visuo-spatial task was used to investigate exploratory behavior as a specific response to novelty
	the exploration task was followed by a visual discrimination task  and the responses were analyzed using signal detection theory
	during exploration women selected a local searching strategy in which the metric distance between what is already known and what is unknown was reduced  whereas men adopted a global strategy based on an approximately uniform distribution of choices
	women's exploratory behavior gives rise to a notion of a secure base warranting a sense of safety while men's behavior does not appear to be influenced by risk
	this sex-related difference was interpreted as a difference in beliefs concerning the likelihood of uncertain events influencing risk evaluation
	males and females seem to differ in spatial abilities and styles  CITATION
	generally  studies involving navigational problems showed that female cognitive style relies more on detailed information  while male style relies more on global information  CITATION
	evolutionary mechanisms could potentially account for sex differences in spatial behavior
	for example  these behavioral differences may be due to mating patterns that induced a selection of large-range navigation in males  CITATION
	mating patterns or mating strategies are linked to the dynamics of reproduction and sexual selection  CITATION   and sexual selection is restricted to characteristics that influence mate choice and competition for mates
	typically  males have to compete through extensive ranging for access to mates while females have to choose mating partners according to reproductive success  CITATION
	another proposition  but exclusively directed at humans  suggested that the division of labor game hunting and plant gathering would have put greater selection pressure on females' spatial memory because females sustained gathering duties  CITATION
	however  as argued by ecuyer-dab and robert  CITATION    the selection of male characteristics depends on females' choice for mates
	in females  however  spatial cognition would have been primarily shaped by the natural selection of a strong concern for survival both of self and of offspring
	this concern would have compelled them to favor low-risk strategies  like concentrating on proximal spatial cues  when coping with space-related problems
	such focusing would have enabled secure navigation based on detailed landmark encoding  as well as  in certain species  regular feeding based on remembering the exact locations of potential resources  p  NUMBER 
	thus  the hypothesis of labor division would be a by-product of sexual selection and not the cause of sex differences in spatial behavior
	taken together  the literature seems to indicate that the key to understanding the evolution of behavioral sex differences relies on the relative costs and benefits of producing offspring  CITATION
	in that context spatial skills play a crucial role since they increase reproductive success and the accessibility to food resource but  at the same time  multiply the risks of getting lost  being killed or consumed by other animals predation
	hence  the survival of mobile species depends on their ability to balance costs and benefits induced by locomotion and this balancing should differ according to sex
	experimental investigations of sex differences in spatial abilities yield apparently disparate results  CITATION
	this might be partly due to the complexity of contemporary experimental designs  but also to a lack of investigations concerning decision-making processes involved in the selection of strategies
	the current study investigates sex differences in basic behaviors like exploration  detection and discrimination involving the selection of strategies when coping with uncertainty
	following the above quotation from ecuyer-dab and robert's  the hypothesis is that women  compared to men  should favor low-risk strategies when coping with space-related problems
	to test this  i used a simple spontaneous two-dimensional exploratory task
	this choice relies on the fact that exploration is a natural behavior and that it is fundamental in acquiring spatial knowledge
	it seems to be based on driving factors such as curiosity  comfort or mastery over one's environment
	moreover  it is commonly defined as serving to reduce uncertainty and thus allow coping with fear  CITATION
	exploration is mainly characterized by a succession of progressions and stops  CITATION   and the selection of exploration could rely on its capacity to act as a regulator of uncertainty
	indeed  progressions are based on decisions taken during stops  and stops correspond to choice points allowing decisions
	voss  CITATION  refers to the exploration process as the generation and testing of hypotheses concerning the object's meaning and potential use
	in order to assess risk-taking  a classical visual discrimination task based on the stimuli observed during the exploration task was used
	the results were analyzed with signal detection theory
	in the constraint satisfaction problem      the aim is to find an assignment of values to a set of variables subject to specified constraints
	in the minimum cost homomorphism problem      one is additionally given weights   for every variable   and value    and the aim is to find an assignment   to the variables that minimizes
	let   denote the   problem parameterized by the set of predicates allowed for constraints
	is related to many well studied combinatorial optimization problems  and concrete applications can be found in  for instance  defence logistics and machine learning
	we show that   can be studied by using algebraic methods similar to those used for csps
	with the aid of algebraic techniques  we classify the computational complexity of   for all choices of
	our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs   gutin  hell  rafiey  yeo  european j of combinatorics   NUMBER  
	constraint satisfaction problems     are a natural way of formalizing a large number of computational problems arising in combinatorial optimization  artificial intelligence  and database theory
	this problem has the following two equivalent formulations    NUMBER   to find an assignment of values to a given set of variables  subject to constraints on the values that can be assigned simultaneously to specified subsets of variables  and   NUMBER   to find a homomorphism between two finite relational structures   and
	applications of  s arise in the propositional logic  database and graph theory  scheduling and many other areas
	during the past  NUMBER  years    and its subproblems has been intensively studied by computer scientists and mathematicians
	considerable attention has been given to the case where the constraints are restricted to a given finite set of relations    called a constraint language  CITATION
	for example  when   is a constraint language over the boolean set   with four ternary predicates            we obtain  NUMBER  sat
	this direction of research has been mainly concerned with the computational complexity of   as a function of
	it has been shown that the complexity of   is highly connected with relational clones of universal algebra  CITATION
	for every constraint language    it has been conjectured that   is either in p or np complete  CITATION
	in the minimum cost homomorphism problem      we are given variables subject to constraints and  additionally  costs on variable value pairs
	now  the task is not just to find any satisfying assignment to the variables  but one that minimizes the total cost
	was introduced in  CITATION  where it was motivated by a real world problem in defence logistics
	the question for which directed graphs   the problem   is polynomial time solvable was considered in  CITATION
	in this paper  we approach the problem in its most general form by algebraic methods and give a complete algebraic characterization of tractable constraint languages
	from this characterization  we obtain a dichotomy for    i e   if   is not polynomial time solvable  then it is np hard
	of course  this dichotomy implies the dichotomy for directed graphs
	in section  NUMBER   we present some preliminaries together with results connecting the complexity of   with conservative algebras
	the main dichotomy theorem is stated in section  NUMBER  and its proof is divided into several parts which can be found in sections  NUMBER   NUMBER 
	the np hardness results are collected in section  NUMBER  followed by the building blocks for the tractability result  existence of majority polymorphisms  section  NUMBER   and connections with optimization in perfect graphs  section  NUMBER  
	section  NUMBER  introduces the concept of  arithmetical deadlocks  which lay the foundation for the final proof in section  NUMBER 
	in section  NUMBER  we reformulate our main result in terms of relational clones
	finally  in section  NUMBER  we explain the relation of our results to previous research and present directions for future research
 Regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems
 Bundle methods for regularized risk minimization (BMRM) and the closely related SVMStruct are considered the best general purpose solvers to tackle this problem
 It was recently shown that BMRM requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution
 In the first part of the paper we use the Hadamard matrix to construct a regularized risk minimization problem and show that these rates cannot be improved
 We then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an  SYMBOL  accurate solution in  SYMBOL  iterations
 Let  SYMBOL  denote samples and  SYMBOL  be the corresponding labels
 Given a training set of  SYMBOL  sample label pairs  SYMBOL , drawn iid from a joint probability distribution on  SYMBOL , many machine learning algorithms solve the following regularized risk minimization problem:     
 Here  SYMBOL  denotes the loss on instance  SYMBOL  using the current model  SYMBOL  and  SYMBOL , the empirical risk, is the average loss on the training set
 The regularizer  SYMBOL  acts as a penalty on the complexity of the classifier and prevents overfitting
 Usually the loss is convex in  SYMBOL  but can be nonsmooth while the regularizer is usually a smooth strongly convex function
 Binary Support Vector Machines (SVMs) are a prototypical example of such regularized risk minimization problems where  SYMBOL  and the loss considered is the binary hinge loss:    
 Recently, a number of solvers have been proposed for the regularized risk minimization problem
 The first and perhaps the best known solver is SVMStruct  CITATION , which was shown to converge in  SYMBOL  iterations to an  SYMBOL  accurate solution
 The convergence analysis of SVMStruct was improved to  SYMBOL  iterations by  CITATION
 In fact,  CITATION  showed that their convergence analysis holds for a more general solver than SVMStruct namely BMRM (Bundle method for regularized risk minimization)
 At every iteration BMRM replaces  SYMBOL  by a piecewise linear lower bound  SYMBOL  and optimizes    to obtain the next iterate  SYMBOL
 Here  SYMBOL  denotes an arbitrary subgradient of  SYMBOL  at  SYMBOL  (see Section ) and  SYMBOL
 The piecewise linear lower bound is successively tightened until the gap     falls below a predefined tolerance  SYMBOL
 Even though BMRM solves an expensive optimization problem at every iteration, the convergence analysis only uses a simple one-dimensional line search to bound the decrease in  SYMBOL
 Furthermore, the empirical convergence behavior of BMRM is much better than the theoretically predicted rates on a number of real life problems
 It was therefore conjectured that the rates of convergence of BMRM could be improved
 In this paper we answer this question in the negative by explicitly constructing a regularized risk minimization problem for which BMRM takes at least  SYMBOL  iterations
 One possible way to circumvent the  SYMBOL  lower bound is to solve the problem in the dual
 Using a very old result of Nesterov  CITATION  we obtain an algorithm for SVMs which only requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution; each iteration of the algorithm requires  SYMBOL  work
 Although we primarily focus on the regularized risk minimization with the binary hinge loss, our algorithm can also be used whenever the empirical risk is piecewise linear and contains a small number of pieces
 Examples of this include multiclass, multi-label, and ordinal regression hinge loss and other related losses### abstract ###
 The influence of lipid molecules on the aggregation of a highly amyloidogenic segment of human islet amyloid polypeptide, hIAPP20 29, and the corresponding sequence from rat has been studied by all-atom replica exchange molecular dynamics simulations with explicit solvent model.
 hIAPP20 29 fragments aggregate into partially ordered -sheet oligomers and then undergo large conformational reorganization and convert into parallel/antiparallel -sheet oligomers in mixed in-register and out-of-register patterns.
 The hydrophobic interaction between lipid tails and residues at positions 23 25 is found to stabilize the ordered -sheet structure, indicating a catalysis role of lipid molecules in hIAPP20 29 self-assembly.
 The rat IAPP variants with three proline residues maintain unstructured micelle-like oligomers, which is consistent with non-amyloidogenic behavior observed in experimental studies.
 Our study provides the atomic resolution descriptions of the catalytic function of lipid molecules on the aggregation of IAPP peptides.
 A range of human diseases including Alzheimer's disease, Parkinson's disease, the spongiform encephalopathy and type 2 diabetes mellitus is associated with amyloid deposits of normally soluble proteins or peptides CITATION CITATION.
 In T2DM, the main protein component of fibrillar protein deposits in the pancreatic islets of langerhans has been identified as a 37-residue hormone referred to as islet amyloid polypeptide or amylin CITATION, which is synthesized in -cells of the pancreas and cosecreted with insulin CITATION, CITATION.
 There are convincing evidences that the toxicity of amyloid related diseases may be caused by the soluble intermediate oligomers instead of mature fibrils CITATION CITATION, and the interaction between lipid bilayer and these soluble oligomer CITATION CITATION.
 For example, channel-like annular structures of oligomers of several amyloidogenic peptides have been observed on the lipid membrane CITATION, CITATION, and have been studied by molecular dynamics simulations as well CITATION, CITATION.
 Moreover, up to 10 percent components in amyloid deposits from patient tissues were lipid molecules, indicating that the lipids can be uptaken from membranes and then wrapped into fibrillar amyloid CITATION CITATION.
 Most studies so far treated the lipid bilayer as a template to exert its influences on the conformation and aggregation properties of peptides CITATION CITATION.
 There is, however, missing information about how individual lipid molecule involving in the peptide aggregation process.
 It will then be beneficial to understand the molecular details of how single lipid molecule influences the assembly process of amyloidogenic peptides which is the main focus of the current study.
 Besides the external factors, such as lipid bilayer, pH value, the sequences of peptide themselves have great effects on the aggregation behaviors.
 Several other species such as non-human primates CITATION, cats CITATION, raccoons CITATION, and rodent species can produce IAPP, but the primary sequence of IAPP varies slightly among species.
 Importantly, IAPP from rodent species, such as rat/mouse IAPP lose capacities of aggregating into amyloid fibrils CITATION, but transgenic mouse models that express human IAPP develop islet deposits CITATION.
 The rIAPP differs from hIAPP in six amino acids and five of them are clustered in a short decapeptide, which is considered to be strongly amyloidogenic and forms similar unbranched fibrils itself to the full-length hIAPP CITATION, CITATION.
 The three proline substitutions in rIAPP20 29 are believed to be highly responsible for the lacking of the amyloidogenic property of the segment or full-length peptide CITATION.
 Although rIAPP has been intensively applied in experimental research acting as a potential peptide inhibitor for peptide aggregation CITATION, CITATION, the molecular mechanism of its resistance to amyloid is still not crystal clear.
 Here, the aggregation of rIAPP20 29 segments is subjected to the same simulation condition as hIAPP20 29 to explore the non-amyloidogenic properties of the peptide and meanwhile to evaluate the simulation results as a negative control.
 Due to the metastable and short-lived nature of soluble pre-fibril oligomers at the early steps of fibril formation, experimental data are usually difficult to obtain CITATION, CITATION.
 Thus, the computational approaches have been employed to complement experimental investigations to gain the insight into the aggregation mechanisms CITATION CITATION.
 Considering multiple copies of peptides needed due to the self-assembly nature of amyloid formation, various simplified representations of molecular systems using implicit solvent models were preferred rather than all-atom models.
 Santini et al. performed ART-OPEP simulations on trimer of A 16 22 by treating side chains as a bead and solvent implicitly CITATION.
 A novel mechanism for single -strand to surmount unnatural registry without dissociation, referred to as reptation was proposed before experimental characterization CITATION.
 Cheon et al. used ProFASi package to reduce the bonded potential energy to include torsional angles only and treated hydrogen bonds explicitly CITATION.
 They were able to carry out two series of 100 Monte Carlo simulations on 20 copies of two fragments A 16 22 and A 25 35.
 They observed early-stage events and obtained an atomic-detailed description of nucleated conformational conversion CITATION model for amyloid aggregation.
 In these studies, simulations were usually started with randomly oriented, extended or random-coiled peptides which underwent ab initio folding to form -sheet oligomers.
 Albeit simplified models allow studying large-scale systems CITATION or observing more events in limited simulation time CITATION, all-atom explicit solvent models can reproduce amyloid aggregation in aqueous environment more accurately and supply more information on sidechain contacts CITATION.
 Nguyen et al. prolonged a series of conventional MD simulations to 300 ns on A 16 22 of 3 6 oligomer size with explicit solvent CITATION.
 The extensive simulations were able to probe the interpeptide sidechain contacts and large conformational fluctuations upon monomer addition to preformed -sheet oligomers in a dock-lock mechanism.
 In our studies, an enhanced-sampling method, replica exchange molecular dynamics CITATION was implemented CITATION, and all water and peptide atoms are treated explicitly by applying OPLS-AA force field CITATION.
 The four copies of amyloidogenic segment hIAPP20 29 and an extra dioleoylphosphatidylcholine lipid molecule were initially set in extended conformation and dispersed in simulation boxes.
 The formation of -sheet containing tetramers, was observed within 100 ns ab initio REMD folding simulations.
 The acquirement of abundant intermediate states suggested two possible -sheet transition pathways.
 Simulation of four hIAPP peptides without lipid molecule was also performed.
 Nonamyloidogenic rat IAPP segments were studied as a negative control with the aim of understanding the inhibitory effect of three proline substitutions.
 this paper extends previous research showing that experienced difficulty of recall can influence evaluative judgments CITATION to a field study of university students rating a course
 students completed a mid-course evaluation form in which they were asked to list either  NUMBER  ways in which the course could be improved a relatively easy task or  NUMBER  ways in which the course could be improved a relatively difficult task
 respondents who had been asked for  NUMBER  critical comments subsequently rated the course more favorably than respondents who had been asked for  NUMBER  critical comments
 an internal analysis suggests that the number of critiques solicited provides a frame against which accessibility of instances is evaluated
 the paper concludes with a discussion of implications of the present results and possible directions for future research
 according to tversky and kahneman's  CITATION  availability heuristic  people sometimes judge the frequency of events in the world by the ease with which examples come to mind
 this process has generally been demonstrated by asking participants to assess the relative likelihood of two categories in which instances of the first category are more difficult to recall than instances of the second category  despite the fact that instances of the first category are more common in the world
 for instance  kahneman and tversky  CITATION  found that most people think the letter r more often appears in english words as the first letter than the third letter  presumably because the first letter provides a better cue for recalling instances of words than does the third letter
 in fact  it turns out that r appears more often as the third than first letter in english words
 schwarz et al CITATION  observed that the classic studies demonstrating the availability heuristic failed to distinguish an interpretation based on ease of retrieval from an alternative interpretation based on content of retrieval in which an event is judged more common when a larger number of examples come to mind
 to tease apart these accounts  schwarz et al CITATION  asked participants in one   study to list either  NUMBER  or  NUMBER  examples of assertive or unassertive behavior that they have exhibited and then rate themselves on their overall degree of assertiveness
 participants rated themselves as more assertive after they had listed  NUMBER  examples of assertive behavior a relatively easy task rather than  NUMBER  examples a relatively difficult task  similarly  they rated themselves as less assertive i e   more unassertive after they had listed  NUMBER  rather than  NUMBER  examples of unassertive behavior
 similar patterns of results have been observed in many other studies of frequency-related judgments  including the rate at which a particular letter occurs in various positions of words  CITATION   the quality of one's own memory  CITATION   the frequency of one's own past behaviors  CITATION   one's susceptibility to heart disease  CITATION  and one's susceptibility to sexual assault  CITATION
 for a review of this literature see schwarz  CITATION
 thus  an abundance of data supports the original interpretation of the availability heuristic  categories are judged to be more common when instances more easily come to mind  even when a smaller absolute number of instances are generated
 this program has been extended from frequency-based judgments to  evaluative judgments of such targets as public transportation  CITATION   luxury automobiles  CITATION   and one's own childhood  CITATION
 for instance  winkielman and schwarz  CITATION  asked participants to recall either  NUMBER  childhood events an easy task or  NUMBER  childhood events a difficult task
 some participants were then led to believe that memories from pleasant periods tend to fade  while others were led to believe that memories from unpleasant periods tend to fade
 when later asked to evaluate their childhood  participants believed that pleasant memories fade rated their childhood more favorably when they completed the difficult task  NUMBER  events than the easy task  NUMBER  events  participants who believed that unpleasant memories fade rated their childhood more favorably when they completed the easy rather than difficult task
 previous studies of the availability heuristic using the paradigm of schwarz et al CITATION  have turned up impressive and robust results
 however  these demonstrations have been restricted primarily to laboratory surveys in which task of recalling examples then making an overall assessment may seem somewhat artificial to participants and the responses of little consequence
 more important  most participants in previous studies presumably had little prior experience with the particular likert scale that served as the dependent measure e g   most had never before rated their childhood or public transportation on a  NUMBER -point scale
 hence  ratings of respondents may be especially susceptible to superficial cues-such as the accessibility of instances-when mapping their beliefs and attitudes onto an unfamiliar response scale
 the present investigation overcomes these limitations through a  field study  of students evaluating a course
 first  evaluations are a normal facet of most university courses in which students are commonly asked to list specific suggestions and also provide a global assessment
 moreover  course evaluations are consequential  as they can influence future course offerings and course staffing  promotion and tenure decisions  and provide information to future prospective students of the target course
 second  students at universities quickly become familiar with standard course evaluation scales and how ratings are distributed across classes  often relying on these scores in choosing among elective courses
 the study of course evaluations is also interesting in its own right
 a number of recent papers have questioned the validity of these ratings  and a lively debate appeared some years ago in the american psychologist  CITATION
 thus far  questions of discriminant validity have mainly focused on the correlation between teaching ratings and apparently irrelevant factors such as the students' expected grades or the course workload
 to date there have been few published investigations of the relationship between the design of course feedback forms and summary course evaluations
 the present study attempts to answer the following provocative question  can one paradoxically obtain higher course ratings by soliciting a greater number of critical comments from students
 In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model
 Using the exactness of the averages for belief propagation for Gaussian models, a  different way of obtaining the covariances is found,  based on Belief Propagation on cavity graphs
 We discuss the relation of this  loop correction algorithm to Expectation Propagation  algorithms for the case in which the model is no longer  Gaussian, but slightly perturbed by nonlinear terms
 Message passing techniques in graphical models allow for the computation of (approximate)  marginal probabilities in a time interval scaling polynomially in the  model size
 Their discovery has consequently revolutionized several  fields of applications in the past years, of which error correcting codes and vision are probably the most prominent examples
 In many cases, the corresponding graphs are loopy, implying either that the error resulting from the application of loopy belief propagation (BP) is negligible for the particular model, or it  can be tolerated for the particular purpose BP serves
 In other cases  more sophisticated refinements of BP are necessary, taking into account (part of) the loop errors
 Finding the optimal treatment of these ``loop errors''  motivates an active field of research, in which  different solutions applying to different model classes are developed
 For models involving many short loops,  like on regular lattices, CVM type approaches work well  CITATION , or tree EP approaches  CITATION
 The latter may also be  applied to correct for an incidental large loop
 Unifying frameworks like the Region graphs of  CITATION   lead to general strategies for selecting the basic clusters underlying such approaches for general model classes
 A recent analysis has shown that the local update equations of BP may be interpreted as the zero order term of an expansion in ``cavity connected correlations''
 These quantities are parameterizations of the ``cavity distributions'',  i e , the  distribution over neighbor variables of a central variable which has been removed from the graph
 The Bethe approximation and BP are recovered when this  cavity distribution is assumed to factorize, whereas the first order correction to the local update equations is obtained when one takes into account the pair cumulants  CITATION
 Estimation of these pair cumulants is possible with extra runs of BP, allowing for new polynomial time algorithms, reducing errors to order  SYMBOL   when applying algorithms of which running time scales with an extra factor  of  SYMBOL   CITATION
 Although this scaling seems heavy, the large benefit of the approach is that it does not require selection of basic clusters or underlying  tree-structures, since it takes into account the effect of all loops that contribute to nontrivial correlations in the cavity distribution at once
 The above ``loop correction''  strategy is applicable in the class of models where a perturbative expansion around the Bethe approximation makes sense, i e , in models with large loops and relatively weak interactions
 The principal requirement  is that the magnitude of pair variable cumulants of cavity distributions is an order smaller than the  single variable cumulants, and third order cumulants are even smaller, etc
 However, heuristics based on the strategy allow for other good algorithms  performing well outside these parameter regimes  CITATION
 So far the approach has been developed for discrete variable models on a more abstract  CITATION  versus practical level  CITATION
 In this paper we apply the idea to graphical models for continuous variables
 We derive the loop corrected belief propagation equations  for simple tractable Gaussian models,  yielding a message passing scheme that, besides the correct average marginals, also yields the correct variances
 Besides that we discuss some approaches potentially  applicable to cases in which extra function approximations are necessary,  and the relation with expectation propagation
 A by-product of our loop corrected belief propagation equations is an algorithm that calculates exact covariance matrices for Gaussian models like the one discussed in  CITATION , but without explicitly using linear response### abstract ###
 this study was designed to assess sex-related differences in the selection of an appropriate strategy when facing novelty
 a simple visuo-spatial task was used to investigate exploratory behavior as a specific response to novelty
 the exploration task was followed by a visual discrimination task  and the responses were analyzed using signal detection theory
 during exploration women selected a local searching strategy in which the metric distance between what is already known and what is unknown was reduced  whereas men adopted a global strategy based on an approximately uniform distribution of choices
 women's exploratory behavior gives rise to a notion of a secure base warranting a sense of safety while men's behavior does not appear to be influenced by risk
 this sex-related difference was interpreted as a difference in beliefs concerning the likelihood of uncertain events influencing risk evaluation
 males and females seem to differ in spatial abilities and styles  CITATION
 generally  studies involving navigational problems showed that female cognitive style relies more on detailed information  while male style relies more on global information  CITATION
 evolutionary mechanisms could potentially account for sex differences in spatial behavior
 for example  these behavioral differences may be due to mating patterns that induced a selection of large-range navigation in males  CITATION
 mating patterns or mating strategies are linked to the dynamics of reproduction and sexual selection  CITATION   and sexual selection is restricted to characteristics that influence mate choice and competition for mates
 typically  males have to compete through extensive ranging for access to mates while females have to choose mating partners according to reproductive success  CITATION
 another proposition  but exclusively directed at humans  suggested that the division of labor game hunting and plant gathering would have put greater selection pressure on females' spatial memory because females sustained gathering duties  CITATION
 however  as argued by ecuyer-dab and robert  CITATION    the selection of male characteristics depends on females' choice for mates
 in females  however  spatial cognition would have been primarily shaped by the natural selection of a strong concern for survival both of self and of offspring
 this concern would have compelled them to favor low-risk strategies  like concentrating on proximal spatial cues  when coping with space-related problems
 such focusing would have enabled secure navigation based on detailed landmark encoding  as well as  in certain species  regular feeding based on remembering the exact locations of potential resources  p  NUMBER 
 thus  the hypothesis of labor division would be a by-product of sexual selection and not the cause of sex differences in spatial behavior
 taken together  the literature seems to indicate that the key to understanding the evolution of behavioral sex differences relies on the relative costs and benefits of producing offspring  CITATION
 in that context spatial skills play a crucial role since they increase reproductive success and the accessibility to food resource but  at the same time  multiply the risks of getting lost  being killed or consumed by other animals predation
 hence  the survival of mobile species depends on their ability to balance costs and benefits induced by locomotion and this balancing should differ according to sex
 experimental investigations of sex differences in spatial abilities yield apparently disparate results  CITATION
 this might be partly due to the complexity of contemporary experimental designs  but also to a lack of investigations concerning decision-making processes involved in the selection of strategies
 the current study investigates sex differences in basic behaviors like exploration  detection and discrimination involving the selection of strategies when coping with uncertainty
 following the above quotation from ecuyer-dab and robert's  the hypothesis is that women  compared to men  should favor low-risk strategies when coping with space-related problems
 to test this  i used a simple spontaneous two-dimensional exploratory task
 this choice relies on the fact that exploration is a natural behavior and that it is fundamental in acquiring spatial knowledge
 it seems to be based on driving factors such as curiosity  comfort or mastery over one's environment
 moreover  it is commonly defined as serving to reduce uncertainty and thus allow coping with fear  CITATION
 exploration is mainly characterized by a succession of progressions and stops  CITATION   and the selection of exploration could rely on its capacity to act as a regulator of uncertainty
 indeed  progressions are based on decisions taken during stops  and stops correspond to choice points allowing decisions
 voss  CITATION  refers to the exploration process as the generation and testing of hypotheses concerning the object's meaning and potential use
 in order to assess risk-taking  a classical visual discrimination task based on the stimuli observed during the exploration task was used
 the results were analyzed with signal detection theory
	The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied
	If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence speed
	For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures
	We show that this is even the case if the model class contains only Bernoulli distributions
	We derive a new upper bound on the prediction error for countable Bernoulli classes
	This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes
	We discuss the application to Machine Learning tasks such as classification and hypothesis testing, and generalization to countable classes of iid models
	``Bayes mixture", ``Solomonoff induction", ``marginalization", all these terms refer to a central induction principle: Obtain a predictive distribution by integrating the product of prior and evidence over the model class
	In many cases however, the Bayes mixture is computationally infeasible, and even a sophisticated approximation is expensive
	The MDL or MAP (maximum a posteriori) estimator is both a common approximation for the Bayes mixture and interesting for its own sake: Use the model with the largest product of prior and evidence
	In practice, the MDL estimator is usually being approximated too, since only a local maximum is determined
	How good are the predictions by Bayes mixtures and MDL
	This question has attracted much attention
	In many cases, an important quality measure is the  total  or cumulative  expected loss  of a predictor
	In particular the square loss is often considered
	Assume that the outcome space is finite, and the model class is continuously parameterized
	Then for Bayes mixture prediction, the cumulative expected square loss is usually small but unbounded, growing with  SYMBOL , where  SYMBOL  is the sample size  CITATION
	This corresponds to an  instantaneous  loss bound of  SYMBOL
	For the MDL predictor, the losses behave similarly  CITATION  under appropriate conditions, in particular with a specific prior
	Note that in order to do MDL for continuous model classes, one needs to  discretize  the parameter space, see also  CITATION
	On the other hand, if the model class is discrete, then Solomonoff's theorem  CITATION  bounds the cumulative expected square loss for the Bayes mixture predictions finitely, namely by  SYMBOL , where  SYMBOL  is the prior weight of the ``true" model  SYMBOL
	The only necessary assumption is that the true distribution  SYMBOL  is contained in the model class, ie that we are dealing with  proper learning
	It has been demonstrated  CITATION , that for both Bayes mixture and MDL, the proper learning assumption can be essential: If it is violated, then learning may fail very badly
	For MDL predictions in the proper learning case, it has been shown  CITATION  that a bound of  SYMBOL  holds
	This bound is exponentially larger than the Solomonoff bound, and it is sharp in general
	A finite bound on the total expected square loss is particularly interesting:   It implies convergence of the predictive to the true probabilities with probability one
	In contrast, an instantaneous loss bound of  SYMBOL  implies only convergence in probability
	Additionally, it gives a  convergence speed , in the sense that errors of a certain magnitude cannot occur too often
	So for both, Bayes mixtures and MDL, convergence with probability one holds, while the convergence speed is exponentially worse for MDL compared to the Bayes mixture
	We avoid the term ``convergence rate" here, since the order of convergence is identical in both cases
	It is eg  SYMBOL  if we additionally assume that the error is monotonically decreasing, which is not necessarily true in general)
	It is therefore natural to ask if there are model classes where the cumulative loss of MDL is comparable to that of Bayes mixture predictions
	In the present work, we concentrate on the simplest possible stochastic case, namely discrete Bernoulli classes
	Note that then the MDL ``predictor" just becomes an estimator, in that it estimates the true parameter and directly uses that for prediction
	Nevertheless, for consistency of terminology, we keep the term predictor
	It might be surprising to discover that in general the cumulative loss is still exponential
	On the other hand, we will give mild conditions on the prior guaranteeing a small bound
	Moreover, it is well-known that the instantaneous square loss of the Maximum Likelihood estimator decays as  SYMBOL  in the Bernoulli case
	The same holds for MDL, as we will see
	If convergence speed is measured in terms of instantaneous losses, then much more general statements are possible  CITATION , this is briefly discussed in Section  
	A particular motivation to consider discrete model classes arises in Algorithmic Information Theory
	From a computational point of view, the largest relevant model class is the class of all computable models on some fixed universal Turing machine, precisely prefix machine  CITATION
	Thus each model corresponds to a program, and there are countably many programs
	Moreover, the models are stochastic, precisely they are  semimeasures  on strings (programs need not halt, otherwise the models were even measures)
	Each model has a natural description length, namely the length of the corresponding program
	If we agree that programs are binary strings, then a prior is defined by two to the negative description length
	By the Kraft inequality, the priors sum up to at most one
	Also the Bernoulli case can be studied in the view of Algorithmic Information Theory
	We call this the  universal setup : Given a universal Turing machine, the related class of Bernoulli distributions is isomorphic to the countable set of computable reals in  SYMBOL
	The description length  SYMBOL  of a parameter  SYMBOL  is then given by the length of its shortest program
	A prior weight may then be defined by  SYMBOL
	If a string  SYMBOL  is generated by a Bernoulli distribution with computable parameter  SYMBOL , then with high probability the two-part complexity of  SYMBOL  with respect to the Bernoulli class does not exceed its algorithmic complexity by more than a constant, as shown by Vovk  CITATION
	That is, the two-part complexity with respect to the Bernoulli class  is  the shortest description, save for an additive constant
	Many Machine Learning tasks are or can be reduced to sequence prediction tasks
	An important example is classification
	The task of classifying a new instance  SYMBOL  after having seen (instance,class) pairs  SYMBOL  can be phrased as to predict the continuation of the sequence  SYMBOL
	Typically the (instance,class) pairs are iid
	Cumulative loss bounds for prediction usually generalize to prediction  conditionalized  to some inputs  CITATION
	Then we can solve classification problems in the standard form
	It is not obvious if and how the proofs in this paper can be conditionalized
	Our main tool for obtaining results is the Kullback-Leibler divergence
	Lemmata for this quantity are stated in Section
	Section  shows that the exponential error bound obtained in  CITATION  is sharp in general
	In Section , we give an upper bound on the instantaneous and the cumulative losses
	The latter bound is small eg under certain conditions on the distribution of the weights, this is the subject of Section
	Section  treats the universal setup
	Finally, in Section  we discuss the results and give conclusions### abstract ###
 third-party punishment has recently received attention as an explanation for human altruism
 feelings of anger in response to norm violations are assumed to motivate third-party sanctions, yet there is only sparse and indirect support for this idea
 we investigated the impact of both anger and guilt feelings on third-party sanctions
 in two studies both emotions were independently manipulated
 results show that anger and guilt independently constitute sufficient but not necessary causes of punishment
 low levels of punishment are observed only when neither emotion is elicited
 we discuss the implications of these findings for the functions of altruistic sanctions
 people often defend the interests of others
 they stand up for their friends if someone speaks ill about them in their absence
 they do not tolerate a colleague being bullied at work
 they boycott consumer products that are produced using child labor
 some even come to the aid of a stranger who is being physically harassed, in spite of obvious personal danger
 in general, people retaliate against injustice even if they are not directly victimized
 sanctioning of norm-violations is vital for prosocial behavior to be sustained  CITATION
 however, punishing norm-violations is costly in terms of time and energy
 it may even impose physical risks
 punishing injustice is therefore considered to be a moral act, particularly when it is performed on behalf of others  CITATION
 this begs the question of what incites third-party sanctions, as they usually oppose self-interest### abstract ###
	A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model.
	The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids.
	Our previous studies have dealt with 7510 proteins of no more than 150 amino acids.
	The proteins are ranked according to the strength of the resistance.
	Most of the predicted top-strength proteins have not yet been studied experimentally.
	Architectures and folds which are likely to yield large forces are identified.
	New types of potent force clamps are discovered.
	They involve disulphide bridges and, in particular, cysteine slipknots.
	An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds.
	These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications.
	A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known.
	This class is characterized through molecular dynamics simulations.
	Atomic force microscopy, optical tweezers, and other tools of nanotechnology have enabled induction and monitoring of large conformational changes in biomolecules.
	Such studies are performed to assess structure of the biomolecules, their elastic properties, and ability to act as nanomachines in a cell.
	Stretching studies of proteins CITATION are of a particular current interest and they have been performed for under a hundred of systems.
	Interpretation of some of these experiments has been helped by all-atom simulations, such as reported in refs. CITATION, CITATION.
	They are limited by of order 100 ns time scales and thus require using unrealistically large constant pulling speeds.
	However, they often elucidate the nature of the force clamp the region responsible for the largest force of resistance to pulling, FORMULA.
	All of the experimental and all-atom simulational studies address merely a tiny fraction of proteins that are stored in the Protein Data Bank CITATION.
	Thus it appears worthwhile to consider a large set of proteins and determine their FORMULA within an approximate model that allows for fast and yet reasonably accurate calculations.
	Structure-based models of proteins, as pioneered by Go and his collaborators CITATION and used in several implementations CITATION CITATION, seem to be suited to this task especially well since they are defined in terms of the native structures away from which stretching is imposed.
	There are many ways, all phenomenological, to construct a structure-based model of a protein.
	504 of possible variants are enumerated and 62 are studied in details in ref. CITATION.
	The variants differ by the choice of effective potentials, nature of the local backbone stiffness, energy-related parameters, and of the coarse-grained degrees of freedom.
	The most crucial choice relates to making a decision about which interactions between amino acids count as native contacts.
	Comparing FORMULA to the corresponding experimental values in 36 available cases selects several optimal models CITATION.
	Among them, there is one which is very simple and which describes a protein in terms of its FORMULA atoms, as labeled by the sequential index FORMULA.
	This model is denoted by FORMULA which stands for, respectively, the Lennard-Jones native contact potentials, local backbone stiffness represented by harmonic terms that favor the native values of local chiralities, the contact map in which there are no FORMULA contacts, and the amplitude of the Lennard-Jones potential, FORMULA, is uniform.
	The contact map is determined by assigning the van der Waals spheres to the heavy atoms and by checking whether spheres belonging to different amino acids overlap in the native state CITATION, CITATION.
	If they do, a contact is declared as native.
	Non-native contacts are considered repulsive.
	Application of this criterion frequently selects the FORMULA contacts as native.
	If the contact map includes these contacts the resulting model will be denoted here as FORMULA.
	On average, it performs worse than FORMULA because the FORMULA contacts usually correspond to the weak van der Waals couplings as can be demonstrated in a sample of proteins by using a software CITATION which analyses atomic configurations from the chemical perspective on molecular bonds.
	Thus the FORMULA couplings should better be removed from the contact map .
	The survey to determine FORMULA in 7510 model proteins with the number of amino acids, FORMULA, not exceeding 150 and 239 longer proteins has been accomplished twice.
	First within the FORMULA model CITATION and soon afterwords within the FORMULA model CITATION.
	The first survey also comes with many details of the methodology whereas the second just presents the outcomes.
	The two surveys are compared in more details in refs. CITATION, CITATION.
	The results differ, particularly when it comes to ranking of the proteins according to the value of FORMULA, but they mutually provide the error bars on the findings.
	They both agree, however, on predicting that there are many proteins whose strength should be considerably larger than the frequently studied benchmark the sarcomere protein titin.
	Near the top of the list, there is the scaffoldin protein c7A which has been recently measured to have FORMULA of about 480 pN CITATION.
	Other findings include establishing correlations with the CATH hierarchical classification scheme CITATION, CITATION, such as that there are no strong FORMULA proteins, and identification of several types of the force clamps.
	The large forces most commonly originate in parallel FORMULA that are sheared CITATION.
	However, there are also clamps with antiparallel FORMULA, unstructured strands, and other kinds.
	The two surveys have been based on the structure download made on July 26, 2005 when the PDB comprised 29 385 entries.
	Many of them correspond to nucleic acids, complexes with nucleic acids and with other proteins, carbohydrates, or come with incomplete files and hence the much smaller number of proteins that could be used in the molecular dynamics studies.
	Here, we present results of still another survey which is based on a download of December 18, 2008 which contains 54 807 structure files and leads to 17 134 acceptable structures with FORMULA not exceeding 250.
	These structures are then analyzed through simulations based on the FORMULA model.
	The numerical code has been improved to allow for acceleration of calculations by a factor of 2.
	The 190 structures with the top values of FORMULA in units of FORMULA are shown in Table 1 and Table S1 of the SI, together with the values of titin and ubiquitin to provide a scale.
	As argued in the Materials and Methods section section, the unit of force, FORMULA, is now estimated to be of order 110 pN.
	All of the corresponding proteins are predicted to be much stronger than titin and none but two of them have been studied experimentally yet.
	In addition to the types of force clamps identified before, we have discovered two new mechanisms of sturdiness.
	One of them involves a cysteine slipknot and is found to be operational in all of the 13 top strength proteins.
	In this motif, a slip-loop is pulled out of a cysteine knot-loop.
	Another involves dragging of a single fragment of the main chain across a cysteine knot-loop.
	The two mechanisms are similar in spirit since both involve dragging of the backbone.
	However, in the CSK case, two fragments of the backbone are participating.
	We make a more systematic identification of the CATH-classified architectures that are linked to mechanical strength and then analyze correlations of the data to the SCOP-based grouping CITATION CITATION.
	The previous surveys did not relate to the SCOP scheme.
	We identify the CATH-based architectures and SCOP-based folds that are associated with the occurrence of a strong resistance to pulling.
	A general observation, however, is that each such group of structures may also include examples of proteins that unravel easily.
	The dynamics of a protein are very sensitive to mechanical details that are largely captured by the contact map and not just by the appearance of a structure.
	On the other hand, if one were to look for mechanically strong proteins then the architectures and folds identified by us should provide a good starting point.
	We also study the dependence of FORMULA on the pulling velocity and characterize the dependence on FORMULA through distributions of the forces.
	The current third survey has been performed within the same FORMULA model as the second survey CITATION.
	However, we reuse and extend it here because the editors of Biophysical Journal retracted the second survey CITATION.
	All of the values of FORMULA are deposited at the website LINK and can by accessed by through the PDB structure code.### abstract ###
	Alternative splicing contributes to both gene regulation and protein diversity.
	To discover broad relationships between regulation of alternative splicing and sequence conservation, we applied a systems approach, using oligonucleotide microarrays designed to capture splicing information across the mouse genome.
	In a set of 22 adult tissues, we observe differential expression of RNA containing at least two alternative splice junctions for about 40 percent of the 6,216 alternative events we could detect.
	Statistical comparisons identify 171 cassette exons whose inclusion or skipping is different in brain relative to other tissues and another 28 exons whose splicing is different in muscle.
	A subset of these exons is associated with unusual blocks of intron sequence whose conservation in vertebrates rivals that of protein-coding exons.
	By focusing on sets of exons with similar regulatory patterns, we have identified new sequence motifs implicated in brain and muscle splicing regulation.
	Of note is a motif that is strikingly similar to the branchpoint consensus but is located downstream of the 5 splice site of exons included in muscle.
	Analysis of three paralogous membrane-associated guanylate kinase genes reveals that each contains a paralogous tissue-regulated exon with a similar tissue inclusion pattern.
	While the intron sequences flanking these exons remain highly conserved among mammalian orthologs, the paralogous flanking intron sequences have diverged considerably, suggesting unusually complex evolution of the regulation of alternative splicing in multigene families.
	Splicing is an essential process that constructs protein coding messenger RNA sequences using tiny segments of information buried in the much larger primary transcripts of the eukaryotic gene.
	Regulated alternative splicing can create different protein coding sequences under different biological circumstances, allowing the production of functionally related but distinct proteins.
	In addition, alternative splicing can mediate the repression of gene expression by stimulating the formation of transcripts subject to nonsense-mediated decay CITATION CITATION.
	Splicing patterns seem distinct in the vertebrate nervous system compared to other tissues CITATION, CITATION, and it is tempting to hypothesize that neural alternative splicing contributed to the rapid evolution of the vertebrate brain without large increases in gene number CITATION .
	Biochemical analysis of alternative splicing has shown that numerous RNA binding proteins influence the use of specific splice sites to stimulate splicing events that lead to particular mRNA isoforms CITATION, CITATION.
	These RNA binding proteins may activate or repress the use of splice sites by binding to nearby sequences in the exon or in the intron.
	In many cases, multiple RNA binding proteins combine to create repressing and activating influences that produce patterns of splicing control CITATION, CITATION.
	Some proteins, such as SR proteins and the CELF proteins, have mostly activating roles, whereas others, such as hnRNP A1, PTB, and nPTB, have mostly repressing roles.
	Certain proteins can either activate or repress splicing in different contexts, depending on the position of their binding sites or the expression of other RNA binding proteins CITATION, CITATION .
	A complete catalog of the RNA sequences corresponding to the enhancers and silencers bound by splicing regulatory proteins would greatly aid the understanding of splicing regulatory networks.
	Thus far, there are only a handful of splicing regulators whose corresponding RNA binding motifs have been identified, whereas there may be many splicing regulators among the hundreds of RNA binding proteins encoded by the mouse genome.
	In addition, several related but distinct genes produce proteins that bind the same or overlapping sets of sequences; for example, Fox-1 and RBM9 each bind UGCAUG CITATION, CITATION, and the branchpoint binding protein SF1 and the protein quaking each bind UACUAAC-like motifs CITATION CITATION.
	Adding to this complexity is the tendency for the mRNAs of RNA binding proteins to be alternatively spliced, leading to multiple RNA binding protein isoforms with potentially different functions.
	Currently, the methods available for expanding the list of known regulators and their target sequences are limited, and the development of this catalog is in the early stage CITATION .
	Much of the available genomic information on alternative splicing is derived by the alignment of large numbers of expressed sequence tags and messenger RNAs to genome sequences.
	The analysis of exons that appear to be constitutive or alternative has led to the successful identification of many distinguishing features of alternatively spliced regions CITATION CITATION, even allowing their accurate prediction without cDNA evidence CITATION, CITATION, CITATION.
	Although cDNA libraries have been invaluable for discovering general features of alternatively spliced exons, it is difficult to connect specific regulatory sequences to specific biological conditions with confidence due to variable and sometimes missing information about the source materials and methods of cDNA library construction.
	The relatively low number of transcripts present from any one gene also makes it difficult to estimate differences in expression levels using library representation as a measure.
	Thus, more direct methods are needed to associate alternative splicing events with underlying biological conditions.
	The recent application of microarray technology to questions of splicing and splicing regulation promises to reveal parallel connections between many splicing events and specific biological or experimental conditions CITATION CITATION.
	Analysis of experimental changes in splicing for many genes simultaneously should reveal biological conditions necessary for proper splicing regulation in a way that analysis of cDNA libraries cannot, and with breadth that cannot be achieved by analysis of a reporter construct or a few endogenous target genes.
	To demonstrate this, we constructed a DNA microarray designed to capture splicing information for about 6,200 alternative events in the mouse transcriptome, using a combination of splice junction and exon probes, and have hybridized RNA from 22 adult mouse tissues.
	We examine splicing in these tissues by asking three questions.
	First we ask, Which RNA isoforms are present in a particular tissue sample?
	To answer this simple question, we used a new method based on comparing the intensity of the probes in a probe set to the distribution of intensities from all probes with similar G C level.
	This is similar in spirit although different in approach to the present-absent calls from Affymetrix MAS 5.0 algorithms CITATION, as this microarray did not contain mismatch probes.
	Using RT-PCR, we show that this method has a true-positive rate of 85 percent.
	Second we ask, Which RNA isoforms are differentially expressed across the tissues examined?
	For each RNA isoform, the intensities of the isoform-specific junction probes were examined across tissues using the Kruskal-Wallis statistical test.
	After correcting for multiple testing, about 40 percent of the 6,216 total alternative splicing events examined were found to have more than one RNA form that was differentially expressed, indicating widespread tissue differences in splicing over the tissues.
	Third we ask, Which cassette exons are included differentially between brain and nonbrain tissues?
	To answer this, we used a regression-based bootstrapping method, which also allows an estimate of the relative change in skipping and inclusion in the two sample groups.
	We analyzed the intron sequences associated with exon skipping events that are differentially regulated in brain or muscle relative to other tissues and found unusual patterns of sequence conservation that provide new information about tissue regulation of alternative splicing and its evolution.### abstract ###
	the paper extends research on fixed-pie perceptions by suggesting that disputants may prefer proposals that are perceived to be equally attractive to both parties i e , balanced rather than one-sided, because balanced agreements are seen as more likely to be successfully implemented
	we test our predictions using data on israeli support for the geneva accords, an agreement for a two state solution negotiated by unofficial delegations of israel and the palestinian authority in 2003
	the results demonstrate that israelis are more likely to support agreements that are seen favorably by other israelis, but - contrary to fixed-pie predictions - israeli support for the accords does not diminish simply because a majority of palestinians favors rather than opposes the accords
	we show that implementation concerns create a demand among israelis for balance in the degree to which each side favors or opposes the agreement
	the effect of balance is noteworthy in that it creates considerable support for proposals even when a majority of israelis and palestinians oppose the deal
	normative models of bargaining and negotiation suggest that if there is potential for mutual benefit, conflicting parties should be able to achieve it  CITATION
	descriptive accounts and empirical investigations of negotiation behavior  CITATION , however, suggest that a number of psychological barriers to conflict resolution are likely to make efficient deal making difficult  CITATION
	for example, research on cognitive biases associated with egocentric perceptions suggests that negotiators and evaluators of negotiated agreements are likely to exhibit a "fixed-pie bias"  CITATION
	the fixed-pie bias refers to the belief that any gain for one party will be associated with an equivalent loss to the other party
	this belief is a "bias" when it persists even in contexts where there is a possibility of compatible interests or mutual benefit
	a large body of research finds that negotiators are susceptible to the fixed-pie bias prior to, during, and even after negotiations  CITATION
	in the current paper we investigate and extend research on fixed pie bias in the context of protracted intergroup conflict
	although the internet as level topology has been extensively studied over the past few years  little is known about the details of the as taxonomy
	an as  node  can represent a wide variety of organizations  e g   large isp  or small private business  university  with vastly different network characteristics  external connectivity patterns  network growth tendencies  and other properties that we can hardly neglect while working on veracious internet representations in simulation environments
	in this paper  we introduce a radically new approach based on machine learning techniques to map all the ases in the internet into a natural as taxonomy
	we successfully classify  NUMBER   NUMBER  percent  of ases with expected accuracy of  NUMBER   NUMBER  percent 
	we release to the community the as level topology dataset augmented with   NUMBER   the as taxonomy information and  NUMBER   the set of as attributes we used to classify ases
	we believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the internet
	the rapid expansion of the internet in the last two decades has produced a large scale system of thousands of diverse  independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments
	from  NUMBER  to  NUMBER  the number of globally routable as identifiers has increased from less than  NUMBER   NUMBER  to more than  NUMBER   NUMBER   exerting significant pressure on interdomain routing as well as other functional and structural parts of the internet
	this impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the internet infrastructure
	in particular  the as level topology is an intermix of networks owned and operated by many different organizations  e g   backbone providers  regional providers  access providers  universities and private companies
	statistical information that faithfully characterizes different as types is on the critical path toward understanding the structure of the internet  as well as for modeling its topology and growth
	in topology modeling  knowledge of as types is mandatory for augmenting synthetically constructed or measured as topologies with realistic intra as and inter as router level topologies
	for example  we expect the network of a dual homed university to be drastically different from that of a dual homed small company
	the university will likely contain dozens of internal routers  thousands of hosts  and many other network elements  switches  servers  firewalls 
	on the other hand  the small company will most probably have a single router and a simple network topology
	since there is such a diversity among different network types  we cannot accurately augment the as level topology with appropriate router level topologies if we cannot characterize the composing ases
	moreover  annotating the ases in the as topology with their types is a prerequisite for modeling the evolution of the internet  since different types of ases exhibit different growth patterns
	for example  internet service providers  isp  grow by attracting new customers and by engaging in business agreements with other isps
	on the other hand  small companies that connect to the internet through one or few isps do not grow significantly over time
	thus  categorizing different types of ases in the internet is necessary to identify network evolution patterns and develop accurate evolution models
	an as taxonomy is also necessary for mapping ip addresses to different types of users
	for example  in traffic analysis studies its often required to distinguish between packets that come from home and business users
	given an as taxonomy  its possible to realize this goal by checking the type of as that originates the prefix in which an ip address lies
	in this work  we introduce a radically new approach based on machine learning to construct a representative as taxonomy
	we develop an algorithm to classify ases based on empirically observed differences between as characteristics
	we use a large set of data from the internet routing registries  irr   CITATION  and from routeviews  CITATION  to identify intrinsic differences between ases of different types
	then  we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ases into six representative classes that reflect ases with different network properties and infrastructures
	we derive macroscopic statistics on the different types of ases in the internet and validate our results using a sample of  NUMBER  manually identified as types
	our validation demonstrates that our classification algorithm achieves high accuracy   NUMBER   NUMBER  percent  of the examined classifications were correct
	finally  we make our results and our classifier publicly available to promote further research and understanding of the internet s structure and evolution
	in section  we start with a brief discussion of related work
	section  describes the data we used  and in section  we specify the set of as classes we use in our experiments
	section  introduces our classification approach and results
	we validate them in section  and conclude in section 
	previous tests of cumulative prospect theory cpt and of the priority heuristic ph found evidence contradicting these two models of risky decision making.
	however, those tests were criticized because they had characteristics that might "trigger" use of other heuristics.
	this paper presents new tests that avoid those characteristics.
	expected values of the gambles are nearly equal in each choice.
	in addition, if a person followed expected value ev, expected utility eu, cpt, or ph in these tests, she would shift her preferences in the same direction as shifts in ev or eu
	in contrast, the transfer of attention exchange model tax and a similarity model predict that people will reverse preferences in the opposite direction
	results contradict the ph, even when ph is modified to include a preliminary similarity evaluation using the ph parameters
	new tests of probability-consequence interaction were also conducted
	strong interactions were observed, contrary to ph
	these results add to the growing bodies of evidence showing that neither cpt nor ph is an accurate description of risky decision making
	this paper compares three models that attempt to describe risky decision making
	these models are cumulative prospect theory cpt  CITATION , birnbaum's  CITATION  transfer of attention exchange model tax, and the priority heuristic ph of brandstatter, gigerenzer, and hertwig  CITATION
	the ph model is based on the idea that people compare one attribute at a time, such as the minimum prizes
	in addition, the similarity model of rubinstein  CITATION  as modified by leland  CITATION  is also relevant to these studies, although these studies were not designed to test that model
	birnbaum  CITATION  reviewed a number of critical tests that refute any rank dependent utility rdu model  CITATION  including rank and sign-dependent utility  CITATION , cpt, and expected utility eu
	birnbaum  CITATION  noted that many of the same tests that refute cpt also contradict the priority heuristic
	for example, the priority heuristic predicted fewer than half of the modal choices analyzed by birnbaum  CITATION , by birnbaum  CITATION , and by birnbaum and navarrete  CITATION
	some of these choices included cases where 90% or more of the participants satisfied stochastic dominance but the priority heuristic predicts indifference
	in other choices, significantly more than half of the participants about 70% of undergraduates violated stochastic dominance, but the priority heuristic predicts that people should satisfy it
	brandstatter, gigerenzer, and hertwig  CITATION  responded that properties of these choices may have induced people to use other heuristics drawn from a person's "adaptive toolbox
	 presumably, decision makers first decide what rule to use, then they either apply that rule or choose to use another rule
	the mechanism that decides what rule to use has not yet been specified; it is described instead with lists of "triggering conditions," which are estimated from data like parameters
	brandstatter et al CITATION  concluded that the priority heuristic does not apply when there is a stochastic dominance relation in the choice
	in addition, brandstatter et al CITATION  argued that certain choices reviewed by birnbaum  CITATION  used gambles that differed in expected value ev
	brandstatter et al CITATION  presented a figure to show that the priority heuristic is not accurate when expected values evs differ, which led them to suppose that two strategies are at work, one for "easy" choices that differ in ev and one for "harder" choices where evs are nearly equal
	from the data, brandstatter et al CITATION  estimated that, when the ratio of ev exceeds 2, people act as if they choose the gamble with the higher ev
	brandstatter et al consider ev ratio as a proxy for the "difficulty" of a choice, but do not necessarily hold that people actually compute ratios of ev
	they argued that the priority heuristic is accurate for "difficult" choices in which evs are nearly equal
	however, birnbaum  CITATION  noted that ev ratios in birnbaum and navarrete  CITATION  had been inside the region where ph is supposed to apply; in that study, the priority heuristic failed to reproduce even half of the modal choices correctly
	brandstatter et al CITATION  replicated part of that study and their results confirmed that the priority heuristic reproduced fewer than half of the modal choices that they chose for replication  CITATION
	to account for the results, brandstatter et al noted that birnbaum and navarrete  CITATION  used many choices in which both gambles of a choice had the same probability distribution and in some choices two branches had the same probability
	ph was not accurate for such choices, so brandstatter, et al CITATION  theorized that people use a "toting up" heuristic for choices in which two branches had the same probability
	in some of the choices in birnbaum and navarrete  CITATION , there was a common probability-consequence branch in both choices, which was theorized to trigger editing rules and other heuristics that were called up to account for the failures of the priority heuristic
	the arguments of brandstatter et al CITATION  might also provide excuses for previous failures of cpt as well
	this paper devises a new type of test that avoids the exceptions stated above
	in these tests, one alternative does not stochastically dominate the other, there are no common probability-consequence branches, probabilities of the consequences are not equal, and expected values are nearly equal
	in addition, unlike previous tests, the new tests use shifts in expected value and expected utility to "help" predictions of ph and cpt
	that is, expected value and expected utility are both manipulated such that, if a person shifts his or her judgments in the same direction as the changes in eu or ev, his or her choices will appear consistent with ph and cpt
	however, the choices are designed so that the tax model with parameters typical of previous research predicts that people will shift their choices in the opposite direction of ev, eu, cpt, and ph### abstract ###
 Numerous psychophysical experiments found that humans preferably rely on a narrow band of spatial frequencies for recognition of face identity.
 A recently conducted theoretical study by the author suggests that this frequency preference reflects an adaptation of the brain's face processing machinery to this specific stimulus class.
 The purpose of the present study is to examine this property in greater detail and to specifically elucidate the implication of internal face features.
 To this end, I parameterized Gabor filters to match the spatial receptive field of contrast sensitive neurons in the primary visual cortex.
 Filter responses to a large number of face images were computed, aligned for internal face features, and response-equalized.
 The results demonstrate that the frequency preference is caused by internal face features.
 Thus, the psychophysically observed human frequency bias for face processing seems to be specifically caused by the intrinsic spatial frequency content of internal face features.
 In the brain, the structure of neuronal circuits for processing sensory information matches the statistical properties of the sensory signals CITATION.
 Taking advantage of these statistical regularities contributes to an optimal encoding of sensory signals in neuronal responses, in the sense that the code conveys the highest information with respect to specific constraints CITATION CITATION.
 Among the various constraints which were formulated we find, for example, keeping metabolic energy consumption as low as possible CITATION CITATION, or keeping total wiring length between processing units at a minimum CITATION, or maximizing the suppression of spatio-temporal redundancy in the input signal CITATION, CITATION CITATION .
 As for visual stimuli, natural images reveal a conspicuous statistical regularity that comes as an approximately linear decrease of their amplitude spectra as a function of spatial frequency CITATION CITATION.
 This means that pairs of luminance values are strongly correlated CITATION, and this property could be exploited for gain controlling of visual neurons.
 Then, visual neurons would have equal sensitivities or response amplitudes independent of their spatial frequency preference CITATION.
 According to this response equalization hypothesis, gain should thus be incremented with increasing spatial frequency, such that the distribution of response amplitudes of frequency-tuned neurons to a typical natural image is flat.
 An argument in favor of employing response equalization is that it would lead to an improvement of information transmission from one neuronal stage to another, because the output of one stage would match the limited dynamic range of a second one CITATION .
 The present article builds upon previously reported results for whitened amplitude spectra of face images CITATION : the whitened spectra reveal a spatial frequency maximum at 10 15 cycles per face, but only if external face features are suppressed.
 The predicted frequency maximum nevertheless agrees well with numerous psychophysical experiments, which found that face identity is preferably processed in a narrow band of spatial frequencies from 8 to 16 cycles per face CITATION CITATION .
 Despite of it all, the results presented in CITATION indicate that the maxima in the amplitude spectra are caused by the compound effect of horizontally oriented internal face features.
 Quantitatively, the maxima thus occur in units of cycles per face height, whereas most psychophysical studies instead measure their results in terms of cycles per face width.
 Furthermore, although a clear enhancement of horizontal amplitudes could be observed in the spectra, horizontal amplitudes showed a somewhat noisy dependence on spatial frequency.
 Both effects are a consequence of that face features were not considered individually, what causes a mixing of the spatial frequency content of individual face features in the spectra.
 The mixing leads to averaging-out effects such that any possible enhancement of spectral amplitudes at other than the horizontal orientation goes unnoticed, but also may cause interference effects which lead to the mentioned noisy dependence of amplitudes on spatial frequency.
 The present study addresses the two issues by means of an extensive analysis of face images by means of Gabor filters.
 The filters were thereby parameterized to match the spatial receptive field of band-limited, oriented and contrast sensitive neurons in the primary visual cortex CITATION CITATION.
 Great care has been taken to guarantee the correct alignment of filter responses with respect to the position of internal face features prior to their averaging.
 Doing so permits to precisely elucidate how the frequency dependence of Gabor responses is related to each of the four internal face features.
 The resulting graphs of whitened Gabor amplitudes versus spatial frequency are smooth and reveal distinct maxima at nearly all orientations.
 The most stable maxima, however, are observed at horizontal feature orientations in the first place, but also at vertical orientations.
 This observation holds true for all of the internal face features.
 The present study therefore shows how the individual internal face features contribute to the psychophysically observed frequency preference, and proposes concrete mechanisms of how higher amplitudes of whitened cell responses at an early level could possibly lead to the psychophysically measured effects.### abstract ###
 we test in the context of a dictator game the proposition that individuals may experience a self-control conflict between the temptation to act selfishly and the better judgment to act pro-socially
 we manipulated the likelihood that individuals would identify self-control conflict, and we measured their trait ability to implement self-control strategies
 our analysis reveals a positive and significant correlation between trait self-control and pro-social behavior in the treatment where we expected a relatively high likelihood of conflict identification-but not in the treatment where we expected a low likelihood
 the magnitude of the effect is of economic significance
 we conclude that subtle cues might prove sufficient to alter individuals' perception of allocation opportunities, thereby prompting individuals to draw on their own cognitive resources to act pro-socially
 lured by temptation, individuals may find themselves acting against their better judgment
 self-control failure, famously termed akrasia in plato's protagoras  CITATION , persists throughout domains of daily life and represents a central issue of both philosophy and modern-day social sciences
 for example, the dieter faced with the opportunity to indulge in a delicious creamy cake may perceive a conflict between indulging and maintaining a good figure
 the student may feel conflicted between the desire to go to the cinema and her better judgment to stay home and study
 and, similarly, the fashionista might feel conflicted between the temptation to purchase new boots and her better judgment to maintain a responsible budget
 perhaps less intuitively, but no less importantly, the question of pro-social versus selfish behavior may be understood in similar terms
 this conceptualization may help reconcile conflicting notions in economics of selfish and pro-social motivations
 that individuals should care much about their own self-interest seems almost tautological and requires little further exposition, but that individuals also should care about the interest of others-at the expense of that of their own-has attracted considerable attention  CITATION
 for example, many individuals voluntarily contribute to charity or to public goods e g , recycling, and they pay their taxes despite low likelihood of punishment for failing to do so
 nonetheless, one could imagine that even individuals of generally pro-social inclination on occasion may feel tempted to act selfishly and hence underreport income to the authorities
 that is, pro-social preferences potentially fly in the face of basic urges for personal gain-or greed-and the individual may thus experience a self-control conflict between better judgment to act pro-socially and the temptation to act selfishly
 self-control-our capacity to overrule temptation-is no less complex than it is important
 a multitude of conceptualizations exist, many of which are complementary
 typically, and in line with classic ideas of the conflict between reason and passion, authors view self-control as a "cold" executive function that guides behavior in the face of "hot" impulses to act against better judgment  CITATION
 willpower, then, represents the combined resources that the executive function-or the planner, in the parlance of thaler and shefrin  CITATION -brings to bear in a deliberate struggle against temptation  CITATION
 such resources may include cognitive strategies to divert attention away from temptation  CITATION , strategies of pre-commitment  CITATION , or possibly the sheer strength of mind to hold back from the song of the sirens
 our conceptualization of self-control mirrors these
 only recently has the psychological literature started to explore how the question of pro-social versus selfish behavior relates to that of self-control
 loewenstein  CITATION  suggests that selfish behavior may be motivated by visceral urges or drive-states, resembling cravings for relief from hunger, pain, and sexual deprivation
 o'donoghue and loewenstein  CITATION  argue that such selfish urges may conflict with the "colder", more abstract preferences for altruism, as visceral urges for sweets may conflict with more abstract preferences for a fine figure or good health
 at present, there is but indirect evidence for this idea
 for example, pronin et al CITATION  show that decisions about others resemble decisions about "future selves", both classes of which contrast to decisions about less abstract "present selves"
 albrecht et al CITATION  report consistent results; individuals who choose between immediate and delayed rewards for themselves exhibit less patience and more affective involvement activation in the dopaminergic reward system than do individuals who make such choices for others-or for themselves in the future
 moreover, curry et al CITATION  find in a standard public goods game that individuals' discount rates are negatively associated with their contributions to the public good
 that is, more "impatient" individuals contributed less to the public good than did "patient" ones
 arriving at similar results, fehr and leibbrandt  CITATION  report that patient vs impatient fishermen, whose time preferences were elicited in the lab, exhibited more cooperative behavior in a common resource problem and were in the field less likely to over-exploit the common pool resource
 furthermore, burks et al CITATION  find that "short-term" patience-the  beta   in the  beta  -&#x3b4;  model-is positively associated with cooperative behavior in a sequential prisoner's dilemma
 however, duffy and smith  CITATION  report no effect of cognitive load-a manipulation intended to deplete cognitive resources and thereby impair self-control-on outcomes across treatments in a repeated multi-player prisoner's dilemma
 an emerging literature on the "default" response in games of trust and reciprocity lends further credence to the notion that altruistic responses require self-control
 achtziger et al CITATION  subjected players in an ultimatum game to cognitive resource depletion, and show that depleted proposers made lower offers-they became less altruistic
 moreover, depleted responders were more likely to reject offers that were unfair to themselves-they exhibited "altruistic punishment"
 halali et al CITATION  report the same for responders, but with a different depletion task
 crockett et al CITATION  subjected responders to acute tryptophan depletion-a procedure that temporarily reduces serotonin levels in the brain and thereby impairs self-control  CITATION ; reduced serotonin levels raised rejection rates and this reduction is positively correlated with impulsive choice in a delay-discounting task  CITATION
 using a trust game, knoch et al CITATION  subjected trustees' right lateral prefrontal cortex to transcranial magnetic stimulation, which reduces functioning in the targeted brain region
 trustees, though cognizant that returning a share of the investments was both strategic and norm-compliant, were unable to do so under impaired executive functioning; self-control seems necessary to act on the better judgment to resist the temptation to keep the received investment entirely for oneself
 closest to our domain of inquiry, piovesan and wengstrom  CITATION  measured response times of participants in a repeated dictator game, lasting 24 periods
 they find both across and within participants that lower response times are associated with more selfish choices
 one interpretation of their results is that the default behavior is to act selfishly and that pro-social behavior requires the successful resolution of a self-control conflict, which slows the response time
 such successful resolution of conflict would require cognitive resources, but hauge et al CITATION  report no effect of cognitive load on players in one-shot dictator games
 in this paper we attempt a more direct test of the hypothesis that pro-social versus selfish behavior may represent a self-control problem
 we employ a standard measure of pure pro-social behavior, the one-shot dictator game, which invokes neither concerns for strategy nor for reciprocity; and a well-grounded psychometric measure of self-control, the rosenbaum self-control schedule  CITATION
 further, we explore the conditions under which we expect an association between self-control and pro-social behavior
 in so doing, we rely on two conditions necessary for successfully exercising restraint in the face of temptation; myrseth and fishbach  CITATION  propose a two-stage model of self-control, which postulates that an individual in the face of temptation first identifies conflict or not between indulging and pursuing a higher-order goal and, second, that the individual next employs self-control strategies if and only if conflict was identified at the first stage see figure 1
 critically, self-control strategies are relevant to the decision to indulge only when the individual has identified self-control conflict
 therefore, one strategy for investigating whether the problem of pro-social versus selfish behavior resembles one of self-control is to test whether the tendency to apply self-control strategies is positively associated with pro-social behavior when individuals have identified self-control conflict, but less so or not at all when individuals have not
 determinants of conflict identification in the face of temptation have been explored only recently
 in some contexts, the question is almost trivial and identification of conflict virtually obvious
 for example, the diabetic dieter probably knows that having even a single, tempting chocolate may incur major costs
 however, the question of self-control conflict is more ambiguous for the non-diabetic dieter, who faces the same chocolate
 having this one chocolate alone will not incur major costs, but doing so regularly might
 similarly, the good citizen may find that a general failure to act generously would represent a major threat to his self-image, but being stingy on just a couple of occasions is a more ambiguous matter
 myrseth and fishbach  CITATION  use the term epsilon cost temptation to denote tempting opportunities that incur nothing but trivial costs when consumed in small amounts, but potentially serious costs when consumed extensively
 they argue that individuals identify self-control conflict in the face of epsilon cost temptation if and only if two conditions are met: a the focal consumption opportunity must be viewed in relation to multiple additional opportunities, and b the decision maker must assume that similar choices are made for each opportunity  CITATION
 that is, considering the question of whether or not to have a delicious creamy cake will evoke self-control conflict in the dieter if the dining opportunity is viewed in relation to future opportunities for dessert consumption, but not if the dining opportunity is viewed in isolation, as a singular episode
 similarly, the question of whether or not to be generous-to donate to a charitable organization-may elicit self-control conflict if the decision is viewed in relation to future decisions, but not if the decision is viewed in isolation
 if viewed in relation to future decisions, the question of how much to donate on a single occasion may have bearing on the decision maker's self-image; donating now-and in the future-indicates a generous character, whereas keeping the money for oneself does not
 however, if viewed in isolation, the question of how much to donate has little bearing on self-image; the present decision of how much to donate is considered only in light of immediate consequences, leaving self-image out of the equation
 because a consistent self-image represents an important motivator for pro-social behavior  CITATION , we expect that individuals are more likely to identify self-control conflict between selfish and pro-social behavior if the allocation decision is seen in relation to future opportunities than if it is seen in isolation
 myrseth and fishbach  CITATION  show that subtle framing manipulations are sufficient to influence identification of self-control conflict in the face of epsilon cost temptation
 they find that presenting a calendar displaying the current month, with a grid separating the dates, raised participants' subsequent consumption of potato chips relative to that of participants whom were presented a calendar without a grid
 they argue that the gridded calendar activated an isolated versus interrelated frame of the choice opportunity; it made participants more likely to isolate the date in question and thus less likely to see the decision task in relation to similar future opportunities
 consequently, the grid reduced the likelihood that participants would identify a conflict between the temptation to have chips and the better judgment to maintain a fine figure and good health
 indeed, participants who viewed the gridded calendar reported experiencing less conflict during their decision to have chips or not than did those who viewed the non-gridded calendar
 furthermore, participants' trait ability to implement self-control strategies, measured by rosenbaum's  CITATION  psychometric scale, was positively associated with chips consumption for those who viewed the calendar without the grid and who were more likely to identify conflict, but not for others who viewed the calendar with and who were less likely to identify conflict
 that is, participants who viewed the calendar without the grid, more likely than those who viewed the calendar with, identified self-control conflict and therefore leveraged their self-control strategies to resist the tempting chips
 to explore our hypothesis that the problem of pro-social versus selfish behavior may represent one of self-control, we have applied the empirical strategy from myrseth and fishbach  CITATION  in the dictator game-a participant is granted an endowment and asked to split it between herself and a recipient  CITATION , and in our case the red cross featured as recipient  CITATION
 the game thus pits pro-social motivations against self-interest
 if pro-social versus selfish behavior could represent a self-control conflict, we would expect participants' trait self-control, as measured by rosenbaum's  CITATION  scale, to correlate positively with pro-social behavior for participants who have just previously viewed a calendar without a grid, but less so or not at all for participants who have viewed a calendar with
 the graph in figure 2 displays donation, as a function of level of self-control, for two different levels of identification likelihood
 in the case of low likelihood, the slope is expected to be weakly positive
 in the case of the higher likelihood, however, the slope is expected to be strictly greater than that in the case of low likelihood
 this means that for a given level of self-control, one might observe substantially different donation behavior depending on whether conflict was identified or not
	The tradeoff between the need to suppress drug-resistant viruses and the problem of treatment toxicity has led to the development of various drug-sparing HIV-1 treatment strategies.
	Here we use a stochastic simulation model for viral dynamics to investigate how the timing and duration of the induction phase of induction maintenance therapies might be optimized.
	Our model suggests that under a variety of biologically plausible conditions, 6 10 mo of induction therapy are needed to achieve durable suppression and maximize the probability of eradicating viruses resistant to the maintenance regimen.
	For induction regimens of more limited duration, a delayed-induction or -intensification period initiated sometime after the start of maintenance therapy appears to be optimal.
	The optimal delay length depends on the fitness of resistant viruses and the rate at which target-cell populations recover after therapy is initiated.
	These observations have implications for both the timing and the kinds of drugs selected for induction maintenance and therapy-intensification strategies.
	The failure of antiretroviral therapies to completely suppress viral replication in some patients represents a major difficulty in the management of HIV infection.
	In therapy-naive patients without clinically apparent resistance mutations, triple-drug therapy with two nucleoside analog reverse transcriptase inhibitors and a protease inhibitor or a non-nucleoside reverse transcriptase inhibitor is standard CITATION.
	In these patients, treatment success rates, defined as viral load 50 copies/ml at 48 wk, range from 70 percent to 80 percent 85 percent.
	However, in patients with previous regimen failure requiring salvage therapy, response rates are usually considerably lower CITATION CITATION, and it is frequently not possible to assemble a three-drug regimen with uncompromised activity against all viral strains present.
	In these individuals, treatment failure often occurs after an initial period of response to a new regimen, and is usually associated with the appearance of multiply drug-resistant viral strains.
	This has led to attempts to treat highly experienced patients with various deep salvage regimens consisting of four, five, or six individual drugs CITATION CITATION.
	These patients are particularly vulnerable to the many drug interactions CITATION and adverse metabolic, hematologic, neurologic, cardiovascular, and gastrointestinal side effects that complicate HIV therapy and seriously undermine the success of clinical management CITATION CITATION .
	The need to minimize drug resistance while reducing treatment-related toxicities has engendered an interest in induction maintenance strategies, in which a period of intensified antiretroviral therapy is followed by a simplified long-term regimen CITATION CITATION.
	Most such trials have yielded higher failure rates in the treatment group than in controls receiving conventional therapy.
	Failure typically occurs during maintenance therapy, and has been attributed to poor regimen adherence CITATION and recrudescence of resistance mutations present before institution of induction therapy CITATION.
	One weakness of existing studies has been that induction therapy consisted of standard three-drug antiretroviral therapy regimens in common clinical use at the time of the study, under conditions now recognized to permit subclinical viral replication CITATION, CITATION.
	Moreover, in these early studies, the induction phase only lasted between 3 to 6 mo, which may be insufficient.
	However, two recent studies have shown the apparent effectiveness of induction therapy for 48 wk followed by maintenance therapy with atazanavir CITATION or lopinvir/ritonavir CITATION, CITATION, and this has led to new optimism concerning IM approaches.
	We have hypothesized that a longer period of a highly suppressive induction therapy that is appropriately timed relative to the start of maintenance therapy may allow minority resistant variants to decay below a stochastic extinction threshold, allowing for successful long-term treatment with simpler and better-tolerated regimens.
	To explore this hypothesis quantitatively, we constructed a detailed computer simulation model of the dynamics of sensitive and resistant viruses during a hypothetical IM regimen.
	We show that the timing and duration of induction therapy relative to maintenance therapy can affect the probability that viruses resistant to the maintenance regimen will be eradicated in ways that are somewhat counterintuitive.
	Under biologically plausible conditions, we find that 6 10 mo of induction therapy are required to maximize the probability of eradicating these resistant viruses.
	For shorter induction periods, we find that it is optimal to use a delayed-induction regimen administered several days to weeks after the start of the intended long-term maintenance therapy.### abstract ###
	Expansion of polyglutamine tracts in proteins results in protein aggregation and is associated with cell death in at least nine neurodegenerative diseases.
	Disease age of onset is correlated with the polyQ insert length above a critical value of 35 40 glutamines.
	The aggregation kinetics of isolated polyQ peptides in vitro also shows a similar critical-length dependence.
	While recent experimental work has provided considerable insights into polyQ aggregation, the molecular mechanism of aggregation is not well understood.
	Here, using computer simulations of isolated polyQ peptides, we show that a mechanism of aggregation is the conformational transition in a single polyQ peptide chain from random coil to a parallel -helix.
	This transition occurs selectively in peptides longer than 37 glutamines.
	In the -helices observed in simulations, all residues adopt -strand backbone dihedral angles, and the polypeptide chain coils around a central helical axis with 18.5 2 residues per turn.
	We also find that mutant polyQ peptides with proline-glycine inserts show formation of antiparallel -hairpins in their ground state, in agreement with experiments.
	The lower stability of mutant -helices explains their lower aggregation rates compared to wild type.
	Our results provide a molecular mechanism for polyQ-mediated aggregation.
	The appearance of polyglutamine -containing aggregates CITATION CITATION is a hallmark of disease progression in all diseases in which CAG-expansions occur in genes CITATION.
	Intranuclear inclusion bodies containing polyQ aggregates have been found in vitro CITATION, CITATION, in cell cultures, animal models, and affected patients CITATION, CITATION.
	The aggregates are known to have a characteristic amyloid topology CITATION.
	The inhibition of oligomerization by the azo-dye Congo red, or by the Hsp70/Hsp40 chaperone system, exerts marked protective effects in vivo and in vitro CITATION, CITATION.
	Aggregation and disease are observed if the number of glutamines in the expansion, n, exceeds a critical value, n C CITATION.
	The nearly universal existence of this criticality in all polyQ-related diseases suggests that when the polyQ insert length exceeds a critical value, a pathological change, largely independent of the host protein, occurs in the polyQ insert itself.
	Therefore, isolated polyQ peptides have been used as model systems for studying polyQ aggregation CITATION, CITATION, CITATION, and it is known that: The nuclear uptake of polyQ peptide aggregates prepared in vitro is cytotoxic in cell cultures CITATION, isolated polyQ peptides have in vitro aggregation properties similar to the corresponding full-length proteins containing the polyQ insert CITATION, CITATION, peptide aggregation follows a nucleated mechanism showing characteristic lag and growth phases CITATION, CITATION, and the glutamine tract-length dependence of the lag-time interval correlates well with the age of onset of disease CITATION.
	Peptides of subcritical lengths have long lag times of aggregation and a corresponding age of onset later than the typical life span of a person.
	Longer peptides have progressively smaller lag times of aggregation, and a correspondingly early age of onset of the disease CITATION .
	Unaggregated polyQ peptides form random coil structures, whereas aggregates are composed of amyloid-like -strands CITATION.
	The conversion of random coil to -strand occurs in an individual polyQ chain CITATION, and fibril formation occurs by addition of other polyQ chains to these monomeric -strand nuclei.
	Therefore, the conformational dynamics of an individual polyQ chain determines both its aggregation mechanism and the structure of the final aggregates.
	The details of the conformational dynamics of polyQ and the length dependence of the dynamics are not well understood CITATION .
	this paper studies quantum annealing  qa  for clustering  which can be seen as an extension of simulated annealing  sa 
	we derive a qa algorithm for clustering and propose an annealing schedule  which is crucial in practice
	experiments show the proposed qa algorithm finds better clustering assignments than sa
	furthermore  qa is as easy as sa to implement
	clustering is one of the most popular methods in data mining
	typically  clustering problems are formulated as optimization problems  which are solved by algorithms  for example the em algorithm or convex relaxation
	however  clustering is typically np hard
	the simulated annealing  sa   CITATION  is a promising candidate
	CITATION  proved sa was able to find the global optimum with a slow cooling schedule of temperature
	although their schedule is in practice too slow for clustering of a large amount of data  it is well known that sa still finds a reasonably good solution even with a faster schedule than what  citeauthor geman NUMBER   proposed
	in statistical mechanics  quantum annealing  qa  has been proposed as a novel alternative to sa  CITATION
	qa adds another dimension     to sa for annealing  see fig
	thus  it can be seen as an extension of sa
	qa has succeeded in specific problems  e g the ising model in statistical mechanics  and it is still unclear that qa works better than sa in general
	we do not actually think qa intuitively helps clustering  but we apply qa to clustering just as procedure to derive an algorithm
	a derived qa algorithm depends on the definition of quantum effect
	we propose quantum effect    which leads to a search strategy fit to clustering
	our contribution is 1) to propose a qa based optimization algorithm for clustering in particular quantum effect for clustering and a good annealing schedule  which is crucial for applications 2) and to experimentally show the proposed algorithm optimizes clustering assignments better than sa
	we also show the proposed algorithm is as easy as sa to implement 
 	the algorithm we propose is a markov chain monte carlo  mcmc  sampler  which we call qa st sampler
	as we explain later  a naive qa sampler is intractable even with mcmc
	thus  we approximate qa by the suzuki trotter  st  expansion  CITATION  to derive a tractable sampler  which is the qa st sampler
	qa st looks like parallel   sas with interaction    see fig  
	at the beginning of the annealing process  qa st is almost the same as   sas
	hence  qa st finds    local  optima independently
	as the annealing process continues  interaction   in fig becomes stronger to move   states closer
	qa st at the end picks up the state with the lowest energy in   states as the final solution
	qa st with the proposed quantum effect   works well for clustering
	fig is an example where data points are grouped into four clusters
	SYMBOL and SYMBOL are locally optimal and   is globally optimal
	suppose SYMBOL is equal to two and SYMBOL and SYMBOL in fig correspond to SYMBOL and SYMBOL in fig
	although   and   are local optima  the interaction   in fig allows   and   to search for a better clustering assignment between   and
	quantum effect   defines the distance metric of clustering assignments
	in this case  the proposed   locates   between   and
	thus  the interaction   gives good chance to go to   because   makes   and   closer  see fig  
	the proposed algorithm actually finds   from   and
	fig is just an example
	however  a similar situation often occurs in clustering
	clustering algorithms in most cases give   almost   globally optimal solutions like   and    where the majority of data points are well clustered  but some of them are not
	thus  a better clustering assignment can be constructed by picking up well clustered data points from many sub optimal clustering assignments
	note an assignment constructed in such a way is located between the sub optimal ones by the proposed quantum effect   so that qa st can find a better assignment between sub optimal ones
	previous tests of cumulative prospect theory cpt and of the priority heuristic ph found evidence contradicting these two models of risky decision making.
	however, those tests were criticized because they had characteristics that might "trigger" use of other heuristics.
	this paper presents new tests that avoid those characteristics.
	expected values of the gambles are nearly equal in each choice.
	in addition, if a person followed expected value ev, expected utility eu, cpt, or ph in these tests, she would shift her preferences in the same direction as shifts in ev or eu
	in contrast, the transfer of attention exchange model tax and a similarity model predict that people will reverse preferences in the opposite direction
	results contradict the ph, even when ph is modified to include a preliminary similarity evaluation using the ph parameters
	new tests of probability-consequence interaction were also conducted
	strong interactions were observed, contrary to ph
	these results add to the growing bodies of evidence showing that neither cpt nor ph is an accurate description of risky decision making
	this paper compares three models that attempt to describe risky decision making
	these models are cumulative prospect theory cpt  CITATION , birnbaum's  CITATION  transfer of attention exchange model tax, and the priority heuristic ph of brandstatter, gigerenzer, and hertwig  CITATION
	the ph model is based on the idea that people compare one attribute at a time, such as the minimum prizes
	in addition, the similarity model of rubinstein  CITATION  as modified by leland  CITATION  is also relevant to these studies, although these studies were not designed to test that model
	birnbaum  CITATION  reviewed a number of critical tests that refute any rank dependent utility rdu model  CITATION  including rank and sign-dependent utility  CITATION , cpt, and expected utility eu
	birnbaum  CITATION  noted that many of the same tests that refute cpt also contradict the priority heuristic
	for example, the priority heuristic predicted fewer than half of the modal choices analyzed by birnbaum  CITATION , by birnbaum  CITATION , and by birnbaum and navarrete  CITATION
	some of these choices included cases where 90% or more of the participants satisfied stochastic dominance but the priority heuristic predicts indifference
	in other choices, significantly more than half of the participants about 70% of undergraduates violated stochastic dominance, but the priority heuristic predicts that people should satisfy it
	brandstatter, gigerenzer, and hertwig  CITATION  responded that properties of these choices may have induced people to use other heuristics drawn from a person's "adaptive toolbox
	 presumably, decision makers first decide what rule to use, then they either apply that rule or choose to use another rule
	the mechanism that decides what rule to use has not yet been specified; it is described instead with lists of "triggering conditions," which are estimated from data like parameters
	brandstatter et al CITATION  concluded that the priority heuristic does not apply when there is a stochastic dominance relation in the choice
	in addition, brandstatter et al CITATION  argued that certain choices reviewed by birnbaum  CITATION  used gambles that differed in expected value ev
	brandstatter et al CITATION  presented a figure to show that the priority heuristic is not accurate when expected values evs differ, which led them to suppose that two strategies are at work, one for "easy" choices that differ in ev and one for "harder" choices where evs are nearly equal
	from the data, brandstatter et al CITATION  estimated that, when the ratio of ev exceeds 2, people act as if they choose the gamble with the higher ev
	brandstatter et al consider ev ratio as a proxy for the "difficulty" of a choice, but do not necessarily hold that people actually compute ratios of ev
	they argued that the priority heuristic is accurate for "difficult" choices in which evs are nearly equal
	however, birnbaum  CITATION  noted that ev ratios in birnbaum and navarrete  CITATION  had been inside the region where ph is supposed to apply; in that study, the priority heuristic failed to reproduce even half of the modal choices correctly
	brandstatter et al CITATION  replicated part of that study and their results confirmed that the priority heuristic reproduced fewer than half of the modal choices that they chose for replication  CITATION
	to account for the results, brandstatter et al noted that birnbaum and navarrete  CITATION  used many choices in which both gambles of a choice had the same probability distribution and in some choices two branches had the same probability
	ph was not accurate for such choices, so brandstatter, et al CITATION  theorized that people use a "toting up" heuristic for choices in which two branches had the same probability
	in some of the choices in birnbaum and navarrete  CITATION , there was a common probability-consequence branch in both choices, which was theorized to trigger editing rules and other heuristics that were called up to account for the failures of the priority heuristic
	the arguments of brandstatter et al CITATION  might also provide excuses for previous failures of cpt as well
	this paper devises a new type of test that avoids the exceptions stated above
	in these tests, one alternative does not stochastically dominate the other, there are no common probability-consequence branches, probabilities of the consequences are not equal, and expected values are nearly equal
	in addition, unlike previous tests, the new tests use shifts in expected value and expected utility to "help" predictions of ph and cpt
	that is, expected value and expected utility are both manipulated such that, if a person shifts his or her judgments in the same direction as the changes in eu or ev, his or her choices will appear consistent with ph and cpt
	however, the choices are designed so that the tax model with parameters typical of previous research predicts that people will shift their choices in the opposite direction of ev, eu, cpt, and ph
	Alternative splicing contributes to both gene regulation and protein diversity.
	To discover broad relationships between regulation of alternative splicing and sequence conservation, we applied a systems approach, using oligonucleotide microarrays designed to capture splicing information across the mouse genome.
	In a set of 22 adult tissues, we observe differential expression of RNA containing at least two alternative splice junctions for about 40 percent of the 6,216 alternative events we could detect.
	Statistical comparisons identify 171 cassette exons whose inclusion or skipping is different in brain relative to other tissues and another 28 exons whose splicing is different in muscle.
	A subset of these exons is associated with unusual blocks of intron sequence whose conservation in vertebrates rivals that of protein-coding exons.
	By focusing on sets of exons with similar regulatory patterns, we have identified new sequence motifs implicated in brain and muscle splicing regulation.
	Of note is a motif that is strikingly similar to the branchpoint consensus but is located downstream of the 5 splice site of exons included in muscle.
	Analysis of three paralogous membrane-associated guanylate kinase genes reveals that each contains a paralogous tissue-regulated exon with a similar tissue inclusion pattern.
	While the intron sequences flanking these exons remain highly conserved among mammalian orthologs, the paralogous flanking intron sequences have diverged considerably, suggesting unusually complex evolution of the regulation of alternative splicing in multigene families.
	Splicing is an essential process that constructs protein coding messenger RNA sequences using tiny segments of information buried in the much larger primary transcripts of the eukaryotic gene.
	Regulated alternative splicing can create different protein coding sequences under different biological circumstances, allowing the production of functionally related but distinct proteins.
	In addition, alternative splicing can mediate the repression of gene expression by stimulating the formation of transcripts subject to nonsense-mediated decay CITATION CITATION.
	Splicing patterns seem distinct in the vertebrate nervous system compared to other tissues CITATION, CITATION, and it is tempting to hypothesize that neural alternative splicing contributed to the rapid evolution of the vertebrate brain without large increases in gene number CITATION .
	Biochemical analysis of alternative splicing has shown that numerous RNA binding proteins influence the use of specific splice sites to stimulate splicing events that lead to particular mRNA isoforms CITATION, CITATION.
	These RNA binding proteins may activate or repress the use of splice sites by binding to nearby sequences in the exon or in the intron.
	In many cases, multiple RNA binding proteins combine to create repressing and activating influences that produce patterns of splicing control CITATION, CITATION.
	Some proteins, such as SR proteins and the CELF proteins, have mostly activating roles, whereas others, such as hnRNP A1, PTB, and nPTB, have mostly repressing roles.
	Certain proteins can either activate or repress splicing in different contexts, depending on the position of their binding sites or the expression of other RNA binding proteins CITATION, CITATION .
	A complete catalog of the RNA sequences corresponding to the enhancers and silencers bound by splicing regulatory proteins would greatly aid the understanding of splicing regulatory networks.
	Thus far, there are only a handful of splicing regulators whose corresponding RNA binding motifs have been identified, whereas there may be many splicing regulators among the hundreds of RNA binding proteins encoded by the mouse genome.
	In addition, several related but distinct genes produce proteins that bind the same or overlapping sets of sequences; for example, Fox-1 and RBM9 each bind UGCAUG CITATION, CITATION, and the branchpoint binding protein SF1 and the protein quaking each bind UACUAAC-like motifs CITATION CITATION.
	Adding to this complexity is the tendency for the mRNAs of RNA binding proteins to be alternatively spliced, leading to multiple RNA binding protein isoforms with potentially different functions.
	Currently, the methods available for expanding the list of known regulators and their target sequences are limited, and the development of this catalog is in the early stage CITATION .
	Much of the available genomic information on alternative splicing is derived by the alignment of large numbers of expressed sequence tags and messenger RNAs to genome sequences.
	The analysis of exons that appear to be constitutive or alternative has led to the successful identification of many distinguishing features of alternatively spliced regions CITATION CITATION, even allowing their accurate prediction without cDNA evidence CITATION, CITATION, CITATION.
	Although cDNA libraries have been invaluable for discovering general features of alternatively spliced exons, it is difficult to connect specific regulatory sequences to specific biological conditions with confidence due to variable and sometimes missing information about the source materials and methods of cDNA library construction.
	The relatively low number of transcripts present from any one gene also makes it difficult to estimate differences in expression levels using library representation as a measure.
	Thus, more direct methods are needed to associate alternative splicing events with underlying biological conditions.
	The recent application of microarray technology to questions of splicing and splicing regulation promises to reveal parallel connections between many splicing events and specific biological or experimental conditions CITATION CITATION.
	Analysis of experimental changes in splicing for many genes simultaneously should reveal biological conditions necessary for proper splicing regulation in a way that analysis of cDNA libraries cannot, and with breadth that cannot be achieved by analysis of a reporter construct or a few endogenous target genes.
	To demonstrate this, we constructed a DNA microarray designed to capture splicing information for about 6,200 alternative events in the mouse transcriptome, using a combination of splice junction and exon probes, and have hybridized RNA from 22 adult mouse tissues.
	We examine splicing in these tissues by asking three questions.
	First we ask, Which RNA isoforms are present in a particular tissue sample?
	To answer this simple question, we used a new method based on comparing the intensity of the probes in a probe set to the distribution of intensities from all probes with similar G C level.
	This is similar in spirit although different in approach to the present-absent calls from Affymetrix MAS 5.0 algorithms CITATION, as this microarray did not contain mismatch probes.
	Using RT-PCR, we show that this method has a true-positive rate of 85 percent.
	Second we ask, Which RNA isoforms are differentially expressed across the tissues examined?
	For each RNA isoform, the intensities of the isoform-specific junction probes were examined across tissues using the Kruskal-Wallis statistical test.
	After correcting for multiple testing, about 40 percent of the 6,216 total alternative splicing events examined were found to have more than one RNA form that was differentially expressed, indicating widespread tissue differences in splicing over the tissues.
	Third we ask, Which cassette exons are included differentially between brain and nonbrain tissues?
	To answer this, we used a regression-based bootstrapping method, which also allows an estimate of the relative change in skipping and inclusion in the two sample groups.
	We analyzed the intron sequences associated with exon skipping events that are differentially regulated in brain or muscle relative to other tissues and found unusual patterns of sequence conservation that provide new information about tissue regulation of alternative splicing and its evolution.
	defensive forecasting is a method of transforming laws of probability  stated in game theoretic terms as strategies for sceptic  into forecasting algorithms
	there are two known varieties of defensive forecasting    continuous    in which sceptic s moves are assumed to depend on the forecasts in a  semi continuous manner and which produces deterministic forecasts  and   randomized    in which the dependence of sceptic s moves on the forecasts is arbitrary and forecaster s moves are allowed to be randomized
	this note shows that the randomized variety can be obtained from the continuous variety by smearing sceptic s moves to make them continuous
	new as compared to version  NUMBER    NUMBER  august  NUMBER   of this report   the assumption of version  NUMBER  that the outcome space   is finite is relaxed  and now it is only assumed to be compact
	in the case where   is finite  it is shown that forecaster can choose his randomized forecasts concentrated on a finite set of cardinality at most
	the continuous variety of defensive forecasting was essentially introduced by levin  CITATION   but was later rediscovered by kakade and foster  CITATION  and takemura  et al    CITATION
	the randomized variety was introduced  in the case of von mises s version of the game theoretic approach to probability  by foster and vohra  CITATION  and further developed by  among others  sandroni  et al    CITATION   these papers  however  were only concerned with asymptotic calibration
	non asymptotic versions of the randomized variety were proposed by sandroni  CITATION   based on standard measure theoretic probability  and vovk and shafer  CITATION   based on game theoretic probability 
	kakade and foster  CITATION  noticed that some calibration results require very little randomization  this will be an important aspect of our theorem  
	this note states two simple results about defensive forecasting  theorem  about the continuous variety and theorem  about the randomized variety
	the proof of theorem  is obtained from the proof of theorem  by blurring sceptic s moves
	in our informal discussions we will be assuming that the set   of all possible outcomes is finite  although we will try to make mathematical statements as general as possible
	the reader who is only interested in the main ideas might choose to specialize theorems  and  and their proofs to the case of finite
	A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model.
	The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids.
	Our previous studies have dealt with 7510 proteins of no more than 150 amino acids.
	The proteins are ranked according to the strength of the resistance.
	Most of the predicted top-strength proteins have not yet been studied experimentally.
	Architectures and folds which are likely to yield large forces are identified.
	New types of potent force clamps are discovered.
	They involve disulphide bridges and, in particular, cysteine slipknots.
	An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds.
	These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications.
	A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known.
	This class is characterized through molecular dynamics simulations.
	Atomic force microscopy, optical tweezers, and other tools of nanotechnology have enabled induction and monitoring of large conformational changes in biomolecules.
	Such studies are performed to assess structure of the biomolecules, their elastic properties, and ability to act as nanomachines in a cell.
	Stretching studies of proteins CITATION are of a particular current interest and they have been performed for under a hundred of systems.
	Interpretation of some of these experiments has been helped by all-atom simulations, such as reported in refs. CITATION, CITATION.
	They are limited by of order 100 ns time scales and thus require using unrealistically large constant pulling speeds.
	However, they often elucidate the nature of the force clamp the region responsible for the largest force of resistance to pulling, FORMULA.
	All of the experimental and all-atom simulational studies address merely a tiny fraction of proteins that are stored in the Protein Data Bank CITATION.
	Thus it appears worthwhile to consider a large set of proteins and determine their FORMULA within an approximate model that allows for fast and yet reasonably accurate calculations.
	Structure-based models of proteins, as pioneered by Go and his collaborators CITATION and used in several implementations CITATION CITATION, seem to be suited to this task especially well since they are defined in terms of the native structures away from which stretching is imposed.
	There are many ways, all phenomenological, to construct a structure-based model of a protein.
	504 of possible variants are enumerated and 62 are studied in details in ref. CITATION.
	The variants differ by the choice of effective potentials, nature of the local backbone stiffness, energy-related parameters, and of the coarse-grained degrees of freedom.
	The most crucial choice relates to making a decision about which interactions between amino acids count as native contacts.
	Comparing FORMULA to the corresponding experimental values in 36 available cases selects several optimal models CITATION.
	Among them, there is one which is very simple and which describes a protein in terms of its FORMULA atoms, as labeled by the sequential index FORMULA.
	This model is denoted by FORMULA which stands for, respectively, the Lennard-Jones native contact potentials, local backbone stiffness represented by harmonic terms that favor the native values of local chiralities, the contact map in which there are no FORMULA contacts, and the amplitude of the Lennard-Jones potential, FORMULA, is uniform.
	The contact map is determined by assigning the van der Waals spheres to the heavy atoms and by checking whether spheres belonging to different amino acids overlap in the native state CITATION, CITATION.
	If they do, a contact is declared as native.
	Non-native contacts are considered repulsive.
	Application of this criterion frequently selects the FORMULA contacts as native.
	If the contact map includes these contacts the resulting model will be denoted here as FORMULA.
	On average, it performs worse than FORMULA because the FORMULA contacts usually correspond to the weak van der Waals couplings as can be demonstrated in a sample of proteins by using a software CITATION which analyses atomic configurations from the chemical perspective on molecular bonds.
	Thus the FORMULA couplings should better be removed from the contact map .
	The survey to determine FORMULA in 7510 model proteins with the number of amino acids, FORMULA, not exceeding 150 and 239 longer proteins has been accomplished twice.
	First within the FORMULA model CITATION and soon afterwords within the FORMULA model CITATION.
	The first survey also comes with many details of the methodology whereas the second just presents the outcomes.
	The two surveys are compared in more details in refs. CITATION, CITATION.
	The results differ, particularly when it comes to ranking of the proteins according to the value of FORMULA, but they mutually provide the error bars on the findings.
	They both agree, however, on predicting that there are many proteins whose strength should be considerably larger than the frequently studied benchmark the sarcomere protein titin.
	Near the top of the list, there is the scaffoldin protein c7A which has been recently measured to have FORMULA of about 480 pN CITATION.
	Other findings include establishing correlations with the CATH hierarchical classification scheme CITATION, CITATION, such as that there are no strong FORMULA proteins, and identification of several types of the force clamps.
	The large forces most commonly originate in parallel FORMULA that are sheared CITATION.
	However, there are also clamps with antiparallel FORMULA, unstructured strands, and other kinds.
	The two surveys have been based on the structure download made on July 26, 2005 when the PDB comprised 29 385 entries.
	Many of them correspond to nucleic acids, complexes with nucleic acids and with other proteins, carbohydrates, or come with incomplete files and hence the much smaller number of proteins that could be used in the molecular dynamics studies.
	Here, we present results of still another survey which is based on a download of December 18, 2008 which contains 54 807 structure files and leads to 17 134 acceptable structures with FORMULA not exceeding 250.
	These structures are then analyzed through simulations based on the FORMULA model.
	The numerical code has been improved to allow for acceleration of calculations by a factor of 2.
	The 190 structures with the top values of FORMULA in units of FORMULA are shown in Table 1 and Table S1 of the SI, together with the values of titin and ubiquitin to provide a scale.
	As argued in the Materials and Methods section section, the unit of force, FORMULA, is now estimated to be of order 110 pN.
	All of the corresponding proteins are predicted to be much stronger than titin and none but two of them have been studied experimentally yet.
	In addition to the types of force clamps identified before, we have discovered two new mechanisms of sturdiness.
	One of them involves a cysteine slipknot and is found to be operational in all of the 13 top strength proteins.
	In this motif, a slip-loop is pulled out of a cysteine knot-loop.
	Another involves dragging of a single fragment of the main chain across a cysteine knot-loop.
	The two mechanisms are similar in spirit since both involve dragging of the backbone.
	However, in the CSK case, two fragments of the backbone are participating.
	We make a more systematic identification of the CATH-classified architectures that are linked to mechanical strength and then analyze correlations of the data to the SCOP-based grouping CITATION CITATION.
	The previous surveys did not relate to the SCOP scheme.
	We identify the CATH-based architectures and SCOP-based folds that are associated with the occurrence of a strong resistance to pulling.
	A general observation, however, is that each such group of structures may also include examples of proteins that unravel easily.
	The dynamics of a protein are very sensitive to mechanical details that are largely captured by the contact map and not just by the appearance of a structure.
	On the other hand, if one were to look for mechanically strong proteins then the architectures and folds identified by us should provide a good starting point.
	We also study the dependence of FORMULA on the pulling velocity and characterize the dependence on FORMULA through distributions of the forces.
	The current third survey has been performed within the same FORMULA model as the second survey CITATION.
	However, we reuse and extend it here because the editors of Biophysical Journal retracted the second survey CITATION.
	All of the values of FORMULA are deposited at the website LINK and can by accessed by through the PDB structure code.### abstract ###
	This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA)
	We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice
	Experiments show the proposed QA algorithm finds better clustering assignments than SA
	Furthermore, QA is as easy as SA to implement
	Clustering is one of the most popular methods in data mining
	Typically, clustering problems are formulated as optimization problems, which are solved by algorithms, for example the EM algorithm or convex relaxation
	However, clustering is typically NP-hard
	The simulated annealing (SA)  CITATION  is a promising candidate
	CITATION  proved SA was able to find the global optimum with a slow cooling schedule of temperature  SYMBOL
	Although their schedule is in practice too slow for clustering of a large amount of data, it is well known that SA still finds a reasonably good solution even with a faster schedule than what CITATION proposed
	In statistical mechanics, quantum annealing (QA) has been proposed as a novel alternative to SA  CITATION
	QA adds another dimension,  SYMBOL , to SA for annealing, see Fig
	Thus, it can be seen as an extension of SA
	QA has succeeded in specific problems, e g the Ising model in statistical mechanics, and it is still unclear that QA works better than SA in general
	We do not actually think QA intuitively helps clustering, but we apply QA to clustering just as procedure to derive an algorithm
	A derived QA algorithm depends on the definition of quantum effect  SYMBOL
	We propose quantum effect  SYMBOL , which leads to a search strategy fit to clustering
	Our contribution is, 1) to propose a QA-based optimization algorithm for clustering, in particular quantum effect  SYMBOL  for clustering  and a good annealing schedule, which is crucial for applications, 2) and to experimentally show the proposed algorithm optimizes clustering assignments better than SA
	We also show the proposed algorithm is as easy as SA to implement
	The algorithm we propose is a Markov chain Monte Carlo (MCMC) sampler, which we call QA-ST sampler
	As we explain later, a naive QA sampler is intractable even with MCMC
	Thus, we approximate QA by the Suzuki-Trotter (ST) expansion  CITATION  to derive a tractable sampler, which is the QA-ST sampler
	QA-ST looks like parallel  SYMBOL  SAs with interaction  SYMBOL  (see Fig )
	At the beginning of the annealing process, QA-ST is almost the same as  SYMBOL  SAs
	Hence, QA-ST finds  SYMBOL  (local) optima independently
	As the annealing process continues, interaction  SYMBOL  in Fig becomes stronger to move  SYMBOL  states closer
	QA-ST at the end picks up the state with the lowest energy in  SYMBOL  states as the final solution
	QA-ST with the proposed quantum effect  SYMBOL  works well for clustering
	Fig is an example where data points are grouped into four clusters
	SYMBOL and  SYMBOL are locally optimal and  SYMBOL  is globally optimal
	Suppose  SYMBOL  is equal to two and  SYMBOL  and  SYMBOL  in Fig correspond to  SYMBOL  and  SYMBOL  in Fig
	Although  SYMBOL  and  SYMBOL  are local optima, the interaction  SYMBOL  in Fig allows  SYMBOL  and  SYMBOL  to search for a better clustering assignment between  SYMBOL  and  SYMBOL
	Quantum effect  SYMBOL  defines the distance metric of clustering assignments
	In this case, the proposed  SYMBOL  locates  SYMBOL  between  SYMBOL  and  SYMBOL
	Thus, the interaction  SYMBOL  gives good chance to go to  SYMBOL  because  SYMBOL  makes  SYMBOL  and  SYMBOL  closer (see Fig )
	The proposed algorithm actually finds  SYMBOL  from  SYMBOL  and  SYMBOL
	Fig is just an example
	However, a similar situation often occurs in clustering
	Clustering algorithms in most cases give ``almost'' globally optimal solutions like  SYMBOL  and  SYMBOL , where the majority of data points are well-clustered, but some of them are not
	Thus, a better clustering assignment can be constructed by picking up well-clustered data points from many sub-optimal clustering assignments
	Note an assignment constructed in such a way is located between the sub-optimal ones by the proposed quantum effect  SYMBOL  so that QA-ST can find a better assignment between sub-optimal ones
 Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms
 There are two known varieties of defensive forecasting: ``continuous'', in which Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous manner and which produces deterministic forecasts, and ``randomized'', in which the dependence of Sceptic's moves on the forecasts is arbitrary and Forecaster's moves are allowed to be randomized
 This note shows that the randomized variety can be obtained from the continuous variety by smearing Sceptic's moves to make them continuous
 New as compared to version 1 (17 August 2007) of this report: The assumption of version 1 that the outcome space  SYMBOL  is finite is relaxed, and now it is only assumed to be compact
 In the case where  SYMBOL  is finite, it is shown that Forecaster can choose his randomized forecasts concentrated on a finite set of cardinality at most  SYMBOL
 The continuous variety of defensive forecasting was essentially introduced by Levin  CITATION , but was later rediscovered by Kakade and Foster  CITATION  and Takemura  et al CITATION
 The randomized variety was introduced (in the case of von Mises's version of the game-theoretic approach to probability) by Foster and Vohra  CITATION  and further developed by, among others, Sandroni  et al CITATION ; these papers, however, were only concerned with asymptotic calibration
 Non-asymptotic versions of the randomized variety were proposed by Sandroni  CITATION  (based on standard measure-theoretic probability) and Vovk and Shafer  CITATION  (based on game-theoretic probability)
 Kakade and Foster  CITATION  noticed that some calibration results require very little randomization (this will be an important aspect of our Theorem )
 This note states two simple results about defensive forecasting, Theorem  about the continuous variety and Theorem  about the randomized variety
 The proof of Theorem  is obtained from the proof of Theorem  by blurring Sceptic's moves
 In our informal discussions we will be assuming that the set  SYMBOL  of all possible outcomes is finite, although we will try to make mathematical statements as general as possible
 The reader who is only interested in the main ideas might choose to specialize Theorems  and  and their proofs to the case of finite  SYMBOL
	Neuronal activity is mediated through changes in the probability of stochastic transitions between open and closed states of ion channels.
	While differences in morphology define neuronal cell types and may underlie neurological disorders, very little is known about influences of stochastic ion channel gating in neurons with complex morphology.
	We introduce and validate new computational tools that enable efficient generation and simulation of models containing stochastic ion channels distributed across dendritic and axonal membranes.
	Comparison of five morphologically distinct neuronal cell types reveals that when all simulated neurons contain identical densities of stochastic ion channels, the amplitude of stochastic membrane potential fluctuations differs between cell types and depends on sub-cellular location.
	For typical neurons, the amplitude of membrane potential fluctuations depends on channel kinetics as well as open probability.
	Using a detailed model of a hippocampal CA1 pyramidal neuron, we show that when intrinsic ion channels gate stochastically, the probability of initiation of dendritic or somatic spikes by dendritic synaptic input varies continuously between zero and one, whereas when ion channels gate deterministically, the probability is either zero or one.
	At physiological firing rates, stochastic gating of dendritic ion channels almost completely accounts for probabilistic somatic and dendritic spikes generated by the fully stochastic model.
	These results suggest that the consequences of stochastic ion channel gating differ globally between neuronal cell-types and locally between neuronal compartments.
	Whereas dendritic neurons are often assumed to behave deterministically, our simulations suggest that a direct consequence of stochastic gating of intrinsic ion channels is that spike output may instead be a probabilistic function of patterns of synaptic input to dendrites.
	The appropriate level of physical detail required to understand how complex processes such as cognition and behavior emerge from more simple biological structures is unclear CITATION, CITATION.
	For example, while it is possible to account for certain aspects of nervous system function using models that represent each neuron as a simple integrate and fire device, it is increasingly clear that this approach does not capture the full range of computations that many real neurons carry out CITATION, CITATION.
	Dendritic and axonal morphology are defining features of neuronal cell types and have important influences on the computations that a neuron performs CITATION.
	Differences in morphology determine how neurons respond to synaptic input and are sufficient to produce distinct patterns of spontaneous activity CITATION and degrees of action potential back-propagation from the soma into the dendrites CITATION.
	Cable theory and compartmental modeling provide a foundation for predicting the propagation of electrical signals in the dendrites and axons of neurons CITATION, CITATION.
	However, while the assumption that transitions between open and closed states of ion channels can be treated as a deterministic process may be sufficient for some purposes, recent evidence suggests that stochastic transitions between the states of individual ion channels could influence computations carried out by neurons CITATION CITATION.
	Stochastic opening and closing of ion channels causes noisy fluctuations in the current or voltage recorded from a neuron CITATION CITATION.
	While cable theory suggests that fluctuations of this kind might be particularly important in fine structures such as axons and dendrites CITATION, we nevertheless know very little about how neuronal morphology and stochastic gating of ion channels interact to determine how neurons respond to synaptic input.
	Given the difficulty of reducing detailed morphological models to simple analytical forms that could also incorporate stochastic gating of individual ion channels CITATION, experimentally constrained numerical simulations will be important to enable these issues to be explored systematically.
	Investigation of stochastic ion channel gating using numerical simulations has been limited by trades-offs between simulation accuracy and computation time CITATION.
	A simple approach is to add noise sources to deterministic models.
	However, as ion channels have multiple functional states with transitions that often depend on the membrane voltage CITATION, CITATION, CITATION, CITATION, this may not accurately account for the noise introduced by ion channel currents.
	A more accurate alternative is to explicitly model transitions between different functional states for each ion channel on a neuron's membrane.
	However, for neurons with complex axonal or dendritic architectures there are two substantial obstacles to this approach.
	First, typical central neurons express large numbers of ion channels and simulations must be repeated many times to obtain statistically valid descriptions CITATION.
	This is a formidable computational task and even relatively straightforward simulations of the consequences of stochastic channel gating can require substantial computing time.
	Second, each neuronal ion channel occupies a specific location on the extra-cellular membrane, whereas most neuronal models represent the distribution of ion channels as the density of a deterministic conductance across an area of membrane.
	Although this formalism has been successful for simulating many aspects of neuronal activity, it is of less use for models that explore the consequences of the localization of individual ion channels, for example to evaluate the macroscopic effects of short range interactions between ion channels and other signaling molecules CITATION, or the consequences of spatially heterogeneous distributions of ion channels within relatively small sub-cellular structures such as dendritic spines and axon terminals CITATION, CITATION .
	To address the functional consequences of stochastic ion channel gating in neurons with extensive dendritic or axonal arborizations we developed a parallel stochastic ion channel simulator, which enables efficient simulation of the electrical activity of neurons with complex morphologies and arbitrary localization of stochastic ion channels on the extracellular membrane, while also addressing limitations of previous approaches.
	We have also developed an interactive tool for visualization and development of models of neurons containing uniquely located ion channels.
	Here, we illustrate the use of PSICS and ICING, outline the computational strategies used and provide benchmark data for evaluation.
	We then identify previously unappreciated differences between the effects of stochastic ion channel gating on somatic and dendritic membrane potential activity in several different morphological classes of neuron.
	We show that the consequences of stochastic gating depend on dendritic morphology and suggest novel functional roles for the kinetics of ion channel gating.
	Using a previously well-validated realistic model of a CA1 pyramidal neuron we demonstrate that stochastic ion channel gating influences spike output in response to dendritic synaptic input.
	We show that stochastic gating of axonal or dendritic ion channels substantially modifies synaptically driven dendritic and axonal spike output, with stochastic gating of voltage-dependent sodium and potassium channels having the greatest impact and hyperpolarization-activated channels the least.
	By demonstrating that neuronal responses to dendritic synaptic input can be intrinsically probabilistic, these results offer a new and general perspective on synaptic integration by central neurons.
	Full documentation for PSICS/ICING as well as the software, source code and examples are available from the project website .
	we propose a nonparametric bayesian factor regression model that accounts for uncertainty in the number of factors  and the relationship between factors
	to accomplish this  we propose a sparse variant of the indian buffet process and couple this with a hierarchical model over factors  based on kingman s coalescent
	we apply this model to two problems  factor analysis and factor regression  in gene expression data analysis
	factor analysis is the task of explaining data by means of a set of  latent factors
	factor  regression  couples this analysis with a prediction task  where the predictions are made solely on the basis of the factor representation
	the latent factor representation achieves two fold benefits    NUMBER   discovering the latent  process  underlying the data    NUMBER   simpler predictive modeling through a compact data representation
	in particular    NUMBER   is motivated by the problem of prediction in the    large p small n    paradigm  CITATION   where the number of features   greatly exceeds the number of examples    potentially resulting in overfitting
	we address three fundamental shortcomings of standard factor analysis approaches  CITATION     NUMBER   we do not assume a known number of factors    NUMBER   we do not assume factors are independent    NUMBER   we do not assume all features are relevant to the factor analysis
	our motivation for this work stems from the task of reconstructing regulatory structure from gene expression data
	in this context  factors correspond to regulatory pathways
	our contributions thus parallel the needs of gene pathway modeling
	in addition  we couple predictive modeling  for factor regression  within the factor analysis framework itself  instead of having to model it separately
	our factor regression model is fundamentally nonparametric
	in particular  we treat the gene to factor relationship nonparametrically by proposing a sparse variant of the indian buffet process  ibp   CITATION   designed to account for the sparsity of relevant genes  features 
	we  couple  this ibp with a hierarchical prior over the factors
	this prior explains the fact that pathways are fundamentally related  some are involved in transcription  some in signaling  some in synthesis
	the nonparametric nature of our sparse ibp requires that the hierarchical prior  also  be nonparametric
	a natural choice is kingman s coalescent  CITATION   a popular distribution over infinite binary trees
	since our motivation is an application in bioinformatics  our notation and terminology will be drawn from that area
	in particular   genes  are  features    samples  are  examples   and  pathways  are  factors
	however  our model is more general
	an alternative application might be to a collaborative filtering problem  in which case our genes might correspond to movies  our samples might correspond to users and our pathways might correspond to genres
	in this context  all three contributions of our model still make sense  we do not know how many movie genres there are  some genres are closely related  romance to comedy versus to action   many movies may be spurious
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm
 In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties
 However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed
 In many machine learning applications, however, this assumption does not hold
 The observations received by the learning algorithm often have some inherent temporal dependence
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid  processes that implies a dependence between observations weakening over time
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-iid scenarios
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms
 These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-iid scenarios
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, such as the VC-dimension, covering numbers, or Rademacher complexity
 These measures characterize a class of hypotheses, independently of any algorithm
 In contrast, the notion of algorithmic stability can be used to derive bounds that are tailored to specific learning algorithms and exploit their particular properties
 A learning algorithm is stable if the hypothesis it outputs varies in a limited way in response to small changes made to the training set
 Algorithmic stability has been used effectively in the past to derive tight generalization bounds  CITATION
 But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (iid)
 In many machine learning applications, this assumption, however, does not hold; in fact, the iid assumption is not tested or derived from any data analysis
 The observations received by the learning algorithm often have some inherent temporal dependence
 This is clear in system diagnosis or time series prediction problems
 Clearly, prices of different stocks on the same day, or of the same stock on different days, may be dependent
 But, a less apparent time dependency may affect data sampled in many other tasks as well
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid processes that implies a dependence between observations weakening over time  CITATION
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the usefulness of stability-bounds to non-iid scenarios
 Our proofs are based on the independent block technique described by  CITATION  and attributed to  CITATION , which is commonly used in such contexts
 However, our analysis differs from previous uses of this technique in that the blocks of points considered are not of equal size
 For our analysis of stationary  SYMBOL -mixing sequences, we make use of a generalized version of McDiarmid's inequality  CITATION  that holds for  SYMBOL -mixing sequences
 This leads to stability-based generalization bounds with the standard exponential form
 Our generalization bounds for stationary  SYMBOL -mixing sequences cover a more general non-iid scenario and use the standard McDiarmid's inequality, however, unlike the  SYMBOL -mixing case, the  SYMBOL -mixing bound presented here is not a purely exponential bound and contains an additive term depending on the mixing coefficient
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression (SVR)  CITATION , Kernel Ridge Regression  CITATION , and Support Vector Machines (SVMs)  CITATION
 Algorithms such as support vector regression (SVR)  CITATION  have been used in the context of time series prediction in which the iid assumption does not hold, some with good experimental results  CITATION
 To our knowledge, the use of these algorithms in non-iid scenarios has not been previously supported by any theoretical analysis
 The stability bounds we give for SVR, SVMs, and many other kernel regularization-based and relative entropy-based regularization algorithms can thus be viewed as the first theoretical basis for their use in such scenarios
 The following sections are organized as follows
 In Section~, we introduce the necessary definitions for the non-iid problems that we are considering and discuss the learning scenarios in that context
 Section~ gives our main generalization bounds for stationary  SYMBOL -mixing sequences based on stability, as well as the illustration of its applications to general kernel regularization-based algorithms, including SVR, KRR, and SVMs, as well as to relative entropy-based regularization algorithms
 Finally, Section~ presents the first known stability bounds for the more general stationary  SYMBOL -mixing scenario
	The tradeoff between the need to suppress drug-resistant viruses and the problem of treatment toxicity has led to the development of various drug-sparing HIV-1 treatment strategies.
	Here we use a stochastic simulation model for viral dynamics to investigate how the timing and duration of the induction phase of induction maintenance therapies might be optimized.
	Our model suggests that under a variety of biologically plausible conditions, 6 10 mo of induction therapy are needed to achieve durable suppression and maximize the probability of eradicating viruses resistant to the maintenance regimen.
	For induction regimens of more limited duration, a delayed-induction or -intensification period initiated sometime after the start of maintenance therapy appears to be optimal.
	The optimal delay length depends on the fitness of resistant viruses and the rate at which target-cell populations recover after therapy is initiated.
	These observations have implications for both the timing and the kinds of drugs selected for induction maintenance and therapy-intensification strategies.
	The failure of antiretroviral therapies to completely suppress viral replication in some patients represents a major difficulty in the management of HIV infection.
	In therapy-naive patients without clinically apparent resistance mutations, triple-drug therapy with two nucleoside analog reverse transcriptase inhibitors and a protease inhibitor or a non-nucleoside reverse transcriptase inhibitor is standard CITATION.
	In these patients, treatment success rates, defined as viral load 50 copies/ml at 48 wk, range from 70 percent to 80 percent 85 percent.
	However, in patients with previous regimen failure requiring salvage therapy, response rates are usually considerably lower CITATION CITATION, and it is frequently not possible to assemble a three-drug regimen with uncompromised activity against all viral strains present.
	In these individuals, treatment failure often occurs after an initial period of response to a new regimen, and is usually associated with the appearance of multiply drug-resistant viral strains.
	This has led to attempts to treat highly experienced patients with various deep salvage regimens consisting of four, five, or six individual drugs CITATION CITATION.
	These patients are particularly vulnerable to the many drug interactions CITATION and adverse metabolic, hematologic, neurologic, cardiovascular, and gastrointestinal side effects that complicate HIV therapy and seriously undermine the success of clinical management CITATION CITATION .
	The need to minimize drug resistance while reducing treatment-related toxicities has engendered an interest in induction maintenance strategies, in which a period of intensified antiretroviral therapy is followed by a simplified long-term regimen CITATION CITATION.
	Most such trials have yielded higher failure rates in the treatment group than in controls receiving conventional therapy.
	Failure typically occurs during maintenance therapy, and has been attributed to poor regimen adherence CITATION and recrudescence of resistance mutations present before institution of induction therapy CITATION.
	One weakness of existing studies has been that induction therapy consisted of standard three-drug antiretroviral therapy regimens in common clinical use at the time of the study, under conditions now recognized to permit subclinical viral replication CITATION, CITATION.
	Moreover, in these early studies, the induction phase only lasted between 3 to 6 mo, which may be insufficient.
	However, two recent studies have shown the apparent effectiveness of induction therapy for 48 wk followed by maintenance therapy with atazanavir CITATION or lopinvir/ritonavir CITATION, CITATION, and this has led to new optimism concerning IM approaches.
	We have hypothesized that a longer period of a highly suppressive induction therapy that is appropriately timed relative to the start of maintenance therapy may allow minority resistant variants to decay below a stochastic extinction threshold, allowing for successful long-term treatment with simpler and better-tolerated regimens.
	To explore this hypothesis quantitatively, we constructed a detailed computer simulation model of the dynamics of sensitive and resistant viruses during a hypothetical IM regimen.
	We show that the timing and duration of induction therapy relative to maintenance therapy can affect the probability that viruses resistant to the maintenance regimen will be eradicated in ways that are somewhat counterintuitive.
	Under biologically plausible conditions, we find that 6 10 mo of induction therapy are required to maximize the probability of eradicating these resistant viruses.
	For shorter induction periods, we find that it is optimal to use a delayed-induction regimen administered several days to weeks after the start of the intended long-term maintenance therapy.
	Metabolic network reconstructions represent valuable scaffolds for -omics data integration and are used to computationally interrogate network properties.
	However, they do not explicitly account for the synthesis of macromolecules.
	Here, we present the first genome-scale, fine-grained reconstruction of Escherichia coli's transcriptional and translational machinery, which produces 423 functional gene products in a sequence-specific manner and accounts for all necessary chemical transformations.
	Legacy data from over 500 publications and three databases were reviewed, and many pathways were considered, including stable RNA maturation and modification, protein complex formation, and iron sulfur cluster biogenesis.
	This reconstruction represents the most comprehensive knowledge base for these important cellular functions in E. coli and is unique in its scope.
	Furthermore, it was converted into a mathematical model and used to: quantitatively integrate gene expression data as reaction constraints and compute functional network states, which were compared to reported experimental data.
	For example, the model predicted accurately the ribosome production, without any parameterization.
	Also, in silico rRNA operon deletion suggested that a high RNA polymerase density on the remaining rRNA operons is needed to reproduce the reported experimental ribosome numbers.
	Moreover, functional protein modules were determined, and many were found to contain gene products from multiple subsystems, highlighting the functional interaction of these proteins.
	This genome-scale reconstruction of E. coli's transcriptional and translational machinery presents a milestone in systems biology because it will enable quantitative integration of -omics datasets and thus the study of the mechanistic principles underlying the genotype phenotype relationship.
	High-throughput experimental technologies enable the production of heterogeneous data, such as expression profiles and proteomic data, for almost any organism of interest.
	A detailed mathematical representation of the in vivo cellular network is required to obtain a holistic understanding of cellular processes from these data sets and to quantitatively integrate them into a biological context.
	One such approach is the bottom-up network reconstruction, which builds manually networks in a brick-by-brick manner using genome annotation and component-specific information CITATION, CITATION.
	This reconstruction procedure is well established for metabolic reaction networks and has been applied to many organisms, including Human CITATION, Saccharomyces cerevisiae CITATION, CITATION, Leishmani major CITATION, Escherichia coli CITATION, Helicobacter pylori CITATION, Pseudomonas aeruginosa CITATION, and Pseudomonas putida CITATION, CITATION .
	These bottom-up metabolic networks differ from other network reconstructions as they are tailored to the genomic content of the target organism and built manually using biochemical, physiological, and other experimental information in addition to the genome annotation.
	Hence, these reconstructions can be thought of as biochemically, genetically, and genomically structured knowledge bases CITATION.
	The reconstruction and modeling procedure is a 4-step process: obtaining a draft reaction list based on genome annotation and biochemical databases, refinement of reaction list using experimental information, conversion of the reaction list into a computable format and application of systems boundaries to define condition-specific models, and the evaluation and validation of the model content using various mathematical methods.
	By iterating step 2 to 4, reconstructions that are self-consistent within their defined scope can be generated.
	Metabolic network reconstruction have demonstrated to be useful in at least 5 areas of applications CITATION : biological discovery CITATION, phenotypic behavior CITATION, bacterial evolution CITATION, network analysis CITATION, and metabolic engineering CITATION.
	This wide range of applications of the metabolic reconstructions is possible because they can be readily converted into predictive, condition-specific models.
	Unlike more traditional approaches to modeling metabolism, the constraint-based modeling approach requires few, if any, parameters CITATION, CITATION.
	The stoichiometric information encoded in the reconstruction can be represented mathematically as a stoichiometric matrix, S, where the rows correspond to the components and the columns correspond to the reactions .
	While the COBRA approach has been successfully applied to metabolic networks, the same principles and assumptions can be also employed to reconstruct and model other cellular functions, such as signaling CITATION CITATION, regulation CITATION, and protein synthesis CITATION.
	In this study, we extended and refined earlier work by Allen et al., which proposed a stoichiometric formalism to model protein synthesis and illustrated it on some E. coli genes and operons CITATION.
	We created a more detailed, gene-specific representation of the transcriptional and translational processes, which explicitly accounts for the sequence-specific synthesis of DNA, mRNA, and proteins.
	This reconstruction enables quantitative integration of high-throughput data such as gene expression, proteomic, and mRNA degradation data.
	Moreover, proteins are produced in high copy numbers in growing cells; thus, any quantitative mechanistic modeling and analysis of high-throughput data needs to account for the synthesis cost associated with these molecules.
	Numerous studies have been published that investigate protein synthesis using kinetic models CITATION CITATION.
	These models are generally tailored to the questions they address making it difficult to readily apply them for modified problems.
	Since stoichiometric relationships are a common requisite for any type of mechanistic modeling, organism-specific BiGG knowledge bases can be used as templates to derive problem-specific, mechanistic models.
	In fact, network stoichiometry is a dominant feature of kinetic models as well CITATION.
	Thus, network reconstruction serves as a platform for steady-state and kinetic modeling .
	In this study, we present a new generation of network reconstructions, which directly account for the synthesis of individual mRNA and proteins.
	We named the mathematical representation of this reconstruction the Expression matrix, or E-matrix, since it encodes the expression of mRNA and proteins.
	All network reactions were formulated to account for gene-specific and E. coli-specific details, such as nucleotide composition, operon association, and sigma factor usage.
	Furthermore, we used information from three databases and more than 500 scientific publications to formulate mechanistically detailed and accurate reactions.
	This reconstruction is the first comprehensive database detailing the available information for these cellular functions and can thus be deemed a knowledge base.
	After conversion of the E-matrix reconstruction into condition-specific models corresponding to different doubling times, we were able to accurately predict the ribosome production reported in literature, without any parameterization.
	Furthermore, we show that the E-matrix can be used to study the effect of rRNA operon deletion.
	Our results predict that a high density of RNA polymerases is required on the remaining rRNA operons, to achieve the reported ribosome numbers.
	Finally, we show that proteins used in the E-matrix could be grouped into functional modules which lead to a more simplified view of the network.
	the paper extends research on fixed-pie perceptions by suggesting that disputants may prefer proposals that are perceived to be equally attractive to both parties i e , balanced rather than one-sided, because balanced agreements are seen as more likely to be successfully implemented
	we test our predictions using data on israeli support for the geneva accords, an agreement for a two state solution negotiated by unofficial delegations of israel and the palestinian authority in 2003
	the results demonstrate that israelis are more likely to support agreements that are seen favorably by other israelis, but - contrary to fixed-pie predictions - israeli support for the accords does not diminish simply because a majority of palestinians favors rather than opposes the accords
	we show that implementation concerns create a demand among israelis for balance in the degree to which each side favors or opposes the agreement
	the effect of balance is noteworthy in that it creates considerable support for proposals even when a majority of israelis and palestinians oppose the deal
	normative models of bargaining and negotiation suggest that if there is potential for mutual benefit, conflicting parties should be able to achieve it  CITATION
	descriptive accounts and empirical investigations of negotiation behavior  CITATION , however, suggest that a number of psychological barriers to conflict resolution are likely to make efficient deal making difficult  CITATION
	for example, research on cognitive biases associated with egocentric perceptions suggests that negotiators and evaluators of negotiated agreements are likely to exhibit a "fixed-pie bias"  CITATION
	the fixed-pie bias refers to the belief that any gain for one party will be associated with an equivalent loss to the other party
	this belief is a "bias" when it persists even in contexts where there is a possibility of compatible interests or mutual benefit
	a large body of research finds that negotiators are susceptible to the fixed-pie bias prior to, during, and even after negotiations  CITATION
	in the current paper we investigate and extend research on fixed pie bias in the context of protracted intergroup conflict### abstract ###
 We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors
 To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent
 We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis
 Factor analysis is the task of explaining data by means of a set of  latent factors
 Factor  regression  couples this analysis with a prediction task, where the predictions are made solely on the basis of the factor representation
 The latent factor representation achieves two-fold benefits: (1) discovering the latent  process  underlying the data; (2) simpler predictive modeling through a compact data representation
 In particular, (2) is motivated by the problem of prediction in the  ``large P small N''  paradigm  CITATION , where the number of features  SYMBOL  greatly exceeds the number of examples  SYMBOL , potentially resulting in overfitting
 We address three fundamental shortcomings of standard factor analysis approaches  CITATION : (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis
 Our motivation for this work stems from the task of reconstructing regulatory structure from gene-expression data
 In this context, factors correspond to regulatory pathways
 Our contributions thus parallel the needs of gene pathway modeling
 In addition, we couple predictive modeling (for factor regression) within the factor analysis framework itself, instead of having to model it separately
 Our factor regression model is fundamentally nonparametric
 In particular, we treat the gene-to-factor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process (IBP)  CITATION , designed to account for the sparsity of relevant genes (features)
 We  couple  this IBP with a hierarchical prior over the factors
 This prior explains the fact that pathways are fundamentally related: some are involved in transcription, some in signaling, some in synthesis
 The nonparametric nature of our sparse IBP requires that the hierarchical prior  also  be nonparametric
 A natural choice is Kingman's coalescent  CITATION , a popular distribution over infinite binary trees
 Since our motivation is an application in bioinformatics, our notation and terminology will be drawn from that area
 In particular,  genes  are  features ,  samples  are  examples , and  pathways  are  factors
 However, our model is more general
 An alternative application might be to a collaborative filtering problem, in which case our genes might correspond to movies, our samples might correspond to users and our pathways might correspond to genres
 In this context, all three contributions of our model still make sense: we do not know how many movie genres there are; some genres are closely related (romance to comedy versus to action); many movies may be spurious
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its perceived disutility relative to not testing is greater than the utility associated with prevention of possible disease
--the prospect theory editing operation, by which a decision maker's reference point is determined, can have important effects on the disutility of the test
--on the basis of the prospect theory value function, this paper develops two approaches to reducing disutility by directing the decision maker's attention to either actual past or expected future losses that result in shifted reference points
--after providing a graphical description of the approaches and a mathematical proof of the direction of their effect on judgment, we briefly illustrate the potential value of these approaches with examples from qualitative research on prostate cancer treatment decisions
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its disutility relative to not testing is greater than the utility associated with prevention of possible disease
--for example, people may feel that the anticipated disutility of a colonoscopy for colorectal cancer screening is great enough relative to the expected utilty of prevention of possible colorectal cancer to dissuade them from seeking colonoscopy
--the prospect theory editing operation  CITATION , by which a decision maker's reference point is determined, can have important impacts on the perceived disutility of the test
--the work of rothman, salovey, and colleagues on message framing has tested prospect theory predictions of how the description of test outcomes as gains or losses as well as the conceptualization of the purpose of the test as preventative vs diagnostic and the consequent perception of whether the test is "safe" or "risky" can affect test rates  CITATION
--specifically, message framing theories predict that when a procedure is perceived as risky e g , cancer screening tests may cause a patient to find out that they have cancer, loss-framed messages will promote testing more strongly than gain-framed messages, because people favor risky prospects over sure prospects in the domain of losses
--on the other hand, when a procedure is perceived as safe e g , sunscreen prevents sunburn and skin cancer, gain-framed messages are predicted to be more effective because people prefer sure prospects to risky prospects in the domain of gains
--several public health intervention studies have examined message framing and generally found evidence favoring the predictions  CITATION
--on the basis of the prospect theory value function, this paper develops two approaches to reducing perceived disutility by directing the decision maker's attention to either actual past or expected future losses that can serve as reference points and are not consequences of the test itself
--these approaches thus differ from message framing, which focuses on how the test outcomes are described and manipulates gain and loss framing
--we instead derive the potential impact of directly refocusing the decision maker's reference point
 Numerous psychophysical experiments found that humans preferably rely on a narrow band of spatial frequencies for recognition of face identity.
 A recently conducted theoretical study by the author suggests that this frequency preference reflects an adaptation of the brain's face processing machinery to this specific stimulus class.
 The purpose of the present study is to examine this property in greater detail and to specifically elucidate the implication of internal face features.
 To this end, I parameterized Gabor filters to match the spatial receptive field of contrast sensitive neurons in the primary visual cortex.
 Filter responses to a large number of face images were computed, aligned for internal face features, and response-equalized.
 The results demonstrate that the frequency preference is caused by internal face features.
 Thus, the psychophysically observed human frequency bias for face processing seems to be specifically caused by the intrinsic spatial frequency content of internal face features.
 In the brain, the structure of neuronal circuits for processing sensory information matches the statistical properties of the sensory signals CITATION.
 Taking advantage of these statistical regularities contributes to an optimal encoding of sensory signals in neuronal responses, in the sense that the code conveys the highest information with respect to specific constraints CITATION CITATION.
 Among the various constraints which were formulated we find, for example, keeping metabolic energy consumption as low as possible CITATION CITATION, or keeping total wiring length between processing units at a minimum CITATION, or maximizing the suppression of spatio-temporal redundancy in the input signal CITATION, CITATION CITATION .
 As for visual stimuli, natural images reveal a conspicuous statistical regularity that comes as an approximately linear decrease of their amplitude spectra as a function of spatial frequency CITATION CITATION.
 This means that pairs of luminance values are strongly correlated CITATION, and this property could be exploited for gain controlling of visual neurons.
 Then, visual neurons would have equal sensitivities or response amplitudes independent of their spatial frequency preference CITATION.
 According to this response equalization hypothesis, gain should thus be incremented with increasing spatial frequency, such that the distribution of response amplitudes of frequency-tuned neurons to a typical natural image is flat.
 An argument in favor of employing response equalization is that it would lead to an improvement of information transmission from one neuronal stage to another, because the output of one stage would match the limited dynamic range of a second one CITATION .
 The present article builds upon previously reported results for whitened amplitude spectra of face images CITATION : the whitened spectra reveal a spatial frequency maximum at 10 15 cycles per face, but only if external face features are suppressed.
 The predicted frequency maximum nevertheless agrees well with numerous psychophysical experiments, which found that face identity is preferably processed in a narrow band of spatial frequencies from 8 to 16 cycles per face CITATION CITATION .
 Despite of it all, the results presented in CITATION indicate that the maxima in the amplitude spectra are caused by the compound effect of horizontally oriented internal face features.
 Quantitatively, the maxima thus occur in units of cycles per face height, whereas most psychophysical studies instead measure their results in terms of cycles per face width.
 Furthermore, although a clear enhancement of horizontal amplitudes could be observed in the spectra, horizontal amplitudes showed a somewhat noisy dependence on spatial frequency.
 Both effects are a consequence of that face features were not considered individually, what causes a mixing of the spatial frequency content of individual face features in the spectra.
 The mixing leads to averaging-out effects such that any possible enhancement of spectral amplitudes at other than the horizontal orientation goes unnoticed, but also may cause interference effects which lead to the mentioned noisy dependence of amplitudes on spatial frequency.
 The present study addresses the two issues by means of an extensive analysis of face images by means of Gabor filters.
 The filters were thereby parameterized to match the spatial receptive field of band-limited, oriented and contrast sensitive neurons in the primary visual cortex CITATION CITATION.
 Great care has been taken to guarantee the correct alignment of filter responses with respect to the position of internal face features prior to their averaging.
 Doing so permits to precisely elucidate how the frequency dependence of Gabor responses is related to each of the four internal face features.
 The resulting graphs of whitened Gabor amplitudes versus spatial frequency are smooth and reveal distinct maxima at nearly all orientations.
 The most stable maxima, however, are observed at horizontal feature orientations in the first place, but also at vertical orientations.
 This observation holds true for all of the internal face features.
 The present study therefore shows how the individual internal face features contribute to the psychophysically observed frequency preference, and proposes concrete mechanisms of how higher amplitudes of whitened cell responses at an early level could possibly lead to the psychophysically measured effects.
 We introduce a new principle for model selection in regression and classification
 Many regression models are controlled by some smoothness or flexibility or complexity parameter  SYMBOL , eg the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials
 Let  SYMBOL  be the (best) regressor of complexity  SYMBOL  on data  SYMBOL
 A more flexible regressor can fit more data  SYMBOL  well than a more rigid one
 If something (here small loss) is easy to achieve it's typically worth less
 We define the loss rank of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL
 We suggest selecting the model complexity  SYMBOL  that has minimal loss rank (LoRP)
 Unlike most penalized maximum likelihood variants (AIC,BIC,MDL), LoRP only depends on the regression functions and the loss function
 It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN
 In this paper we formalize, discuss, and motivate LoRP, study it for specific regression problems, in particular linear ones, and compare it to other model selection schemes
 Consider a regression or classification problem in which we want to determine the functional relationship  SYMBOL  from data  SYMBOL , ie we seek a function  SYMBOL  such that  SYMBOL  is close to the unknown  SYMBOL  for all  SYMBOL
 One may define regressor  SYMBOL  directly, eg `average the  SYMBOL  values of the  SYMBOL  nearest neighbors (kNN) of  SYMBOL  in  SYMBOL ', or select the  SYMBOL  from a class of functions  SYMBOL  that has smallest (training) error on  SYMBOL
 If the class  SYMBOL  is not too large, e g the polynomials of fixed reasonable degree  SYMBOL , this often works well
 What remains is to select the right model complexity  SYMBOL , like  SYMBOL  or  SYMBOL
 This selection cannot be based on the training error, since the more complex the model (large  SYMBOL , small  SYMBOL ) the better the fit on  SYMBOL  (perfect for  SYMBOL  and  SYMBOL )
 This problem is called overfitting, for which various remedies have been suggested:  We will not discuss empirical test set methods like cross-validation, but only training set based methods
 See eg CITATION  for a comparison of cross-validation with Bayesian model selection
 Training set based model selection methods allow using all data  SYMBOL  for regression
 The most popular ones can be regarded as penalized versions of Maximum Likelihood (ML)
 In addition to the function class  SYMBOL , one has to specify a sampling model  SYMBOL , eg that the  SYMBOL  have independent Gaussian distribution with mean  SYMBOL
 ML chooses  SYMBOL , Penalized ML (PML) then chooses  SYMBOL Penalty SYMBOL , where the penalty depends on the used approach (MDL  CITATION , BIC  CITATION , AIC  CITATION )
 In particular, modern MDL  CITATION  has sound exact foundations and works very well in practice
 All PML variants rely on a proper sampling model (which may be difficult to establish), ignore (or at least do not tell how to incorporate) a potentially given loss function, and are typically limited to (semi)parametric models
 The main goal of the paper is to establish a criterion for selecting the ``best'' model complexity  SYMBOL based on regressors  SYMBOL  given as a black box without insight into the origin or inner structure of  SYMBOL , that does not depend on things often not given (like a stochastic noise model),  and that exploits what is given (like the loss function)
 The key observation we exploit is that large classes  SYMBOL  or more flexible   regressors  SYMBOL  can fit more data  SYMBOL  well than more rigid ones, eg many  SYMBOL  can be fit well with high order polynomials
 We define the  loss rank  of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL , as measured by some loss function
 The loss rank is large for regressors fitting  SYMBOL  not well  and  for too flexible regressors (in both cases the regressor fits many other  SYMBOL  better)
 The loss rank has a minimum for not too flexible regressors which fit  SYMBOL  not too bad
 We claim that minimizing the loss rank is a suitable model selection criterion, since it trades off the quality of fit with the flexibility of the model
 Unlike PML, our new Loss Rank Principle (LoRP) works without a noise (stochastic sampling) model, and is directly applicable to any non-parametric regressor, like kNN
 In Section , after giving a brief introduction to regression, we formally state LoRP for model selection
 To make it applicable to real problems, we have to generalize it to continuous spaces and regularize infinite loss ranks
 In Section  we derive explicit expressions for the loss rank for the important class of linear regressors, which includes kNN, polynomial, linear basis function (LBFR), Kernel, and projective regression
 In Section  we compare linear LoRP to Bayesian model selection for linear regression with Gaussian noise and prior, and in Section  to PML, in particular MDL, BIC, AIC, and MacKay's  CITATION  and Hastie's et al  CITATION  trace formulas for the effective dimension
 In this paper we just scratch at the surface of LoRP
 Section  contains further considerations, to be elaborated on in the future
 Regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems
 Bundle methods for regularized risk minimization (BMRM) and the closely related SVMStruct are considered the best general purpose solvers to tackle this problem
 It was recently shown that BMRM requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution
 In the first part of the paper we use the Hadamard matrix to construct a regularized risk minimization problem and show that these rates cannot be improved
 We then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an  SYMBOL  accurate solution in  SYMBOL  iterations
 Let  SYMBOL  denote samples and  SYMBOL  be the corresponding labels
 Given a training set of  SYMBOL  sample label pairs  SYMBOL , drawn iid from a joint probability distribution on  SYMBOL , many machine learning algorithms solve the following regularized risk minimization problem:     
 Here  SYMBOL  denotes the loss on instance  SYMBOL  using the current model  SYMBOL  and  SYMBOL , the empirical risk, is the average loss on the training set
 The regularizer  SYMBOL  acts as a penalty on the complexity of the classifier and prevents overfitting
 Usually the loss is convex in  SYMBOL  but can be nonsmooth while the regularizer is usually a smooth strongly convex function
 Binary Support Vector Machines (SVMs) are a prototypical example of such regularized risk minimization problems where  SYMBOL  and the loss considered is the binary hinge loss:    
 Recently, a number of solvers have been proposed for the regularized risk minimization problem
 The first and perhaps the best known solver is SVMStruct  CITATION , which was shown to converge in  SYMBOL  iterations to an  SYMBOL  accurate solution
 The convergence analysis of SVMStruct was improved to  SYMBOL  iterations by  CITATION
 In fact,  CITATION  showed that their convergence analysis holds for a more general solver than SVMStruct namely BMRM (Bundle method for regularized risk minimization)
 At every iteration BMRM replaces  SYMBOL  by a piecewise linear lower bound  SYMBOL  and optimizes    to obtain the next iterate  SYMBOL
 Here  SYMBOL  denotes an arbitrary subgradient of  SYMBOL  at  SYMBOL  (see Section ) and  SYMBOL
 The piecewise linear lower bound is successively tightened until the gap     falls below a predefined tolerance  SYMBOL
 Even though BMRM solves an expensive optimization problem at every iteration, the convergence analysis only uses a simple one-dimensional line search to bound the decrease in  SYMBOL
 Furthermore, the empirical convergence behavior of BMRM is much better than the theoretically predicted rates on a number of real life problems
 It was therefore conjectured that the rates of convergence of BMRM could be improved
 In this paper we answer this question in the negative by explicitly constructing a regularized risk minimization problem for which BMRM takes at least  SYMBOL  iterations
 One possible way to circumvent the  SYMBOL  lower bound is to solve the problem in the dual
 Using a very old result of Nesterov  CITATION  we obtain an algorithm for SVMs which only requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution; each iteration of the algorithm requires  SYMBOL  work
 Although we primarily focus on the regularized risk minimization with the binary hinge loss, our algorithm can also be used whenever the empirical risk is piecewise linear and contains a small number of pieces
 Examples of this include multiclass, multi-label, and ordinal regression hinge loss and other related losses### abstract ###
	this paper extends previous research showing that experienced difficulty of recall can influence evaluative judgments CITATION to a field study of university students rating a course
	students completed a mid-course evaluation form in which they were asked to list either  NUMBER  ways in which the course could be improved a relatively easy task or  NUMBER  ways in which the course could be improved a relatively difficult task
	respondents who had been asked for  NUMBER  critical comments subsequently rated the course more favorably than respondents who had been asked for  NUMBER  critical comments
	an internal analysis suggests that the number of critiques solicited provides a frame against which accessibility of instances is evaluated
	the paper concludes with a discussion of implications of the present results and possible directions for future research
	according to tversky and kahneman's  CITATION  availability heuristic  people sometimes judge the frequency of events in the world by the ease with which examples come to mind
	this process has generally been demonstrated by asking participants to assess the relative likelihood of two categories in which instances of the first category are more difficult to recall than instances of the second category  despite the fact that instances of the first category are more common in the world
	for instance  kahneman and tversky  CITATION  found that most people think the letter r more often appears in english words as the first letter than the third letter  presumably because the first letter provides a better cue for recalling instances of words than does the third letter
	in fact  it turns out that r appears more often as the third than first letter in english words
	schwarz et al CITATION  observed that the classic studies demonstrating the availability heuristic failed to distinguish an interpretation based on ease of retrieval from an alternative interpretation based on content of retrieval in which an event is judged more common when a larger number of examples come to mind
	to tease apart these accounts  schwarz et al CITATION  asked participants in one   study to list either  NUMBER  or  NUMBER  examples of assertive or unassertive behavior that they have exhibited and then rate themselves on their overall degree of assertiveness
	participants rated themselves as more assertive after they had listed  NUMBER  examples of assertive behavior a relatively easy task rather than  NUMBER  examples a relatively difficult task  similarly  they rated themselves as less assertive i e   more unassertive after they had listed  NUMBER  rather than  NUMBER  examples of unassertive behavior
	similar patterns of results have been observed in many other studies of frequency-related judgments  including the rate at which a particular letter occurs in various positions of words  CITATION   the quality of one's own memory  CITATION   the frequency of one's own past behaviors  CITATION   one's susceptibility to heart disease  CITATION  and one's susceptibility to sexual assault  CITATION
	for a review of this literature see schwarz  CITATION
	thus  an abundance of data supports the original interpretation of the availability heuristic  categories are judged to be more common when instances more easily come to mind  even when a smaller absolute number of instances are generated
	this program has been extended from frequency-based judgments to  evaluative judgments of such targets as public transportation  CITATION   luxury automobiles  CITATION   and one's own childhood  CITATION
	for instance  winkielman and schwarz  CITATION  asked participants to recall either  NUMBER  childhood events an easy task or  NUMBER  childhood events a difficult task
	some participants were then led to believe that memories from pleasant periods tend to fade  while others were led to believe that memories from unpleasant periods tend to fade
	when later asked to evaluate their childhood  participants believed that pleasant memories fade rated their childhood more favorably when they completed the difficult task  NUMBER  events than the easy task  NUMBER  events  participants who believed that unpleasant memories fade rated their childhood more favorably when they completed the easy rather than difficult task
	previous studies of the availability heuristic using the paradigm of schwarz et al CITATION  have turned up impressive and robust results
	however  these demonstrations have been restricted primarily to laboratory surveys in which task of recalling examples then making an overall assessment may seem somewhat artificial to participants and the responses of little consequence
	more important  most participants in previous studies presumably had little prior experience with the particular likert scale that served as the dependent measure e g   most had never before rated their childhood or public transportation on a  NUMBER -point scale
	hence  ratings of respondents may be especially susceptible to superficial cues-such as the accessibility of instances-when mapping their beliefs and attitudes onto an unfamiliar response scale
	the present investigation overcomes these limitations through a  field study  of students evaluating a course
	first  evaluations are a normal facet of most university courses in which students are commonly asked to list specific suggestions and also provide a global assessment
	moreover  course evaluations are consequential  as they can influence future course offerings and course staffing  promotion and tenure decisions  and provide information to future prospective students of the target course
	second  students at universities quickly become familiar with standard course evaluation scales and how ratings are distributed across classes  often relying on these scores in choosing among elective courses
	the study of course evaluations is also interesting in its own right
	a number of recent papers have questioned the validity of these ratings  and a lively debate appeared some years ago in the american psychologist  CITATION
	thus far  questions of discriminant validity have mainly focused on the correlation between teaching ratings and apparently irrelevant factors such as the students' expected grades or the course workload
	to date there have been few published investigations of the relationship between the design of course feedback forms and summary course evaluations
	the present study attempts to answer the following provocative question  can one paradoxically obtain higher course ratings by soliciting a greater number of critical comments from students### abstract ###
	Human Immunodeficiency Virus 1 uses for entry into host cells a receptor and one of two co-receptors.
	Recently, a new class of antiretroviral drugs has entered clinical practice that specifically bind to the co-receptor CCR5, and thus inhibit virus entry.
	Accurate prediction of the co-receptor used by the virus in the patient is important as it allows for personalized selection of effective drugs and prognosis of disease progression.
	We have investigated whether it is possible to predict co-receptor usage accurately by analyzing the amino acid sequence of the main determinant of co-receptor usage, i.e., the third variable loop V3 of the gp120 protein.
	We developed a two-level machine learning approach that in the first level considers two different properties important for protein-protein binding derived from structural models of V3 and V3 sequences.
	The second level combines the two predictions of the first level.
	The two-level method predicts usage of CXCR4 co-receptor for new V3 sequences within seconds, with an area under the ROC curve of 0.937 0.004.
	Moreover, it is relatively robust against insertions and deletions, which frequently occur in V3.
	The approach could help clinicians to find optimal personalized treatments, and it offers new insights into the molecular basis of co-receptor usage.
	For instance, it quantifies the importance for co-receptor usage of a pocket that probably is responsible for binding sulfated tyrosine.
	Specific protein interactions are central to biological processes, and the infection of cells with viruses is no exception there.
	In the case of pathogenic viruses, such protein interactions are potential targets for medical intervention.
	An example of particularly high relevance is Human Immunodeficiency Virus 1.
	HIV-1 enters human cells in a process that comprises several steps, including the binding of the viral gp120 protein to the cellular receptor protein CD4 and a co-receptor protein, usually one of the two chemokine receptors CCR5 and CXCR4 CITATION.
	The type of co-receptor used by the virus, the so-called co-receptor tropism, has a prognostic value, since patients with a CXCR4-tropic virus progress faster to Acquired Immunodeficiency Syndrome compared to patients with a CCR5-tropic virus CITATION.
	In addition to the purely X4- and R5-tropic viruses, there are also dual-tropic strains, able to use both co-receptors.
	Recently, the first drug that binds to CCR5, and thus inhibits productive binding of gp120, has been approved by regulatory authorities in several countries.
	This has made the determination of co-receptor tropism directly relevant to anti-retroviral treatment, as CCR5-inhibitors are of course inactive against X4 virus.
	The standard way of determining co-receptor tropism is by cell-based assays CITATION, CITATION.
	The main drawbacks of these assays are that they are currently only carried out by a handful of specialized laboratories worldwide, and that the overall procedure typically takes several weeks.
	These impediments to the wide application of entry inhibitors could be overcome by an approach similar to genotypic drug resistance testing CITATION, where drug resistance of a viral strain is inferred from comparison of mutational patterns obtained from sequencing parts of the genome of that strain with patterns of validated resistance mutations.
	This is a relatively fast and cheap standard procedure established in many clinics.
	At first glance, genotypic testing for co-receptor tropism seems to be possible since the main molecular determinant of tropism is known to be the third variable loop of the viral glycoprotein gp120 CITATION, a peptide stretch of about 35 amino-acids with a disulfide bridge connecting the terminal cysteins.
	Unfortunately, as suggested by its name, V3 is notorious for its high sequence variability CITATION including also some variability in length, and this has made it difficult to use it as a basis for genotypic co-receptor tropism testing.
	Nevertheless, the relevance of the quest has prompted many groups to develop models that link properties of V3 to co-receptor tropism.
	The importance of electrostatics for co-receptor tropism has been recognized early on, and the best-known model, the so-called 11/25-rule, refers to charges of V3-residues 11 and 25: if one of these is positive, then the virus is CXCR4-tropic CITATION, CITATION.
	This rule has a specificity of more than 0.9, but only a low to moderate sensitivity of about 0.4 0.6, depending on the test data, which is not satisfactory for routine clinical application.
	To improve predictions from sequence, several groups have applied machine learning methods, such as artificial neural networks CITATION, position specific scoring matrices CITATION, decision trees, or support vector machines CITATION.
	Still, prediction accuracies fall short of what seems reasonable for regular clinical use CITATION.
	It is unclear whether the limited accuracies are the footprint of tropism-determinants outside V3, or the consequence of model imperfections.
	A milestone for the understanding of co-receptor tropism was the X-ray structure of gp120 with the V3 loop in a biological context CITATION.
	This paved the way for the development of prediction methods that use, in addition to V3 sequence, structural information.
	To our knowledge, the first of these methods has been that of Sander et al. CITATION, which was mainly based on geometric distances of amino-acid pairs within the structure of V3.
	Although our method, detailed in the following, relies on the same experimental structure by Huang et al. CITATION, it differs from that of Sander et al. in several respects, e.g. it deals with indels, and, perhaps most crucially, it uses as descriptors properties that directly determine interaction of V3 with the co-receptors.
	By the latter we consider a seemingly trivial but fundamental fact that so far has not been thoroughly exploited: although V3 is highly variable, all X4-tropic V3 loops share one property, namely, they preferentially have a physical binding interaction with CXCR4, while R5-tropic V3 loops preferably interacts with CCR5.
	The accuracy of the method makes it attractive as clinical tool for patient tailored decisions on treatment with entry inhibitors, and it suggests that co-receptor tropism can be explained almost exclusively based on V3.### abstract ###
 The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied
 If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence speed
 For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures
 We show that this is even the case if the model class contains only Bernoulli distributions
 We derive a new upper bound on the prediction error for countable Bernoulli classes
 This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes
 We discuss the application to Machine Learning tasks such as classification and hypothesis testing, and generalization to countable classes of iid models
 ``Bayes mixture", ``Solomonoff induction", ``marginalization", all these terms refer to a central induction principle: Obtain a predictive distribution by integrating the product of prior and evidence over the model class
 In many cases however, the Bayes mixture is computationally infeasible, and even a sophisticated approximation is expensive
 The MDL or MAP (maximum a posteriori) estimator is both a common approximation for the Bayes mixture and interesting for its own sake: Use the model with the largest product of prior and evidence
 In practice, the MDL estimator is usually being approximated too, since only a local maximum is determined
 How good are the predictions by Bayes mixtures and MDL
 This question has attracted much attention
 In many cases, an important quality measure is the  total  or cumulative  expected loss  of a predictor
 In particular the square loss is often considered
 Assume that the outcome space is finite, and the model class is continuously parameterized
 Then for Bayes mixture prediction, the cumulative expected square loss is usually small but unbounded, growing with  SYMBOL , where  SYMBOL  is the sample size  CITATION
 This corresponds to an  instantaneous  loss bound of  SYMBOL
 For the MDL predictor, the losses behave similarly  CITATION  under appropriate conditions, in particular with a specific prior
 Note that in order to do MDL for continuous model classes, one needs to  discretize  the parameter space, see also  CITATION
 On the other hand, if the model class is discrete, then Solomonoff's theorem  CITATION  bounds the cumulative expected square loss for the Bayes mixture predictions finitely, namely by  SYMBOL , where  SYMBOL  is the prior weight of the ``true" model  SYMBOL
 The only necessary assumption is that the true distribution  SYMBOL  is contained in the model class, ie that we are dealing with  proper learning
 It has been demonstrated  CITATION , that for both Bayes mixture and MDL, the proper learning assumption can be essential: If it is violated, then learning may fail very badly
 For MDL predictions in the proper learning case, it has been shown  CITATION  that a bound of  SYMBOL  holds
 This bound is exponentially larger than the Solomonoff bound, and it is sharp in general
 A finite bound on the total expected square loss is particularly interesting:   It implies convergence of the predictive to the true probabilities with probability one
 In contrast, an instantaneous loss bound of  SYMBOL  implies only convergence in probability
 Additionally, it gives a  convergence speed , in the sense that errors of a certain magnitude cannot occur too often
 So for both, Bayes mixtures and MDL, convergence with probability one holds, while the convergence speed is exponentially worse for MDL compared to the Bayes mixture
 We avoid the term ``convergence rate" here, since the order of convergence is identical in both cases
 It is eg  SYMBOL  if we additionally assume that the error is monotonically decreasing, which is not necessarily true in general
 It is therefore natural to ask if there are model classes where the cumulative loss of MDL is comparable to that of Bayes mixture predictions
 In the present work, we concentrate on the simplest possible stochastic case, namely discrete Bernoulli classes
 Note that then the MDL ``predictor" just becomes an estimator, in that it estimates the true parameter and directly uses that for prediction
 Nevertheless, for consistency of terminology, we keep the term predictor
 It might be surprising to discover that in general the cumulative loss is still exponential
 On the other hand, we will give mild conditions on the prior guaranteeing a small bound
 Moreover, it is well-known that the instantaneous square loss of the Maximum Likelihood estimator decays as  SYMBOL  in the Bernoulli case
 The same holds for MDL, as we will see
 If convergence speed is measured in terms of instantaneous losses, then much more general statements are possible  CITATION , this is briefly discussed in Section
 A particular motivation to consider discrete model classes arises in Algorithmic Information Theory
 From a computational point of view, the largest relevant model class is the class of all computable models on some fixed universal Turing machine, precisely prefix machine  CITATION
 Thus each model corresponds to a program, and there are countably many programs
 Moreover, the models are stochastic, precisely they are  semimeasures  on strings (programs need not halt, otherwise the models were even measures)
 Each model has a natural description length, namely the length of the corresponding program
 If we agree that programs are binary strings, then a prior is defined by two to the negative description length
 By the Kraft inequality, the priors sum up to at most one
 Also the Bernoulli case can be studied in the view of Algorithmic Information Theory
 We call this the  universal setup : Given a universal Turing machine, the related class of Bernoulli distributions is isomorphic to the countable set of computable reals in  SYMBOL
 The description length  SYMBOL  of a parameter  SYMBOL  is then given by the length of its shortest program
 A prior weight may then be defined by  SYMBOL
 If a string  SYMBOL  is generated by a Bernoulli distribution with computable parameter  SYMBOL , then with high probability the two-part complexity of  SYMBOL  with respect to the Bernoulli class does not exceed its algorithmic complexity by more than a constant, as shown by Vovk  CITATION
 That is, the two-part complexity with respect to the Bernoulli class  is  the shortest description, save for an additive constant
 Many Machine Learning tasks are or can be reduced to sequence prediction tasks
 An important example is classification
 The task of classifying a new instance  SYMBOL  after having seen (instance,class) pairs  SYMBOL  can be phrased as to predict the continuation of the sequence  SYMBOL
 Typically the (instance,class) pairs are iid
 Cumulative loss bounds for prediction usually generalize to prediction  conditionalized  to some inputs  CITATION
 Then we can solve classification problems in the standard form
 It is not obvious if and how the proofs in this paper can be conditionalized
 Our main tool for obtaining results is the Kullback-Leibler divergence
 Lemmata for this quantity are stated in Section
 Section  shows that the exponential error bound obtained in  CITATION  is sharp in general
 In Section , we give an upper bound on the instantaneous and the cumulative losses
 The latter bound is small eg under certain conditions on the distribution of the weights, this is the subject of Section
 Section  treats the universal setup
 Finally, in Section  we discuss the results and give conclusions### abstract ###
 previous tests of cumulative prospect theory cpt and of the priority heuristic ph found evidence contradicting these two models of risky decision making
 however, those tests were criticized because they had characteristics that might "trigger" use of other heuristics
 this paper presents new tests that avoid those characteristics
 expected values of the gambles are nearly equal in each choice
 in addition, if a person followed expected value ev, expected utility eu, cpt, or ph in these tests, she would shift her preferences in the same direction as shifts in ev or eu
 in contrast, the transfer of attention exchange model tax and a similarity model predict that people will reverse preferences in the opposite direction
 results contradict the ph, even when ph is modified to include a preliminary similarity evaluation using the ph parameters
 new tests of probability-consequence interaction were also conducted
 strong interactions were observed, contrary to ph
 these results add to the growing bodies of evidence showing that neither cpt nor ph is an accurate description of risky decision making
 this paper compares three models that attempt to describe risky decision making
 these models are cumulative prospect theory cpt  CITATION , birnbaum's  CITATION  transfer of attention exchange model tax, and the priority heuristic ph of brandstatter, gigerenzer, and hertwig  CITATION
 the ph model is based on the idea that people compare one attribute at a time, such as the minimum prizes
 in addition, the similarity model of rubinstein  CITATION  as modified by leland  CITATION  is also relevant to these studies, although these studies were not designed to test that model
 birnbaum  CITATION  reviewed a number of critical tests that refute any rank dependent utility rdu model  CITATION  including rank and sign-dependent utility  CITATION , cpt, and expected utility eu
 birnbaum  CITATION  noted that many of the same tests that refute cpt also contradict the priority heuristic
 for example, the priority heuristic predicted fewer than half of the modal choices analyzed by birnbaum  CITATION , by birnbaum  CITATION , and by birnbaum and navarrete  CITATION
 some of these choices included cases where 90% or more of the participants satisfied stochastic dominance but the priority heuristic predicts indifference
 in other choices, significantly more than half of the participants about 70% of undergraduates violated stochastic dominance, but the priority heuristic predicts that people should satisfy it
 brandstatter, gigerenzer, and hertwig  CITATION  responded that properties of these choices may have induced people to use other heuristics drawn from a person's "adaptive toolbox"
 presumably, decision makers first decide what rule to use, then they either apply that rule or choose to use another rule
 the mechanism that decides what rule to use has not yet been specified; it is described instead with lists of "triggering conditions," which are estimated from data like parameters
 brandstatter et al CITATION  concluded that the priority heuristic does not apply when there is a stochastic dominance relation in the choice
 in addition, brandstatter et al CITATION  argued that certain choices reviewed by birnbaum  CITATION  used gambles that differed in expected value ev
 brandstatter et al CITATION  presented a figure to show that the priority heuristic is not accurate when expected values evs differ, which led them to suppose that two strategies are at work, one for "easy" choices that differ in ev and one for "harder" choices where evs are nearly equal
 from the data, brandstatter et al CITATION  estimated that, when the ratio of ev exceeds 2, people act as if they choose the gamble with the higher ev
 brandstatter et al consider ev ratio as a proxy for the "difficulty" of a choice, but do not necessarily hold that people actually compute ratios of ev
 they argued that the priority heuristic is accurate for "difficult" choices in which evs are nearly equal
 however, birnbaum  CITATION  noted that ev ratios in birnbaum and navarrete  CITATION  had been inside the region where ph is supposed to apply; in that study, the priority heuristic failed to reproduce even half of the modal choices correctly
 brandstatter et al CITATION  replicated part of that study and their results confirmed that the priority heuristic reproduced fewer than half of the modal choices that they chose for replication  CITATION
 to account for the results, brandstatter et al noted that birnbaum and navarrete  CITATION  used many choices in which both gambles of a choice had the same probability distribution and in some choices two branches had the same probability
 ph was not accurate for such choices, so brandstatter, et al CITATION  theorized that people use a "toting up" heuristic for choices in which two branches had the same probability
 in some of the choices in birnbaum and navarrete  CITATION , there was a common probability-consequence branch in both choices, which was theorized to trigger editing rules and other heuristics that were called up to account for the failures of the priority heuristic
 the arguments of brandstatter et al CITATION  might also provide excuses for previous failures of cpt as well
 this paper devises a new type of test that avoids the exceptions stated above
 in these tests, one alternative does not stochastically dominate the other, there are no common probability-consequence branches, probabilities of the consequences are not equal, and expected values are nearly equal
 in addition, unlike previous tests, the new tests use shifts in expected value and expected utility to "help" predictions of ph and cpt
 that is, expected value and expected utility are both manipulated such that, if a person shifts his or her judgments in the same direction as the changes in eu or ev, his or her choices will appear consistent with ph and cpt
 however, the choices are designed so that the tax model with parameters typical of previous research predicts that people will shift their choices in the opposite direction of ev, eu, cpt, and ph
 Human Immunodeficiency Virus 1 uses for entry into host cells a receptor and one of two co-receptors.
 Recently, a new class of antiretroviral drugs has entered clinical practice that specifically bind to the co-receptor CCR5, and thus inhibit virus entry.
 Accurate prediction of the co-receptor used by the virus in the patient is important as it allows for personalized selection of effective drugs and prognosis of disease progression.
 We have investigated whether it is possible to predict co-receptor usage accurately by analyzing the amino acid sequence of the main determinant of co-receptor usage, i.e., the third variable loop V3 of the gp120 protein.
 We developed a two-level machine learning approach that in the first level considers two different properties important for protein-protein binding derived from structural models of V3 and V3 sequences.
 The second level combines the two predictions of the first level.
 The two-level method predicts usage of CXCR4 co-receptor for new V3 sequences within seconds, with an area under the ROC curve of 0.937 0.004.
 Moreover, it is relatively robust against insertions and deletions, which frequently occur in V3.
 The approach could help clinicians to find optimal personalized treatments, and it offers new insights into the molecular basis of co-receptor usage.
 For instance, it quantifies the importance for co-receptor usage of a pocket that probably is responsible for binding sulfated tyrosine.
 Specific protein interactions are central to biological processes, and the infection of cells with viruses is no exception there.
 In the case of pathogenic viruses, such protein interactions are potential targets for medical intervention.
 An example of particularly high relevance is Human Immunodeficiency Virus 1.
 HIV-1 enters human cells in a process that comprises several steps, including the binding of the viral gp120 protein to the cellular receptor protein CD4 and a co-receptor protein, usually one of the two chemokine receptors CCR5 and CXCR4 CITATION.
 The type of co-receptor used by the virus, the so-called co-receptor tropism, has a prognostic value, since patients with a CXCR4-tropic virus progress faster to Acquired Immunodeficiency Syndrome compared to patients with a CCR5-tropic virus CITATION.
 In addition to the purely X4- and R5-tropic viruses, there are also dual-tropic strains, able to use both co-receptors.
 Recently, the first drug that binds to CCR5, and thus inhibits productive binding of gp120, has been approved by regulatory authorities in several countries.
 This has made the determination of co-receptor tropism directly relevant to anti-retroviral treatment, as CCR5-inhibitors are of course inactive against X4 virus.
 The standard way of determining co-receptor tropism is by cell-based assays CITATION, CITATION.
 The main drawbacks of these assays are that they are currently only carried out by a handful of specialized laboratories worldwide, and that the overall procedure typically takes several weeks.
 These impediments to the wide application of entry inhibitors could be overcome by an approach similar to genotypic drug resistance testing CITATION, where drug resistance of a viral strain is inferred from comparison of mutational patterns obtained from sequencing parts of the genome of that strain with patterns of validated resistance mutations.
 This is a relatively fast and cheap standard procedure established in many clinics.
 At first glance, genotypic testing for co-receptor tropism seems to be possible since the main molecular determinant of tropism is known to be the third variable loop of the viral glycoprotein gp120 CITATION, a peptide stretch of about 35 amino-acids with a disulfide bridge connecting the terminal cysteins.
 Unfortunately, as suggested by its name, V3 is notorious for its high sequence variability CITATION including also some variability in length, and this has made it difficult to use it as a basis for genotypic co-receptor tropism testing.
 Nevertheless, the relevance of the quest has prompted many groups to develop models that link properties of V3 to co-receptor tropism.
 The importance of electrostatics for co-receptor tropism has been recognized early on, and the best-known model, the so-called 11/25-rule, refers to charges of V3-residues 11 and 25: if one of these is positive, then the virus is CXCR4-tropic CITATION, CITATION.
 This rule has a specificity of more than 0.9, but only a low to moderate sensitivity of about 0.4 0.6, depending on the test data, which is not satisfactory for routine clinical application.
 To improve predictions from sequence, several groups have applied machine learning methods, such as artificial neural networks CITATION, position specific scoring matrices CITATION, decision trees, or support vector machines CITATION.
 Still, prediction accuracies fall short of what seems reasonable for regular clinical use CITATION.
 It is unclear whether the limited accuracies are the footprint of tropism-determinants outside V3, or the consequence of model imperfections.
 A milestone for the understanding of co-receptor tropism was the X-ray structure of gp120 with the V3 loop in a biological context CITATION.
 This paved the way for the development of prediction methods that use, in addition to V3 sequence, structural information.
 To our knowledge, the first of these methods has been that of Sander et al. CITATION, which was mainly based on geometric distances of amino-acid pairs within the structure of V3.
 Although our method, detailed in the following, relies on the same experimental structure by Huang et al. CITATION, it differs from that of Sander et al. in several respects, e.g. it deals with indels, and, perhaps most crucially, it uses as descriptors properties that directly determine interaction of V3 with the co-receptors.
 By the latter we consider a seemingly trivial but fundamental fact that so far has not been thoroughly exploited: although V3 is highly variable, all X4-tropic V3 loops share one property, namely, they preferentially have a physical binding interaction with CXCR4, while R5-tropic V3 loops preferably interacts with CCR5.
 The accuracy of the method makes it attractive as clinical tool for patient tailored decisions on treatment with entry inhibitors, and it suggests that co-receptor tropism can be explained almost exclusively based on V3.### abstract ###
	in this paper we derive the equations for loop corrected belief propagation on a continuous variable gaussian model
	using the exactness of the averages for belief propagation for gaussian models  a  different way of obtaining the covariances is found   based on belief propagation on cavity graphs
	we discuss the relation of this  loop correction algorithm to expectation propagation  algorithms for the case in which the model is no longer  gaussian  but slightly perturbed by nonlinear terms
	message passing techniques in graphical models allow for the computation of  approximate   marginal probabilities in a time interval scaling polynomially in the  model size
	their discovery has consequently revolutionized several  fields of applications in the past years  of which error correcting codes and vision are probably the most prominent examples
	in many cases  the corresponding graphs are loopy  implying either that the error resulting from the application of loopy belief propagation  bp  is negligible for the particular model  or it  can be tolerated for the particular purpose bp serves
	in other cases  more sophisticated refinements of bp are necessary  taking into account  part of  the loop errors
	finding the optimal treatment of these   loop errors    motivates an active field of research  in which  different solutions applying to different model classes are developed
	for models involving many short loops   like on regular lattices  cvm type approaches work well  CITATION   or tree ep approaches  CITATION
	the latter may also be  applied to correct for an incidental large loop
	unifying frameworks like the region graphs of  CITATION   lead to general strategies for selecting the basic clusters underlying such approaches for general model classes
	a recent analysis has shown that the local update equations of bp may be interpreted as the zero order term of an expansion in   cavity connected correlations  
	these quantities are parameterizations of the   cavity distributions     i e   the  distribution over neighbor variables of a central variable which has been removed from the graph
	the bethe approximation and bp are recovered when this  cavity distribution is assumed to factorize  whereas the first order correction to the local update equations is obtained when one takes into account the pair cumulants  CITATION
	estimation of these pair cumulants is possible with extra runs of bp  allowing for new polynomial time algorithms  reducing errors to order    when applying algorithms of which running time scales with an extra factor  of    CITATION
	although this scaling seems heavy  the large benefit of the approach is that it does not require selection of basic clusters or underlying  tree structures  since it takes into account the effect of all loops that contribute to nontrivial correlations in the cavity distribution at once
	the above   loop correction    strategy is applicable in the class of models where a perturbative expansion around the bethe approximation makes sense  i e   in models with large loops and relatively weak interactions
	the principal requirement  is that the magnitude of pair variable cumulants of cavity distributions is an order smaller than the  single variable cumulants  and third order cumulants are even smaller  etc
	however  heuristics based on the strategy allow for other good algorithms  performing well outside these parameter regimes  CITATION
	so far the approach has been developed for discrete variable models on a more abstract  CITATION  versus practical level  CITATION
	in this paper we apply the idea to graphical models for continuous variables
	we derive the loop corrected belief propagation equations  for simple tractable gaussian models   yielding a message passing scheme that  besides the correct average marginals  also yields the correct variances
	besides that we discuss some approaches potentially  applicable to cases in which extra function approximations are necessary   and the relation with expectation propagation
	a by product of our loop corrected belief propagation equations is an algorithm that calculates exact covariance matrices for gaussian models like the one discussed in  CITATION   but without explicitly using linear response
 Metabolic network reconstructions represent valuable scaffolds for -omics data integration and are used to computationally interrogate network properties.
 However, they do not explicitly account for the synthesis of macromolecules.
 Here, we present the first genome-scale, fine-grained reconstruction of Escherichia coli's transcriptional and translational machinery, which produces 423 functional gene products in a sequence-specific manner and accounts for all necessary chemical transformations.
 Legacy data from over 500 publications and three databases were reviewed, and many pathways were considered, including stable RNA maturation and modification, protein complex formation, and iron sulfur cluster biogenesis.
 This reconstruction represents the most comprehensive knowledge base for these important cellular functions in E. coli and is unique in its scope.
 Furthermore, it was converted into a mathematical model and used to: quantitatively integrate gene expression data as reaction constraints and compute functional network states, which were compared to reported experimental data.
 For example, the model predicted accurately the ribosome production, without any parameterization.
 Also, in silico rRNA operon deletion suggested that a high RNA polymerase density on the remaining rRNA operons is needed to reproduce the reported experimental ribosome numbers.
 Moreover, functional protein modules were determined, and many were found to contain gene products from multiple subsystems, highlighting the functional interaction of these proteins.
 This genome-scale reconstruction of E. coli's transcriptional and translational machinery presents a milestone in systems biology because it will enable quantitative integration of -omics datasets and thus the study of the mechanistic principles underlying the genotype phenotype relationship.
 High-throughput experimental technologies enable the production of heterogeneous data, such as expression profiles and proteomic data, for almost any organism of interest.
 A detailed mathematical representation of the in vivo cellular network is required to obtain a holistic understanding of cellular processes from these data sets and to quantitatively integrate them into a biological context.
 One such approach is the bottom-up network reconstruction, which builds manually networks in a brick-by-brick manner using genome annotation and component-specific information CITATION, CITATION.
 This reconstruction procedure is well established for metabolic reaction networks and has been applied to many organisms, including Human CITATION, Saccharomyces cerevisiae CITATION, CITATION, Leishmani major CITATION, Escherichia coli CITATION, Helicobacter pylori CITATION, Pseudomonas aeruginosa CITATION, and Pseudomonas putida CITATION, CITATION .
 These bottom-up metabolic networks differ from other network reconstructions as they are tailored to the genomic content of the target organism and built manually using biochemical, physiological, and other experimental information in addition to the genome annotation.
 Hence, these reconstructions can be thought of as biochemically, genetically, and genomically structured knowledge bases CITATION.
 The reconstruction and modeling procedure is a 4-step process: obtaining a draft reaction list based on genome annotation and biochemical databases, refinement of reaction list using experimental information, conversion of the reaction list into a computable format and application of systems boundaries to define condition-specific models, and the evaluation and validation of the model content using various mathematical methods.
 By iterating step 2 to 4, reconstructions that are self-consistent within their defined scope can be generated.
 Metabolic network reconstruction have demonstrated to be useful in at least 5 areas of applications CITATION : biological discovery CITATION, phenotypic behavior CITATION, bacterial evolution CITATION, network analysis CITATION, and metabolic engineering CITATION.
 This wide range of applications of the metabolic reconstructions is possible because they can be readily converted into predictive, condition-specific models.
 Unlike more traditional approaches to modeling metabolism, the constraint-based modeling approach requires few, if any, parameters CITATION, CITATION.
 The stoichiometric information encoded in the reconstruction can be represented mathematically as a stoichiometric matrix, S, where the rows correspond to the components and the columns correspond to the reactions .
 While the COBRA approach has been successfully applied to metabolic networks, the same principles and assumptions can be also employed to reconstruct and model other cellular functions, such as signaling CITATION CITATION, regulation CITATION, and protein synthesis CITATION.
 In this study, we extended and refined earlier work by Allen et al., which proposed a stoichiometric formalism to model protein synthesis and illustrated it on some E. coli genes and operons CITATION.
 We created a more detailed, gene-specific representation of the transcriptional and translational processes, which explicitly accounts for the sequence-specific synthesis of DNA, mRNA, and proteins.
 This reconstruction enables quantitative integration of high-throughput data such as gene expression, proteomic, and mRNA degradation data.
 Moreover, proteins are produced in high copy numbers in growing cells; thus, any quantitative mechanistic modeling and analysis of high-throughput data needs to account for the synthesis cost associated with these molecules.
 Numerous studies have been published that investigate protein synthesis using kinetic models CITATION CITATION.
 These models are generally tailored to the questions they address making it difficult to readily apply them for modified problems.
 Since stoichiometric relationships are a common requisite for any type of mechanistic modeling, organism-specific BiGG knowledge bases can be used as templates to derive problem-specific, mechanistic models.
 In fact, network stoichiometry is a dominant feature of kinetic models as well CITATION.
 Thus, network reconstruction serves as a platform for steady-state and kinetic modeling .
 In this study, we present a new generation of network reconstructions, which directly account for the synthesis of individual mRNA and proteins.
 We named the mathematical representation of this reconstruction the Expression matrix, or E-matrix, since it encodes the expression of mRNA and proteins.
 All network reactions were formulated to account for gene-specific and E. coli-specific details, such as nucleotide composition, operon association, and sigma factor usage.
 Furthermore, we used information from three databases and more than 500 scientific publications to formulate mechanistically detailed and accurate reactions.
 This reconstruction is the first comprehensive database detailing the available information for these cellular functions and can thus be deemed a knowledge base.
 After conversion of the E-matrix reconstruction into condition-specific models corresponding to different doubling times, we were able to accurately predict the ribosome production reported in literature, without any parameterization.
 Furthermore, we show that the E-matrix can be used to study the effect of rRNA operon deletion.
 Our results predict that a high density of RNA polymerases is required on the remaining rRNA operons, to achieve the reported ribosome numbers.
 Finally, we show that proteins used in the E-matrix could be grouped into functional modules which lead to a more simplified view of the network.
 A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model.
 The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids.
 Our previous studies have dealt with 7510 proteins of no more than 150 amino acids.
 The proteins are ranked according to the strength of the resistance.
 Most of the predicted top-strength proteins have not yet been studied experimentally.
 Architectures and folds which are likely to yield large forces are identified.
 New types of potent force clamps are discovered.
 They involve disulphide bridges and, in particular, cysteine slipknots.
 An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds.
 These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications.
 A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known.
 This class is characterized through molecular dynamics simulations.
 Atomic force microscopy, optical tweezers, and other tools of nanotechnology have enabled induction and monitoring of large conformational changes in biomolecules.
 Such studies are performed to assess structure of the biomolecules, their elastic properties, and ability to act as nanomachines in a cell.
 Stretching studies of proteins CITATION are of a particular current interest and they have been performed for under a hundred of systems.
 Interpretation of some of these experiments has been helped by all-atom simulations, such as reported in refs. CITATION, CITATION.
 They are limited by of order 100 ns time scales and thus require using unrealistically large constant pulling speeds.
 However, they often elucidate the nature of the force clamp the region responsible for the largest force of resistance to pulling, FORMULA.
 All of the experimental and all-atom simulational studies address merely a tiny fraction of proteins that are stored in the Protein Data Bank CITATION.
 Thus it appears worthwhile to consider a large set of proteins and determine their FORMULA within an approximate model that allows for fast and yet reasonably accurate calculations.
 Structure-based models of proteins, as pioneered by Go and his collaborators CITATION and used in several implementations CITATION CITATION, seem to be suited to this task especially well since they are defined in terms of the native structures away from which stretching is imposed.
 There are many ways, all phenomenological, to construct a structure-based model of a protein.
 504 of possible variants are enumerated and 62 are studied in details in ref. CITATION.
 The variants differ by the choice of effective potentials, nature of the local backbone stiffness, energy-related parameters, and of the coarse-grained degrees of freedom.
 The most crucial choice relates to making a decision about which interactions between amino acids count as native contacts.
 Comparing FORMULA to the corresponding experimental values in 36 available cases selects several optimal models CITATION.
 Among them, there is one which is very simple and which describes a protein in terms of its FORMULA atoms, as labeled by the sequential index FORMULA.
 This model is denoted by FORMULA which stands for, respectively, the Lennard-Jones native contact potentials, local backbone stiffness represented by harmonic terms that favor the native values of local chiralities, the contact map in which there are no FORMULA contacts, and the amplitude of the Lennard-Jones potential, FORMULA, is uniform.
 The contact map is determined by assigning the van der Waals spheres to the heavy atoms and by checking whether spheres belonging to different amino acids overlap in the native state CITATION, CITATION.
 If they do, a contact is declared as native.
 Non-native contacts are considered repulsive.
 Application of this criterion frequently selects the FORMULA contacts as native.
 If the contact map includes these contacts the resulting model will be denoted here as FORMULA.
 On average, it performs worse than FORMULA because the FORMULA contacts usually correspond to the weak van der Waals couplings as can be demonstrated in a sample of proteins by using a software CITATION which analyses atomic configurations from the chemical perspective on molecular bonds.
 Thus the FORMULA couplings should better be removed from the contact map .
 The survey to determine FORMULA in 7510 model proteins with the number of amino acids, FORMULA, not exceeding 150 and 239 longer proteins has been accomplished twice.
 First within the FORMULA model CITATION and soon afterwords within the FORMULA model CITATION.
 The first survey also comes with many details of the methodology whereas the second just presents the outcomes.
 The two surveys are compared in more details in refs. CITATION, CITATION.
 The results differ, particularly when it comes to ranking of the proteins according to the value of FORMULA, but they mutually provide the error bars on the findings.
 They both agree, however, on predicting that there are many proteins whose strength should be considerably larger than the frequently studied benchmark the sarcomere protein titin.
 Near the top of the list, there is the scaffoldin protein c7A which has been recently measured to have FORMULA of about 480 pN CITATION.
 Other findings include establishing correlations with the CATH hierarchical classification scheme CITATION, CITATION, such as that there are no strong FORMULA proteins, and identification of several types of the force clamps.
 The large forces most commonly originate in parallel FORMULA that are sheared CITATION.
 However, there are also clamps with antiparallel FORMULA, unstructured strands, and other kinds.
 The two surveys have been based on the structure download made on July 26, 2005 when the PDB comprised 29 385 entries.
 Many of them correspond to nucleic acids, complexes with nucleic acids and with other proteins, carbohydrates, or come with incomplete files and hence the much smaller number of proteins that could be used in the molecular dynamics studies.
 Here, we present results of still another survey which is based on a download of December 18, 2008 which contains 54 807 structure files and leads to 17 134 acceptable structures with FORMULA not exceeding 250.
 These structures are then analyzed through simulations based on the FORMULA model.
 The numerical code has been improved to allow for acceleration of calculations by a factor of 2.
 The 190 structures with the top values of FORMULA in units of FORMULA are shown in Table 1 and Table S1 of the SI, together with the values of titin and ubiquitin to provide a scale.
 As argued in the Materials and Methods section section, the unit of force, FORMULA, is now estimated to be of order 110 pN.
 All of the corresponding proteins are predicted to be much stronger than titin and none but two of them have been studied experimentally yet.
 In addition to the types of force clamps identified before, we have discovered two new mechanisms of sturdiness.
 One of them involves a cysteine slipknot and is found to be operational in all of the 13 top strength proteins.
 In this motif, a slip-loop is pulled out of a cysteine knot-loop.
 Another involves dragging of a single fragment of the main chain across a cysteine knot-loop.
 The two mechanisms are similar in spirit since both involve dragging of the backbone.
 However, in the CSK case, two fragments of the backbone are participating.
 We make a more systematic identification of the CATH-classified architectures that are linked to mechanical strength and then analyze correlations of the data to the SCOP-based grouping CITATION CITATION.
 The previous surveys did not relate to the SCOP scheme.
 We identify the CATH-based architectures and SCOP-based folds that are associated with the occurrence of a strong resistance to pulling.
 A general observation, however, is that each such group of structures may also include examples of proteins that unravel easily.
 The dynamics of a protein are very sensitive to mechanical details that are largely captured by the contact map and not just by the appearance of a structure.
 On the other hand, if one were to look for mechanically strong proteins then the architectures and folds identified by us should provide a good starting point.
 We also study the dependence of FORMULA on the pulling velocity and characterize the dependence on FORMULA through distributions of the forces.
 The current third survey has been performed within the same FORMULA model as the second survey CITATION.
 However, we reuse and extend it here because the editors of Biophysical Journal retracted the second survey CITATION.
 All of the values of FORMULA are deposited at the website LINK and can by accessed by through the PDB structure code.
 The influence of lipid molecules on the aggregation of a highly amyloidogenic segment of human islet amyloid polypeptide, hIAPP20 29, and the corresponding sequence from rat has been studied by all-atom replica exchange molecular dynamics simulations with explicit solvent model.
 hIAPP20 29 fragments aggregate into partially ordered -sheet oligomers and then undergo large conformational reorganization and convert into parallel/antiparallel -sheet oligomers in mixed in-register and out-of-register patterns.
 The hydrophobic interaction between lipid tails and residues at positions 23 25 is found to stabilize the ordered -sheet structure, indicating a catalysis role of lipid molecules in hIAPP20 29 self-assembly.
 The rat IAPP variants with three proline residues maintain unstructured micelle-like oligomers, which is consistent with non-amyloidogenic behavior observed in experimental studies.
 Our study provides the atomic resolution descriptions of the catalytic function of lipid molecules on the aggregation of IAPP peptides.
 A range of human diseases including Alzheimer's disease, Parkinson's disease, the spongiform encephalopathy and type 2 diabetes mellitus is associated with amyloid deposits of normally soluble proteins or peptides CITATION CITATION.
 In T2DM, the main protein component of fibrillar protein deposits in the pancreatic islets of langerhans has been identified as a 37-residue hormone referred to as islet amyloid polypeptide or amylin CITATION, which is synthesized in -cells of the pancreas and cosecreted with insulin CITATION, CITATION.
 There are convincing evidences that the toxicity of amyloid related diseases may be caused by the soluble intermediate oligomers instead of mature fibrils CITATION CITATION, and the interaction between lipid bilayer and these soluble oligomer CITATION CITATION.
 For example, channel-like annular structures of oligomers of several amyloidogenic peptides have been observed on the lipid membrane CITATION, CITATION, and have been studied by molecular dynamics simulations as well CITATION, CITATION.
 Moreover, up to 10 percent components in amyloid deposits from patient tissues were lipid molecules, indicating that the lipids can be uptaken from membranes and then wrapped into fibrillar amyloid CITATION CITATION.
 Most studies so far treated the lipid bilayer as a template to exert its influences on the conformation and aggregation properties of peptides CITATION CITATION.
 There is, however, missing information about how individual lipid molecule involving in the peptide aggregation process.
 It will then be beneficial to understand the molecular details of how single lipid molecule influences the assembly process of amyloidogenic peptides which is the main focus of the current study.
 Besides the external factors, such as lipid bilayer, pH value, the sequences of peptide themselves have great effects on the aggregation behaviors.
 Several other species such as non-human primates CITATION, cats CITATION, raccoons CITATION, and rodent species can produce IAPP, but the primary sequence of IAPP varies slightly among species.
 Importantly, IAPP from rodent species, such as rat/mouse IAPP lose capacities of aggregating into amyloid fibrils CITATION, but transgenic mouse models that express human IAPP develop islet deposits CITATION.
 The rIAPP differs from hIAPP in six amino acids and five of them are clustered in a short decapeptide, which is considered to be strongly amyloidogenic and forms similar unbranched fibrils itself to the full-length hIAPP CITATION, CITATION.
 The three proline substitutions in rIAPP20 29 are believed to be highly responsible for the lacking of the amyloidogenic property of the segment or full-length peptide CITATION.
 Although rIAPP has been intensively applied in experimental research acting as a potential peptide inhibitor for peptide aggregation CITATION, CITATION, the molecular mechanism of its resistance to amyloid is still not crystal clear.
 Here, the aggregation of rIAPP20 29 segments is subjected to the same simulation condition as hIAPP20 29 to explore the non-amyloidogenic properties of the peptide and meanwhile to evaluate the simulation results as a negative control.
 Due to the metastable and short-lived nature of soluble pre-fibril oligomers at the early steps of fibril formation, experimental data are usually difficult to obtain CITATION, CITATION.
 Thus, the computational approaches have been employed to complement experimental investigations to gain the insight into the aggregation mechanisms CITATION CITATION.
 Considering multiple copies of peptides needed due to the self-assembly nature of amyloid formation, various simplified representations of molecular systems using implicit solvent models were preferred rather than all-atom models.
 Santini et al. performed ART-OPEP simulations on trimer of A 16 22 by treating side chains as a bead and solvent implicitly CITATION.
 A novel mechanism for single -strand to surmount unnatural registry without dissociation, referred to as reptation was proposed before experimental characterization CITATION.
 Cheon et al. used ProFASi package to reduce the bonded potential energy to include torsional angles only and treated hydrogen bonds explicitly CITATION.
 They were able to carry out two series of 100 Monte Carlo simulations on 20 copies of two fragments A 16 22 and A 25 35.
 They observed early-stage events and obtained an atomic-detailed description of nucleated conformational conversion CITATION model for amyloid aggregation.
 In these studies, simulations were usually started with randomly oriented, extended or random-coiled peptides which underwent ab initio folding to form -sheet oligomers.
 Albeit simplified models allow studying large-scale systems CITATION or observing more events in limited simulation time CITATION, all-atom explicit solvent models can reproduce amyloid aggregation in aqueous environment more accurately and supply more information on sidechain contacts CITATION.
 Nguyen et al. prolonged a series of conventional MD simulations to 300 ns on A 16 22 of 3 6 oligomer size with explicit solvent CITATION.
 The extensive simulations were able to probe the interpeptide sidechain contacts and large conformational fluctuations upon monomer addition to preformed -sheet oligomers in a dock-lock mechanism.
 In our studies, an enhanced-sampling method, replica exchange molecular dynamics CITATION was implemented CITATION, and all water and peptide atoms are treated explicitly by applying OPLS-AA force field CITATION.
 The four copies of amyloidogenic segment hIAPP20 29 and an extra dioleoylphosphatidylcholine lipid molecule were initially set in extended conformation and dispersed in simulation boxes.
 The formation of -sheet containing tetramers, was observed within 100 ns ab initio REMD folding simulations.
 The acquirement of abundant intermediate states suggested two possible -sheet transition pathways.
 Simulation of four hIAPP peptides without lipid molecule was also performed.
 Nonamyloidogenic rat IAPP segments were studied as a negative control with the aim of understanding the inhibitory effect of three proline substitutions.### abstract ###
	when judging their likelihood of success in competitive tasks, people tend to be overoptimistic for easy tasks and overpessimistic for hard tasks the shared circumstance effect; sce
	previous research has shown that feedback and experience from repeated-play competitions has a limited impact on sces
	however, in this paper, we suggest that competitive situations, in which the shared difficulty or easiness of the task is more transparent, will be more amenable to debiasing via repeated play
	pairs of participants competed in, made predictions about, and received feedback on, multiple rounds of a throwing task involving both easy- and hard-to-aim objects
	participants initially showed robust sces, but they also showed a significant reduction in bias after only one round of feedback
	these and other results support a more positive view than suggested from past research on the potential for sces to be debiased through outcome feedback
	competition abounds in everyday life, where we contend with others for top grades, jobs, trophies, and mates
	when resources are at a premium, it is optimal to enter into competitive environments in which we are certain to fare well and to avoid those in which we are doomed to fail
	however, when people evaluate their likelihood of success in competitions, they are subject to a robust bias: a competitor should consider the strengths and weaknesses of the self and the other competitors  CITATION , but people often give too much weight to evidence related to their own strengths and weaknesses and too little weight to such evidence about the competitor  CITATION
	this egocentrism results in overoptimism when the circumstances of the competition are favorable, such as when competitors in a trivia game learn that the questions will be from an easy category-even though they'll be easy for everyone  CITATION
	egocentrism also results in overpessimism when the circumstances are unfavorable e g , a difficult trivia category
	this phenomenon of being more optimistic when shared competitive circumstances are favorable than when they are not has been dubbed the shared-circumstance effect  CITATION , and it has been replicated across a variety of settings e g , general knowledge tasks, card games, athletic competitions
	in most previous studies on the sce, participants were presented with novel, non-repeated competitive situations
	these situations did not allow people to learn from past experiences or from feedback within the immediate competitive context
	however, in everyday contexts there are often opportunities to learn how a shared circumstance tends to affect the self, others, and outcomes
	for example, when a tennis tournament is played during a string of windy days, players can have several opportunities to observe how the weather affects themselves and their competitors
	to examine whether egocentrism and sces persist in repeated-play contexts, rose and windschitl  CITATION  had pairs of participants compete against each other in multiple rounds of a trivia contest that involved easy and hard categories
	in each round, participants estimated their likelihood of beating their competitor, answered trivia questions, and received feedback about who won
	in initial rounds, there were robust sces; participants were much more optimistic about winning easy categories than hard ones
	if they encountered the same hard and easy categories across rounds, the participants learned from feedback
	that is, the sce shrank-but slowly-across six rounds with the same categories
	the sce was never eliminated, even after six rounds
	also, for a seventh round, participants were told there would be new categories
	the sce for that round dramatically and fully rebounded; it was every bit as large as observed for round 1
	these results provide a bleak view of how well people can learn from feedback and avoid sces
	moreover, results from a study by moore and cain  CITATION , which also used repeated plays with feedback, suggest an even bleaker view
	those researchers also used easy and difficult quizzes as shared-circumstance manipulations, but found virtually no reduction in sces after numerous rounds with feedback
	CITATION are people's abilities to learn to avoid sces-based on feedback-really as bleak as this prior research might suggest
	we argue that some shared circumstances are more transparently shared than others, and this may affect how readily people learn to avoid the bias that creates sces-and how easily they can transfer this learning to a slightly new set of shared circumstances
	by transparently shared, we are referring to how obvious it is that a circumstance that is helpful or hindering to the self will affect others in largely the same way
	in the present study, we examined the influence of repeated feedback on sces
	however, unlike past research, we used a competition in which the difficulty of the shared circumstance relative to competitions, for example, involving easy and hard trivia categories is more transparently shared
	in a multi-round paradigm, participants competed in object-tossing competitions
	in each round, two competitors each had 8 throws per object-attempting to land the object inside a target area
	there was always one easy-to-aim object e g , a beanbag and one hard-to-aim object e g , a paper plate, which constituted our shared-circumstance manipulation
	full feedback was given during and after each round, and predictions were solicited before each round and also before a final round with novel objects
	we suspected that a tossing competition, rather than a trivia competition, would produce less bleak results about the debiasing of sces through repeated play
	in the case of trivia, watching one's competitor fail to answer trivia questions doesn't give any insight about why the category is difficult for that person
	also, knowing that one's competitor struggled on a difficult category does not provide obvious information about why he or she might struggle on another difficult category
	however, in the case of throwing, watching a competitor fail when tossing an object probably illuminates the relevance of specific shared, situational circumstances i e , the properties of the specific objects as well as a more general awareness of the relevance that object properties have on throwing success-for anyone
	for example, seeing a paper plate fly unpredictably will likely give an observer a clear impression that the object will fly unpredictably regardless of who is throwing it
	for participants, this enhances the appreciation that one's struggles are not primarily due to personal characteristics but to properties of the tossed objects; this would then be useful in mitigating egocentrism and sces, even when new objects are introduced
	consequently, we expected that, even though participants might reveal a robust sce at round 1 prior to any feedback or observations regarding their competitor, they would show a pronounced learning effect after the feedback and observations of round 1
	that is, they would show significantly reduced sces starting immediately after round 1
	we also expected that this learning would be generally transferable### abstract ###
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its perceived disutility relative to not testing is greater than the utility associated with prevention of possible disease
--the prospect theory editing operation, by which a decision maker's reference point is determined, can have important effects on the disutility of the test
--on the basis of the prospect theory value function, this paper develops two approaches to reducing disutility by directing the decision maker's attention to either actual past or expected future losses that result in shifted reference points
--after providing a graphical description of the approaches and a mathematical proof of the direction of their effect on judgment, we briefly illustrate the potential value of these approaches with examples from qualitative research on prostate cancer treatment decisions
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its disutility relative to not testing is greater than the utility associated with prevention of possible disease
--for example, people may feel that the anticipated disutility of a colonoscopy for colorectal cancer screening is great enough relative to the expected utilty of prevention of possible colorectal cancer to dissuade them from seeking colonoscopy
--the prospect theory editing operation  CITATION , by which a decision maker's reference point is determined, can have important impacts on the perceived disutility of the test
--the work of rothman, salovey, and colleagues on message framing has tested prospect theory predictions of how the description of test outcomes as gains or losses as well as the conceptualization of the purpose of the test as preventative vs diagnostic and the consequent perception of whether the test is "safe" or "risky" can affect test rates  CITATION
--specifically, message framing theories predict that when a procedure is perceived as risky e g , cancer screening tests may cause a patient to find out that they have cancer, loss-framed messages will promote testing more strongly than gain-framed messages, because people favor risky prospects over sure prospects in the domain of losses
--on the other hand, when a procedure is perceived as safe e g , sunscreen prevents sunburn and skin cancer, gain-framed messages are predicted to be more effective because people prefer sure prospects to risky prospects in the domain of gains
--several public health intervention studies have examined message framing and generally found evidence favoring the predictions  CITATION
--on the basis of the prospect theory value function, this paper develops two approaches to reducing perceived disutility by directing the decision maker's attention to either actual past or expected future losses that can serve as reference points and are not consequences of the test itself
--these approaches thus differ from message framing, which focuses on how the test outcomes are described and manipulates gain and loss framing
--we instead derive the potential impact of directly refocusing the decision maker's reference point
 Although the Internet AS-level topology has been extensively studied over the past few years, little is known about the details of the AS taxonomy
 An AS "node" can represent a wide variety of organizations, e g , large ISP, or small private business, university, with vastly different network characteristics, external connectivity patterns, network growth tendencies, and other properties that we can hardly neglect while working on veracious Internet representations in simulation environments
 In this paper, we introduce a radically new approach based on machine learning techniques to map all the ASes in the Internet into a natural AS taxonomy
 We successfully classify ~95.3\% of ASes with expected accuracy of ~78.1\%
 We release to the community the AS-level topology dataset augmented with: 1) the AS taxonomy information and 2) the set of AS attributes we used to classify ASes
 We believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the Internet
 The rapid expansion of the Internet in the last two decades has produced a large-scale system of thousands of diverse, independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments
 From 1997 to 2005 the number of globally routable AS identifiers has increased from less than 2,000 to more than 20,000, exerting significant pressure on interdomain routing as well as other functional and structural parts of the Internet
 This impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the Internet infrastructure
 In particular, the AS-level topology is an intermix of networks owned and operated by many different organizations, e g , backbone providers, regional providers, access providers, universities and private companies
 Statistical information that faithfully characterizes different AS types is on the critical path toward understanding the structure of the Internet, as well as for modeling its topology and growth
 In topology modeling, knowledge of AS types is mandatory for augmenting synthetically constructed or measured AS topologies with realistic intra-AS and inter-AS router-level topologies
 For example, we expect the network of a dual-homed university to be drastically different from that of a dual-homed small company
 The university will likely contain dozens of internal routers, thousands of hosts, and many other network elements (switches, servers, firewalls)
 On the other hand, the small company will most probably have a single router and a simple network topology
 Since there is such a diversity among different network types, we cannot accurately augment the AS-level topology with appropriate router-level topologies if we cannot characterize the composing ASes
 Moreover, annotating the ASes in the AS topology with their types is a prerequisite for modeling the evolution of the Internet, since different types of ASes exhibit different growth patterns
 For example, Internet Service Providers (ISP) grow by attracting new customers and by engaging in business agreements with other ISPs
 On the other hand, small companies that connect to the Internet through one or few ISPs do not grow significantly over time
 Thus, categorizing different types of ASes in the Internet is necessary to identify network evolution patterns and develop accurate evolution models
 An AS taxonomy is also necessary for mapping IP addresses to different types of users
 For example, in traffic analysis studies its often required to distinguish between packets that come from home and business users
 Given an AS taxonomy, its possible to realize this goal by checking the type of AS that originates the prefix in which an IP address lies
 In this work, we introduce a radically new approach based on machine learning to construct a representative AS taxonomy
 We develop an algorithm to classify ASes based on empirically observed differences between AS characteristics
 We use a large set of data from the Internet Routing Registries~(IRR)~ CITATION  and from RouteViews~ CITATION  to identify intrinsic differences between ASes of different types
 Then, we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ASes into six representative classes that reflect ASes with different network properties and infrastructures
 We derive macroscopic statistics on the different types of ASes in the Internet and validate our results using a sample of~1200 manually identified AS types
 Our validation demonstrates that our classification algorithm achieves high accuracy:~78 1\% of the examined classifications were correct
 Finally, we make our results and our classifier publicly available to promote further research and understanding of the Internet's structure and evolution
 In Section~ we start with a brief discussion of related work
 Section~ describes the data we used, and in Section~ we specify the set of AS classes we use in our experiments
 Section~ introduces our classification approach and results
 We validate them in Section~ and conclude in Section~
	Numerous psychophysical experiments found that humans preferably rely on a narrow band of spatial frequencies for recognition of face identity.
	A recently conducted theoretical study by the author suggests that this frequency preference reflects an adaptation of the brain's face processing machinery to this specific stimulus class.
	The purpose of the present study is to examine this property in greater detail and to specifically elucidate the implication of internal face features.
	To this end, I parameterized Gabor filters to match the spatial receptive field of contrast sensitive neurons in the primary visual cortex.
	Filter responses to a large number of face images were computed, aligned for internal face features, and response-equalized.
	The results demonstrate that the frequency preference is caused by internal face features.
	Thus, the psychophysically observed human frequency bias for face processing seems to be specifically caused by the intrinsic spatial frequency content of internal face features.
	In the brain, the structure of neuronal circuits for processing sensory information matches the statistical properties of the sensory signals CITATION.
	Taking advantage of these statistical regularities contributes to an optimal encoding of sensory signals in neuronal responses, in the sense that the code conveys the highest information with respect to specific constraints CITATION CITATION.
	Among the various constraints which were formulated we find, for example, keeping metabolic energy consumption as low as possible CITATION CITATION, or keeping total wiring length between processing units at a minimum CITATION, or maximizing the suppression of spatio-temporal redundancy in the input signal CITATION, CITATION CITATION .
	As for visual stimuli, natural images reveal a conspicuous statistical regularity that comes as an approximately linear decrease of their amplitude spectra as a function of spatial frequency CITATION CITATION.
	This means that pairs of luminance values are strongly correlated CITATION, and this property could be exploited for gain controlling of visual neurons.
	Then, visual neurons would have equal sensitivities or response amplitudes independent of their spatial frequency preference CITATION.
	According to this response equalization hypothesis, gain should thus be incremented with increasing spatial frequency, such that the distribution of response amplitudes of frequency-tuned neurons to a typical natural image is flat.
	An argument in favor of employing response equalization is that it would lead to an improvement of information transmission from one neuronal stage to another, because the output of one stage would match the limited dynamic range of a second one CITATION .
	The present article builds upon previously reported results for whitened amplitude spectra of face images CITATION : the whitened spectra reveal a spatial frequency maximum at 10 15 cycles per face, but only if external face features are suppressed.
	The predicted frequency maximum nevertheless agrees well with numerous psychophysical experiments, which found that face identity is preferably processed in a narrow band of spatial frequencies from 8 to 16 cycles per face CITATION CITATION .
	Despite of it all, the results presented in CITATION indicate that the maxima in the amplitude spectra are caused by the compound effect of horizontally oriented internal face features.
	Quantitatively, the maxima thus occur in units of cycles per face height, whereas most psychophysical studies instead measure their results in terms of cycles per face width.
	Furthermore, although a clear enhancement of horizontal amplitudes could be observed in the spectra, horizontal amplitudes showed a somewhat noisy dependence on spatial frequency.
	Both effects are a consequence of that face features were not considered individually, what causes a mixing of the spatial frequency content of individual face features in the spectra.
	The mixing leads to averaging-out effects such that any possible enhancement of spectral amplitudes at other than the horizontal orientation goes unnoticed, but also may cause interference effects which lead to the mentioned noisy dependence of amplitudes on spatial frequency.
	The present study addresses the two issues by means of an extensive analysis of face images by means of Gabor filters.
	The filters were thereby parameterized to match the spatial receptive field of band-limited, oriented and contrast sensitive neurons in the primary visual cortex CITATION CITATION.
	Great care has been taken to guarantee the correct alignment of filter responses with respect to the position of internal face features prior to their averaging.
	Doing so permits to precisely elucidate how the frequency dependence of Gabor responses is related to each of the four internal face features.
	The resulting graphs of whitened Gabor amplitudes versus spatial frequency are smooth and reveal distinct maxima at nearly all orientations.
	The most stable maxima, however, are observed at horizontal feature orientations in the first place, but also at vertical orientations.
	This observation holds true for all of the internal face features.
	The present study therefore shows how the individual internal face features contribute to the psychophysically observed frequency preference, and proposes concrete mechanisms of how higher amplitudes of whitened cell responses at an early level could possibly lead to the psychophysically measured effects.
 The firing rate of single neurons in the mammalian hippocampus has been demonstrated to encode for a range of spatial and non-spatial stimuli.
 It has also been demonstrated that phase of firing, with respect to the theta oscillation that dominates the hippocampal EEG during stereotype learning behaviour, correlates with an animal's spatial location.
 These findings have led to the hypothesis that the hippocampus operates using a dual coding system.
 To investigate the phenomenon of dual coding in the hippocampus, we examine a spiking recurrent network model with theta coded neural dynamics and an STDP rule that mediates rate-coded Hebbian learning when pre- and post-synaptic firing is stochastic.
 We demonstrate that this plasticity rule can generate both symmetric and asymmetric connections between neurons that fire at concurrent or successive theta phase, respectively, and subsequently produce both pattern completion and sequence prediction from partial cues.
 This unifies previously disparate auto- and hetero-associative network models of hippocampal function and provides them with a firmer basis in modern neurobiology.
 Furthermore, the encoding and reactivation of activity in mutually exciting Hebbian cell assemblies demonstrated here is believed to represent a fundamental mechanism of cognitive processing in the brain.
 The hippocampus and surrounding medial temporal lobe are implicated in declarative memory function in humans and other mammals CITATION.
 Electrophysiology studies in a range of species have demonstrated that the activity of single pyramidal cells within this region can encode for the presence of both spatial and non-spatial stimuli CITATION.
 The majority of empirical investigation has focussed on place cells neurons whose firing rate is directly correlated with an animal's spatial location within the corresponding place field CITATION.
 Subsequent research has identified similar single cell responses to a variety of non-spatial cues including odour CITATION, complex visual images CITATION, CITATION, CITATION, running speed CITATION and the concept of a bed or nest CITATION.
 It has also been demonstrated that the exact timing of place cell discharge, relative to the theta oscillation which dominates the hippocampal EEG during learning, correlates with distance travelled through a place field CITATION, CITATION, CITATION CITATION.
 This phase precession mechanism creates a compressed theta coded firing pattern in place cells which corresponds to the sequence of place fields being traversed CITATION.
 These findings have led to the hypothesis that the hippocampus operates using a dual rate and temporal coding system CITATION, CITATION.
 Here we present a spiking neural network model which utilises a dual coding system in order to encode and recall both symmetric and asymmetric connections between neurons that exhibit repeated synchronous and asynchronous firing patterns respectively.
 The postulated mnemonic function of the hippocampus has been extensively modelled using recurrent neural networks, and this approach is supported by empirical data CITATION CITATION.
 The biological correlate of these models is widely believed to be the CA3 region, which exhibits dense recurrent connectivity and wherein synaptic plasticity can be easily and reliably induced.
 Pharmacological and genetic knockout studies have demonstrated that NMDAr-dependent synaptic plasticity in CA3 is critical for the rapid encoding of novel information, and synaptic output from CA3 critical for its retrieval CITATION, CITATION.
 Recurrent neural network models of hippocampal mnemonic function have generally utilised rate-coded Hebbian learning rules to generate reciprocal associations between neurons with concurrently elevated firing rates CITATION, CITATION.
 Hypothetically, this corresponds to the presence of either multiple stimuli or multiple overlapping place fields encountered at a single location CITATION CITATION.
 The hippocampus is also implicated in sequence learning, and temporally asymmetric plasticity rules have subsequently been employed in recurrent network models to generate hetero-associative connections between neurons that fire with repeated temporal correlation CITATION CITATION.
 Hypothetically, this corresponds to a sequence of place fields being traversed or stimuli being encountered on a behavioural timescale CITATION.
 Importantly, previous computational models of hetero-associative learning have typically encoded each successive stage of a learned sequence with the activity of a single neuron, while empirical studies estimate that place fields are typically encoded by an ensemble of several hundred place cells CITATION, CITATION CITATION.
 No computational model has thus far integrated auto- and hetero- associative learning in order to simultaneously generate both bi-directional and asymmetric connections between neurons that are active at the same and successive theta phases respectively using a single temporally asymmetric synaptic plasticity rule.
 Empirical data indicates that changes in the strength of synapses within the hippocampus can depend upon temporal correlations in pre- and post- synaptic firing according to a spike-timing dependent plasticity rule CITATION CITATION.
 It is not yet clear if rate-coded auto-associative network models of hippocampal mnemonic function are compatible with STDP or theta coded neural dynamics.
        Here, we examine the synaptic dynamics generated by several different STDP rules in a spiking recurrent neural network model of CA3 during the encoding of temporal, rate and dual coded activity patterns created by a phenomenological model of phase precession.
 We demonstrate that under certain conditions - the STDP rule can generate both bi-directional connections between neurons which burst at concurrent theta phase and asymmetric connections between neurons which fire at consecutive theta phase.
 Subsequent superthreshold stimulation of a small number of simulated neurons generates putative recall activity, driven by recurrent excitation, that corresponds to pattern completion and/or sequence prediction in auto- and/or hetero- associative connections respectively.
 Interestingly, these neural dynamics are reminiscent of sharp wave ripple activity observed in vivo CITATION CITATION.
 These findings demonstrate that STDP and theta coded neural dynamics are compatible with rate-coded auto-associative network models of hippocampal function.
 Furthermore, the encoding and reactivation of dual coded Hebbian phase sequences of activity in mutually exciting neuronal ensembles demonstrated here has been proposed as a general neural coding mechanism for cognitive processing CITATION, CITATION CITATION .
	The firing rate of single neurons in the mammalian hippocampus has been demonstrated to encode for a range of spatial and non-spatial stimuli.
	It has also been demonstrated that phase of firing, with respect to the theta oscillation that dominates the hippocampal EEG during stereotype learning behaviour, correlates with an animal's spatial location.
	These findings have led to the hypothesis that the hippocampus operates using a dual coding system.
	To investigate the phenomenon of dual coding in the hippocampus, we examine a spiking recurrent network model with theta coded neural dynamics and an STDP rule that mediates rate-coded Hebbian learning when pre- and post-synaptic firing is stochastic.
	We demonstrate that this plasticity rule can generate both symmetric and asymmetric connections between neurons that fire at concurrent or successive theta phase, respectively, and subsequently produce both pattern completion and sequence prediction from partial cues.
	This unifies previously disparate auto- and hetero-associative network models of hippocampal function and provides them with a firmer basis in modern neurobiology.
	Furthermore, the encoding and reactivation of activity in mutually exciting Hebbian cell assemblies demonstrated here is believed to represent a fundamental mechanism of cognitive processing in the brain.
	The hippocampus and surrounding medial temporal lobe are implicated in declarative memory function in humans and other mammals CITATION.
	Electrophysiology studies in a range of species have demonstrated that the activity of single pyramidal cells within this region can encode for the presence of both spatial and non-spatial stimuli CITATION.
	The majority of empirical investigation has focussed on place cells neurons whose firing rate is directly correlated with an animal's spatial location within the corresponding place field CITATION.
	Subsequent research has identified similar single cell responses to a variety of non-spatial cues including odour CITATION, complex visual images CITATION, CITATION, CITATION, running speed CITATION and the concept of a bed or nest CITATION.
	It has also been demonstrated that the exact timing of place cell discharge, relative to the theta oscillation which dominates the hippocampal EEG during learning, correlates with distance travelled through a place field CITATION, CITATION, CITATION CITATION.
	This phase precession mechanism creates a compressed theta coded firing pattern in place cells which corresponds to the sequence of place fields being traversed CITATION.
	These findings have led to the hypothesis that the hippocampus operates using a dual rate and temporal coding system CITATION, CITATION.
	Here we present a spiking neural network model which utilises a dual coding system in order to encode and recall both symmetric and asymmetric connections between neurons that exhibit repeated synchronous and asynchronous firing patterns respectively.
	The postulated mnemonic function of the hippocampus has been extensively modelled using recurrent neural networks, and this approach is supported by empirical data CITATION CITATION.
	The biological correlate of these models is widely believed to be the CA3 region, which exhibits dense recurrent connectivity and wherein synaptic plasticity can be easily and reliably induced.
	Pharmacological and genetic knockout studies have demonstrated that NMDAr-dependent synaptic plasticity in CA3 is critical for the rapid encoding of novel information, and synaptic output from CA3 critical for its retrieval CITATION, CITATION.
	Recurrent neural network models of hippocampal mnemonic function have generally utilised rate-coded Hebbian learning rules to generate reciprocal associations between neurons with concurrently elevated firing rates CITATION, CITATION.
	Hypothetically, this corresponds to the presence of either multiple stimuli or multiple overlapping place fields encountered at a single location CITATION CITATION.
	The hippocampus is also implicated in sequence learning, and temporally asymmetric plasticity rules have subsequently been employed in recurrent network models to generate hetero-associative connections between neurons that fire with repeated temporal correlation CITATION CITATION.
	Hypothetically, this corresponds to a sequence of place fields being traversed or stimuli being encountered on a behavioural timescale CITATION.
	Importantly, previous computational models of hetero-associative learning have typically encoded each successive stage of a learned sequence with the activity of a single neuron, while empirical studies estimate that place fields are typically encoded by an ensemble of several hundred place cells CITATION, CITATION CITATION.
	No computational model has thus far integrated auto- and hetero- associative learning in order to simultaneously generate both bi-directional and asymmetric connections between neurons that are active at the same and successive theta phases respectively using a single temporally asymmetric synaptic plasticity rule.
	Empirical data indicates that changes in the strength of synapses within the hippocampus can depend upon temporal correlations in pre- and post- synaptic firing according to a spike-timing dependent plasticity rule CITATION CITATION.
	It is not yet clear if rate-coded auto-associative network models of hippocampal mnemonic function are compatible with STDP or theta coded neural dynamics.
	Here, we examine the synaptic dynamics generated by several different STDP rules in a spiking recurrent neural network model of CA3 during the encoding of temporal, rate and dual coded activity patterns created by a phenomenological model of phase precession.
	We demonstrate that under certain conditions - the STDP rule can generate both bi-directional connections between neurons which burst at concurrent theta phase and asymmetric connections between neurons which fire at consecutive theta phase.
	Subsequent superthreshold stimulation of a small number of simulated neurons generates putative recall activity, driven by recurrent excitation, that corresponds to pattern completion and/or sequence prediction in auto- and/or hetero- associative connections respectively.
	Interestingly, these neural dynamics are reminiscent of sharp wave ripple activity observed in vivo CITATION CITATION.
	These findings demonstrate that STDP and theta coded neural dynamics are compatible with rate-coded auto-associative network models of hippocampal function.
	Furthermore, the encoding and reactivation of dual coded Hebbian phase sequences of activity in mutually exciting neuronal ensembles demonstrated here has been proposed as a general neural coding mechanism for cognitive processing CITATION, CITATION CITATION .
 Alternative splicing contributes to both gene regulation and protein diversity.
 To discover broad relationships between regulation of alternative splicing and sequence conservation, we applied a systems approach, using oligonucleotide microarrays designed to capture splicing information across the mouse genome.
 In a set of 22 adult tissues, we observe differential expression of RNA containing at least two alternative splice junctions for about 40 percent of the 6,216 alternative events we could detect.
 Statistical comparisons identify 171 cassette exons whose inclusion or skipping is different in brain relative to other tissues and another 28 exons whose splicing is different in muscle.
 A subset of these exons is associated with unusual blocks of intron sequence whose conservation in vertebrates rivals that of protein-coding exons.
 By focusing on sets of exons with similar regulatory patterns, we have identified new sequence motifs implicated in brain and muscle splicing regulation.
 Of note is a motif that is strikingly similar to the branchpoint consensus but is located downstream of the 5 splice site of exons included in muscle.
 Analysis of three paralogous membrane-associated guanylate kinase genes reveals that each contains a paralogous tissue-regulated exon with a similar tissue inclusion pattern.
 While the intron sequences flanking these exons remain highly conserved among mammalian orthologs, the paralogous flanking intron sequences have diverged considerably, suggesting unusually complex evolution of the regulation of alternative splicing in multigene families.
 Splicing is an essential process that constructs protein coding messenger RNA sequences using tiny segments of information buried in the much larger primary transcripts of the eukaryotic gene.
 Regulated alternative splicing can create different protein coding sequences under different biological circumstances, allowing the production of functionally related but distinct proteins.
 In addition, alternative splicing can mediate the repression of gene expression by stimulating the formation of transcripts subject to nonsense-mediated decay CITATION CITATION.
 Splicing patterns seem distinct in the vertebrate nervous system compared to other tissues CITATION, CITATION, and it is tempting to hypothesize that neural alternative splicing contributed to the rapid evolution of the vertebrate brain without large increases in gene number CITATION .
 Biochemical analysis of alternative splicing has shown that numerous RNA binding proteins influence the use of specific splice sites to stimulate splicing events that lead to particular mRNA isoforms CITATION, CITATION.
 These RNA binding proteins may activate or repress the use of splice sites by binding to nearby sequences in the exon or in the intron.
 In many cases, multiple RNA binding proteins combine to create repressing and activating influences that produce patterns of splicing control CITATION, CITATION.
 Some proteins, such as SR proteins and the CELF proteins, have mostly activating roles, whereas others, such as hnRNP A1, PTB, and nPTB, have mostly repressing roles.
 Certain proteins can either activate or repress splicing in different contexts, depending on the position of their binding sites or the expression of other RNA binding proteins CITATION, CITATION .
 A complete catalog of the RNA sequences corresponding to the enhancers and silencers bound by splicing regulatory proteins would greatly aid the understanding of splicing regulatory networks.
 Thus far, there are only a handful of splicing regulators whose corresponding RNA binding motifs have been identified, whereas there may be many splicing regulators among the hundreds of RNA binding proteins encoded by the mouse genome.
 In addition, several related but distinct genes produce proteins that bind the same or overlapping sets of sequences; for example, Fox-1 and RBM9 each bind UGCAUG CITATION, CITATION, and the branchpoint binding protein SF1 and the protein quaking each bind UACUAAC-like motifs CITATION CITATION.
 Adding to this complexity is the tendency for the mRNAs of RNA binding proteins to be alternatively spliced, leading to multiple RNA binding protein isoforms with potentially different functions.
 Currently, the methods available for expanding the list of known regulators and their target sequences are limited, and the development of this catalog is in the early stage CITATION .
 Much of the available genomic information on alternative splicing is derived by the alignment of large numbers of expressed sequence tags and messenger RNAs to genome sequences.
 The analysis of exons that appear to be constitutive or alternative has led to the successful identification of many distinguishing features of alternatively spliced regions CITATION CITATION, even allowing their accurate prediction without cDNA evidence CITATION, CITATION, CITATION.
 Although cDNA libraries have been invaluable for discovering general features of alternatively spliced exons, it is difficult to connect specific regulatory sequences to specific biological conditions with confidence due to variable and sometimes missing information about the source materials and methods of cDNA library construction.
 The relatively low number of transcripts present from any one gene also makes it difficult to estimate differences in expression levels using library representation as a measure.
 Thus, more direct methods are needed to associate alternative splicing events with underlying biological conditions.
 The recent application of microarray technology to questions of splicing and splicing regulation promises to reveal parallel connections between many splicing events and specific biological or experimental conditions CITATION CITATION.
 Analysis of experimental changes in splicing for many genes simultaneously should reveal biological conditions necessary for proper splicing regulation in a way that analysis of cDNA libraries cannot, and with breadth that cannot be achieved by analysis of a reporter construct or a few endogenous target genes.
 To demonstrate this, we constructed a DNA microarray designed to capture splicing information for about 6,200 alternative events in the mouse transcriptome, using a combination of splice junction and exon probes, and have hybridized RNA from 22 adult mouse tissues.
 We examine splicing in these tissues by asking three questions.
 First we ask, Which RNA isoforms are present in a particular tissue sample?
 To answer this simple question, we used a new method based on comparing the intensity of the probes in a probe set to the distribution of intensities from all probes with similar G C level.
 This is similar in spirit although different in approach to the present-absent calls from Affymetrix MAS 5.0 algorithms CITATION, as this microarray did not contain mismatch probes.
 Using RT-PCR, we show that this method has a true-positive rate of 85 percent.
 Second we ask, Which RNA isoforms are differentially expressed across the tissues examined?
 For each RNA isoform, the intensities of the isoform-specific junction probes were examined across tissues using the Kruskal-Wallis statistical test.
 After correcting for multiple testing, about 40 percent of the 6,216 total alternative splicing events examined were found to have more than one RNA form that was differentially expressed, indicating widespread tissue differences in splicing over the tissues.
 Third we ask, Which cassette exons are included differentially between brain and nonbrain tissues?
 To answer this, we used a regression-based bootstrapping method, which also allows an estimate of the relative change in skipping and inclusion in the two sample groups.
 We analyzed the intron sequences associated with exon skipping events that are differentially regulated in brain or muscle relative to other tissues and found unusual patterns of sequence conservation that provide new information about tissue regulation of alternative splicing and its evolution.
 In the constraint satisfaction problem ( SYMBOL ), the aim is to find an assignment of values to a set of variables subject to specified constraints
 In the minimum cost homomorphism problem ( SYMBOL ), one is additionally given weights  SYMBOL  for every variable  SYMBOL  and value  SYMBOL , and the aim is to find an assignment  SYMBOL  to the variables that minimizes  SYMBOL
 Let  SYMBOL  denote the  SYMBOL  problem parameterized by the set of predicates allowed for constraints
 SYMBOL  is related to many well-studied combinatorial optimization problems, and concrete applications can be found in, for instance, defence logistics and machine learning
 We show that  SYMBOL  can be studied by using algebraic methods similar to those used for CSPs
 With the aid of algebraic techniques, we classify the computational complexity of  SYMBOL  for all choices of  SYMBOL
 Our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs CITATION
 Constraint satisfaction problems ( SYMBOL ) are a natural way of formalizing a large number of computational problems arising in combinatorial optimization, artificial intelligence, and database theory
 This problem has the following two equivalent formulations: (1) to find an assignment of values to a given set of variables, subject to constraints on the values that can be assigned simultaneously to specified subsets of variables, and (2) to find a homomorphism between two finite relational structures  SYMBOL  and  SYMBOL
 Applications of  SYMBOL s arise in the propositional logic, database and graph theory, scheduling and many other areas
 During the past 30 years,  SYMBOL  and its subproblems has been intensively studied by computer scientists and mathematicians
 Considerable attention has been given to the case where the constraints are restricted to a given finite set of relations  SYMBOL , called a constraint language  CITATION
 For example, when  SYMBOL  is a constraint language over the boolean set  SYMBOL  with four ternary predicates  SYMBOL ,  SYMBOL ,  SYMBOL ,  SYMBOL  we obtain 3-SAT
 This direction of research has been mainly concerned with the computational complexity of  SYMBOL  as a function of  SYMBOL
 It has been shown that the complexity of  SYMBOL  is highly connected with relational clones of universal algebra  CITATION
 For every constraint language  SYMBOL , it has been conjectured that  SYMBOL  is either in P or NP-complete  CITATION
 In the minimum cost homomorphism problem ( SYMBOL ), we are given variables subject to constraints and, additionally, costs on variable/value pairs
 Now, the task is not just to find any satisfying assignment to the variables, but one that minimizes the total cost
 SYMBOL  was introduced in  CITATION  where it was motivated by a real-world problem in defence logistics
 The question for which directed graphs  SYMBOL  the problem  SYMBOL  is polynomial-time solvable was considered in  CITATION
 In this paper, we approach the problem in its most general form by algebraic methods and give a complete algebraic characterization of tractable constraint languages
 From this characterization, we obtain a dichotomy for  SYMBOL , i e , if  SYMBOL  is not polynomial-time solvable, then it is NP-hard
 Of course, this dichotomy implies the dichotomy for directed graphs
 In Section 2, we present some preliminaries together with results connecting the complexity of  SYMBOL  with conservative algebras
 The main dichotomy theorem is stated in Section 3 and its proof is divided into several parts which can be found in Sections 4-8
 The NP-hardness results are collected in Section 4 followed by the building blocks for the tractability result: existence of majority polymorphisms (Section 5) and connections with optimization in perfect graphs (Section 6)
 Section 7 introduces the concept of  arithmetical deadlocks  which lay the foundation for the final proof in Section 8
 In Section 9 we reformulate our main result in terms of relational clones
 Finally, in Section 10 we explain the relation of our results to previous research and present directions for future research
 Metabolic network reconstructions represent valuable scaffolds for -omics data integration and are used to computationally interrogate network properties.
 However, they do not explicitly account for the synthesis of macromolecules.
 Here, we present the first genome-scale, fine-grained reconstruction of Escherichia coli's transcriptional and translational machinery, which produces 423 functional gene products in a sequence-specific manner and accounts for all necessary chemical transformations.
 Legacy data from over 500 publications and three databases were reviewed, and many pathways were considered, including stable RNA maturation and modification, protein complex formation, and iron sulfur cluster biogenesis.
 This reconstruction represents the most comprehensive knowledge base for these important cellular functions in E. coli and is unique in its scope.
 Furthermore, it was converted into a mathematical model and used to: quantitatively integrate gene expression data as reaction constraints and compute functional network states, which were compared to reported experimental data.
 For example, the model predicted accurately the ribosome production, without any parameterization.
 Also, in silico rRNA operon deletion suggested that a high RNA polymerase density on the remaining rRNA operons is needed to reproduce the reported experimental ribosome numbers.
 Moreover, functional protein modules were determined, and many were found to contain gene products from multiple subsystems, highlighting the functional interaction of these proteins.
 This genome-scale reconstruction of E. coli's transcriptional and translational machinery presents a milestone in systems biology because it will enable quantitative integration of -omics datasets and thus the study of the mechanistic principles underlying the genotype phenotype relationship.
 High-throughput experimental technologies enable the production of heterogeneous data, such as expression profiles and proteomic data, for almost any organism of interest.
 A detailed mathematical representation of the in vivo cellular network is required to obtain a holistic understanding of cellular processes from these data sets and to quantitatively integrate them into a biological context.
 One such approach is the bottom-up network reconstruction, which builds manually networks in a brick-by-brick manner using genome annotation and component-specific information CITATION, CITATION.
 This reconstruction procedure is well established for metabolic reaction networks and has been applied to many organisms, including Human CITATION, Saccharomyces cerevisiae CITATION, CITATION, Leishmani major CITATION, Escherichia coli CITATION, Helicobacter pylori CITATION, Pseudomonas aeruginosa CITATION, and Pseudomonas putida CITATION, CITATION .
 These bottom-up metabolic networks differ from other network reconstructions as they are tailored to the genomic content of the target organism and built manually using biochemical, physiological, and other experimental information in addition to the genome annotation.
 Hence, these reconstructions can be thought of as biochemically, genetically, and genomically structured knowledge bases CITATION.
 The reconstruction and modeling procedure is a 4-step process: obtaining a draft reaction list based on genome annotation and biochemical databases, refinement of reaction list using experimental information, conversion of the reaction list into a computable format and application of systems boundaries to define condition-specific models, and the evaluation and validation of the model content using various mathematical methods.
 By iterating step 2 to 4, reconstructions that are self-consistent within their defined scope can be generated.
 Metabolic network reconstruction have demonstrated to be useful in at least 5 areas of applications CITATION : biological discovery CITATION, phenotypic behavior CITATION, bacterial evolution CITATION, network analysis CITATION, and metabolic engineering CITATION.
 This wide range of applications of the metabolic reconstructions is possible because they can be readily converted into predictive, condition-specific models.
 Unlike more traditional approaches to modeling metabolism, the constraint-based modeling approach requires few, if any, parameters CITATION, CITATION.
 The stoichiometric information encoded in the reconstruction can be represented mathematically as a stoichiometric matrix, S, where the rows correspond to the components and the columns correspond to the reactions .
 While the COBRA approach has been successfully applied to metabolic networks, the same principles and assumptions can be also employed to reconstruct and model other cellular functions, such as signaling CITATION CITATION, regulation CITATION, and protein synthesis CITATION.
 In this study, we extended and refined earlier work by Allen et al., which proposed a stoichiometric formalism to model protein synthesis and illustrated it on some E. coli genes and operons CITATION.
 We created a more detailed, gene-specific representation of the transcriptional and translational processes, which explicitly accounts for the sequence-specific synthesis of DNA, mRNA, and proteins.
 This reconstruction enables quantitative integration of high-throughput data such as gene expression, proteomic, and mRNA degradation data.
 Moreover, proteins are produced in high copy numbers in growing cells; thus, any quantitative mechanistic modeling and analysis of high-throughput data needs to account for the synthesis cost associated with these molecules.
 Numerous studies have been published that investigate protein synthesis using kinetic models CITATION CITATION.
 These models are generally tailored to the questions they address making it difficult to readily apply them for modified problems.
 Since stoichiometric relationships are a common requisite for any type of mechanistic modeling, organism-specific BiGG knowledge bases can be used as templates to derive problem-specific, mechanistic models.
 In fact, network stoichiometry is a dominant feature of kinetic models as well CITATION.
 Thus, network reconstruction serves as a platform for steady-state and kinetic modeling .
 In this study, we present a new generation of network reconstructions, which directly account for the synthesis of individual mRNA and proteins.
 We named the mathematical representation of this reconstruction the Expression matrix, or E-matrix, since it encodes the expression of mRNA and proteins.
 All network reactions were formulated to account for gene-specific and E. coli-specific details, such as nucleotide composition, operon association, and sigma factor usage.
 Furthermore, we used information from three databases and more than 500 scientific publications to formulate mechanistically detailed and accurate reactions.
 This reconstruction is the first comprehensive database detailing the available information for these cellular functions and can thus be deemed a knowledge base.
 After conversion of the E-matrix reconstruction into condition-specific models corresponding to different doubling times, we were able to accurately predict the ribosome production reported in literature, without any parameterization.
 Furthermore, we show that the E-matrix can be used to study the effect of rRNA operon deletion.
 Our results predict that a high density of RNA polymerases is required on the remaining rRNA operons, to achieve the reported ribosome numbers.
 Finally, we show that proteins used in the E-matrix could be grouped into functional modules which lead to a more simplified view of the network.
 Neuronal activity is mediated through changes in the probability of stochastic transitions between open and closed states of ion channels.
 While differences in morphology define neuronal cell types and may underlie neurological disorders, very little is known about influences of stochastic ion channel gating in neurons with complex morphology.
 We introduce and validate new computational tools that enable efficient generation and simulation of models containing stochastic ion channels distributed across dendritic and axonal membranes.
 Comparison of five morphologically distinct neuronal cell types reveals that when all simulated neurons contain identical densities of stochastic ion channels, the amplitude of stochastic membrane potential fluctuations differs between cell types and depends on sub-cellular location.
 For typical neurons, the amplitude of membrane potential fluctuations depends on channel kinetics as well as open probability.
 Using a detailed model of a hippocampal CA1 pyramidal neuron, we show that when intrinsic ion channels gate stochastically, the probability of initiation of dendritic or somatic spikes by dendritic synaptic input varies continuously between zero and one, whereas when ion channels gate deterministically, the probability is either zero or one.
 At physiological firing rates, stochastic gating of dendritic ion channels almost completely accounts for probabilistic somatic and dendritic spikes generated by the fully stochastic model.
 These results suggest that the consequences of stochastic ion channel gating differ globally between neuronal cell-types and locally between neuronal compartments.
 Whereas dendritic neurons are often assumed to behave deterministically, our simulations suggest that a direct consequence of stochastic gating of intrinsic ion channels is that spike output may instead be a probabilistic function of patterns of synaptic input to dendrites.
 The appropriate level of physical detail required to understand how complex processes such as cognition and behavior emerge from more simple biological structures is unclear CITATION, CITATION.
 For example, while it is possible to account for certain aspects of nervous system function using models that represent each neuron as a simple integrate and fire device, it is increasingly clear that this approach does not capture the full range of computations that many real neurons carry out CITATION, CITATION.
 Dendritic and axonal morphology are defining features of neuronal cell types and have important influences on the computations that a neuron performs CITATION.
 Differences in morphology determine how neurons respond to synaptic input and are sufficient to produce distinct patterns of spontaneous activity CITATION and degrees of action potential back-propagation from the soma into the dendrites CITATION.
 Cable theory and compartmental modeling provide a foundation for predicting the propagation of electrical signals in the dendrites and axons of neurons CITATION, CITATION.
 However, while the assumption that transitions between open and closed states of ion channels can be treated as a deterministic process may be sufficient for some purposes, recent evidence suggests that stochastic transitions between the states of individual ion channels could influence computations carried out by neurons CITATION CITATION.
 Stochastic opening and closing of ion channels causes noisy fluctuations in the current or voltage recorded from a neuron CITATION CITATION.
 While cable theory suggests that fluctuations of this kind might be particularly important in fine structures such as axons and dendrites CITATION, we nevertheless know very little about how neuronal morphology and stochastic gating of ion channels interact to determine how neurons respond to synaptic input.
 Given the difficulty of reducing detailed morphological models to simple analytical forms that could also incorporate stochastic gating of individual ion channels CITATION, experimentally constrained numerical simulations will be important to enable these issues to be explored systematically.
 Investigation of stochastic ion channel gating using numerical simulations has been limited by trades-offs between simulation accuracy and computation time CITATION.
 A simple approach is to add noise sources to deterministic models.
 However, as ion channels have multiple functional states with transitions that often depend on the membrane voltage CITATION, CITATION, CITATION, CITATION, this may not accurately account for the noise introduced by ion channel currents.
 A more accurate alternative is to explicitly model transitions between different functional states for each ion channel on a neuron's membrane.
 However, for neurons with complex axonal or dendritic architectures there are two substantial obstacles to this approach.
 First, typical central neurons express large numbers of ion channels and simulations must be repeated many times to obtain statistically valid descriptions CITATION.
 This is a formidable computational task and even relatively straightforward simulations of the consequences of stochastic channel gating can require substantial computing time.
 Second, each neuronal ion channel occupies a specific location on the extra-cellular membrane, whereas most neuronal models represent the distribution of ion channels as the density of a deterministic conductance across an area of membrane.
 Although this formalism has been successful for simulating many aspects of neuronal activity, it is of less use for models that explore the consequences of the localization of individual ion channels, for example to evaluate the macroscopic effects of short range interactions between ion channels and other signaling molecules CITATION, or the consequences of spatially heterogeneous distributions of ion channels within relatively small sub-cellular structures such as dendritic spines and axon terminals CITATION, CITATION .
 To address the functional consequences of stochastic ion channel gating in neurons with extensive dendritic or axonal arborizations we developed a parallel stochastic ion channel simulator, which enables efficient simulation of the electrical activity of neurons with complex morphologies and arbitrary localization of stochastic ion channels on the extracellular membrane, while also addressing limitations of previous approaches.
 We have also developed an interactive tool for visualization and development of models of neurons containing uniquely located ion channels.
 Here, we illustrate the use of PSICS and ICING, outline the computational strategies used and provide benchmark data for evaluation.
 We then identify previously unappreciated differences between the effects of stochastic ion channel gating on somatic and dendritic membrane potential activity in several different morphological classes of neuron.
 We show that the consequences of stochastic gating depend on dendritic morphology and suggest novel functional roles for the kinetics of ion channel gating.
 Using a previously well-validated realistic model of a CA1 pyramidal neuron we demonstrate that stochastic ion channel gating influences spike output in response to dendritic synaptic input.
 We show that stochastic gating of axonal or dendritic ion channels substantially modifies synaptically driven dendritic and axonal spike output, with stochastic gating of voltage-dependent sodium and potassium channels having the greatest impact and hyperpolarization-activated channels the least.
 By demonstrating that neuronal responses to dendritic synaptic input can be intrinsically probabilistic, these results offer a new and general perspective on synaptic integration by central neurons.
 Full documentation for PSICS/ICING as well as the software, source code and examples are available from the project website .
 The tradeoff between the need to suppress drug-resistant viruses and the problem of treatment toxicity has led to the development of various drug-sparing HIV-1 treatment strategies.
 Here we use a stochastic simulation model for viral dynamics to investigate how the timing and duration of the induction phase of induction maintenance therapies might be optimized.
 Our model suggests that under a variety of biologically plausible conditions, 6 10 mo of induction therapy are needed to achieve durable suppression and maximize the probability of eradicating viruses resistant to the maintenance regimen.
 For induction regimens of more limited duration, a delayed-induction or -intensification period initiated sometime after the start of maintenance therapy appears to be optimal.
 The optimal delay length depends on the fitness of resistant viruses and the rate at which target-cell populations recover after therapy is initiated.
 These observations have implications for both the timing and the kinds of drugs selected for induction maintenance and therapy-intensification strategies.
 The failure of antiretroviral therapies to completely suppress viral replication in some patients represents a major difficulty in the management of HIV infection.
 In therapy-naive patients without clinically apparent resistance mutations, triple-drug therapy with two nucleoside analog reverse transcriptase inhibitors and a protease inhibitor or a non-nucleoside reverse transcriptase inhibitor is standard CITATION.
 In these patients, treatment success rates, defined as viral load 50 copies/ml at 48 wk, range from 70 percent to 80 percent 85 percent.
 However, in patients with previous regimen failure requiring salvage therapy, response rates are usually considerably lower CITATION CITATION, and it is frequently not possible to assemble a three-drug regimen with uncompromised activity against all viral strains present.
 In these individuals, treatment failure often occurs after an initial period of response to a new regimen, and is usually associated with the appearance of multiply drug-resistant viral strains.
 This has led to attempts to treat highly experienced patients with various deep salvage regimens consisting of four, five, or six individual drugs CITATION CITATION.
 These patients are particularly vulnerable to the many drug interactions CITATION and adverse metabolic, hematologic, neurologic, cardiovascular, and gastrointestinal side effects that complicate HIV therapy and seriously undermine the success of clinical management CITATION CITATION .
 The need to minimize drug resistance while reducing treatment-related toxicities has engendered an interest in induction maintenance strategies, in which a period of intensified antiretroviral therapy is followed by a simplified long-term regimen CITATION CITATION.
 Most such trials have yielded higher failure rates in the treatment group than in controls receiving conventional therapy.
 Failure typically occurs during maintenance therapy, and has been attributed to poor regimen adherence CITATION and recrudescence of resistance mutations present before institution of induction therapy CITATION.
 One weakness of existing studies has been that induction therapy consisted of standard three-drug antiretroviral therapy regimens in common clinical use at the time of the study, under conditions now recognized to permit subclinical viral replication CITATION, CITATION.
 Moreover, in these early studies, the induction phase only lasted between 3 to 6 mo, which may be insufficient.
 However, two recent studies have shown the apparent effectiveness of induction therapy for 48 wk followed by maintenance therapy with atazanavir CITATION or lopinvir/ritonavir CITATION, CITATION, and this has led to new optimism concerning IM approaches.
 We have hypothesized that a longer period of a highly suppressive induction therapy that is appropriately timed relative to the start of maintenance therapy may allow minority resistant variants to decay below a stochastic extinction threshold, allowing for successful long-term treatment with simpler and better-tolerated regimens.
 To explore this hypothesis quantitatively, we constructed a detailed computer simulation model of the dynamics of sensitive and resistant viruses during a hypothetical IM regimen.
 We show that the timing and duration of induction therapy relative to maintenance therapy can affect the probability that viruses resistant to the maintenance regimen will be eradicated in ways that are somewhat counterintuitive.
 Under biologically plausible conditions, we find that 6 10 mo of induction therapy are required to maximize the probability of eradicating these resistant viruses.
 For shorter induction periods, we find that it is optimal to use a delayed-induction regimen administered several days to weeks after the start of the intended long-term maintenance therapy.
	most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used  independently of any algorithm
	in contrast  the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties
	however  as in much of learning theory  existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed
	in many machine learning applications  however  this assumption does not hold
	the observations received by the learning algorithm often have some inherent temporal dependence
	this paper studies the scenario where the observations are drawn from a stationary   mixing or   mixing sequence  a widely adopted assumption in the study of non independent and identically distributed   processes that implies a dependence between observations weakening over time
	we prove novel and distinct stability based generalization bounds for stationary   mixing and   mixing sequences
	these bounds strictly generalize the bounds given in the independent and identically distributed   case and apply to all stable learning algorithms  thereby extending the use of stability bounds to non independent and identically distributed   scenarios
	we also illustrate the application of our   mixing generalization bounds to general classes of learning algorithms  including support vector regression  kernel ridge regression  and support vector machines  and many other kernel regularization based and relative entropy based regularization algorithms
	these novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non independent and identically distributed   scenarios   
	most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used  such as the vc dimension  covering numbers  or rademacher complexity
	these measures characterize a class of hypotheses  independently of any algorithm
	in contrast  the notion of algorithmic stability can be used to derive bounds that are tailored to specific learning algorithms and exploit their particular properties
	a learning algorithm is stable if the hypothesis it outputs varies in a limited way in response to small changes made to the training set
	algorithmic stability has been used effectively in the past to derive tight generalization bounds  CITATION
	but  as in much of learning theory  existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed  independent and identically distributed  
	in many machine learning applications  this assumption  however  does not hold  in fact  the independent and identically distributed   assumption is not tested or derived from any data analysis
	the observations received by the learning algorithm often have some inherent temporal dependence
	this is clear in system diagnosis or time series prediction problems
	clearly  prices of different stocks on the same day  or of the same stock on different days  may be dependent
	but  a less apparent time dependency may affect data sampled in many other tasks as well
	this paper studies the scenario where the observations are drawn from a stationary   mixing or   mixing sequence  a widely adopted assumption in the study of non independent and identically distributed   processes that implies a dependence between observations weakening over time  CITATION
	we prove novel and distinct stability based generalization bounds for stationary   mixing and   mixing sequences
	these bounds strictly generalize the bounds given in the independent and identically distributed   case and apply to all stable learning algorithms  thereby extending the usefulness of stability bounds to non independent and identically distributed   scenarios
	our proofs are based on the independent block technique described by  CITATION  and attributed to  CITATION   which is commonly used in such contexts
	however  our analysis differs from previous uses of this technique in that the blocks of points considered are not of equal size
	for our analysis of stationary   mixing sequences  we make use of a generalized version of mcdiarmid s inequality  CITATION  that holds for   mixing sequences
	this leads to stability based generalization bounds with the standard exponential form
	our generalization bounds for stationary   mixing sequences cover a more general non independent and identically distributed   scenario and use the standard mcdiarmid s inequality  however  unlike the   mixing case  the   mixing bound presented here is not a purely exponential bound and contains an additive term depending on the mixing coefficient
	we also illustrate the application of our   mixing generalization bounds to general classes of learning algorithms  including support vector regression  svr   CITATION   kernel ridge regression  CITATION   and support vector machines  svms   CITATION
	algorithms such as support vector regression  svr   CITATION  have been used in the context of time series prediction in which the independent and identically distributed   assumption does not hold  some with good experimental results  CITATION
	to our knowledge  the use of these algorithms in non independent and identically distributed   scenarios has not been previously supported by any theoretical analysis
	the stability bounds we give for svr  svms  and many other kernel regularization based and relative entropy based regularization algorithms can thus be viewed as the first theoretical basis for their use in such scenarios
	the following sections are organized as follows
	in section   we introduce the necessary definitions for the non independent and identically distributed   problems that we are considering and discuss the learning scenarios in that context
	section  gives our main generalization bounds for stationary   mixing sequences based on stability  as well as the illustration of its applications to general kernel regularization based algorithms  including svr  krr  and svms  as well as to relative entropy based regularization algorithms
	finally  section  presents the first known stability bounds for the more general stationary   mixing scenario
 This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA)
 We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice
 Experiments show the proposed QA algorithm finds better clustering assignments than SA
 Furthermore, QA is as easy as SA to implement
 Clustering is one of the most popular methods in data mining
 Typically, clustering problems are formulated as optimization problems, which are solved by algorithms, for example the EM algorithm or convex relaxation
 However, clustering is typically NP-hard
 The simulated annealing (SA)  CITATION  is a promising candidate
 CITATION  proved SA was able to find the global optimum with a slow cooling schedule of temperature  SYMBOL
 Although their schedule is in practice too slow for clustering of a large amount of data, it is well known that SA still finds a reasonably good solution even with a faster schedule than what CITATION proposed
 In statistical mechanics, quantum annealing (QA) has been proposed as a novel alternative to SA  CITATION
 QA adds another dimension,  SYMBOL , to SA for annealing, see Fig
 Thus, it can be seen as an extension of SA
 QA has succeeded in specific problems, e g the Ising model in statistical mechanics, and it is still unclear that QA works better than SA in general
 We do not actually think QA intuitively helps clustering, but we apply QA to clustering just as procedure to derive an algorithm
 A derived QA algorithm depends on the definition of quantum effect  SYMBOL
 We propose quantum effect  SYMBOL , which leads to a search strategy fit to clustering
 Our contribution is, 1) to propose a QA-based optimization algorithm for clustering, in particular quantum effect  SYMBOL  for clustering  and a good annealing schedule, which is crucial for applications, 2) and to experimentally show the proposed algorithm optimizes clustering assignments better than SA
 We also show the proposed algorithm is as easy as SA to implement
 The algorithm we propose is a Markov chain Monte Carlo (MCMC) sampler, which we call QA-ST sampler
 As we explain later, a naive QA sampler is intractable even with MCMC
 Thus, we approximate QA by the Suzuki-Trotter (ST) expansion  CITATION  to derive a tractable sampler, which is the QA-ST sampler
 QA-ST looks like parallel  SYMBOL  SAs with interaction  SYMBOL  (see Fig )
 At the beginning of the annealing process, QA-ST is almost the same as  SYMBOL  SAs
 Hence, QA-ST finds  SYMBOL  (local) optima independently
 As the annealing process continues, interaction  SYMBOL  in Fig becomes stronger to move  SYMBOL  states closer
 QA-ST at the end picks up the state with the lowest energy in  SYMBOL  states as the final solution
 QA-ST with the proposed quantum effect  SYMBOL  works well for clustering
 Fig is an example where data points are grouped into four clusters
 SYMBOL and  SYMBOL are locally optimal and  SYMBOL  is globally optimal
 Suppose  SYMBOL  is equal to two and  SYMBOL  and  SYMBOL  in Fig correspond to  SYMBOL  and  SYMBOL  in Fig
 Although  SYMBOL  and  SYMBOL  are local optima, the interaction  SYMBOL  in Fig allows  SYMBOL  and  SYMBOL  to search for a better clustering assignment between  SYMBOL  and  SYMBOL
 Quantum effect  SYMBOL  defines the distance metric of clustering assignments
 In this case, the proposed  SYMBOL  locates  SYMBOL  between  SYMBOL  and  SYMBOL
 Thus, the interaction  SYMBOL  gives good chance to go to  SYMBOL  because  SYMBOL  makes  SYMBOL  and  SYMBOL  closer (see Fig )
 The proposed algorithm actually finds  SYMBOL  from  SYMBOL  and  SYMBOL
 Fig is just an example
 However, a similar situation often occurs in clustering
 Clustering algorithms in most cases give ``almost'' globally optimal solutions like  SYMBOL  and  SYMBOL , where the majority of data points are well-clustered, but some of them are not
 Thus, a better clustering assignment can be constructed by picking up well-clustered data points from many sub-optimal clustering assignments
 Note an assignment constructed in such a way is located between the sub-optimal ones by the proposed quantum effect  SYMBOL  so that QA-ST can find a better assignment between sub-optimal ones
	The influence of lipid molecules on the aggregation of a highly amyloidogenic segment of human islet amyloid polypeptide, hIAPP20 29, and the corresponding sequence from rat has been studied by all-atom replica exchange molecular dynamics simulations with explicit solvent model.
	hIAPP20 29 fragments aggregate into partially ordered -sheet oligomers and then undergo large conformational reorganization and convert into parallel/antiparallel -sheet oligomers in mixed in-register and out-of-register patterns.
	The hydrophobic interaction between lipid tails and residues at positions 23 25 is found to stabilize the ordered -sheet structure, indicating a catalysis role of lipid molecules in hIAPP20 29 self-assembly.
	The rat IAPP variants with three proline residues maintain unstructured micelle-like oligomers, which is consistent with non-amyloidogenic behavior observed in experimental studies.
	Our study provides the atomic resolution descriptions of the catalytic function of lipid molecules on the aggregation of IAPP peptides.
	A range of human diseases including Alzheimer's disease, Parkinson's disease, the spongiform encephalopathy and type 2 diabetes mellitus is associated with amyloid deposits of normally soluble proteins or peptides CITATION CITATION.
	In T2DM, the main protein component of fibrillar protein deposits in the pancreatic islets of langerhans has been identified as a 37-residue hormone referred to as islet amyloid polypeptide or amylin CITATION, which is synthesized in -cells of the pancreas and cosecreted with insulin CITATION, CITATION.
	There are convincing evidences that the toxicity of amyloid related diseases may be caused by the soluble intermediate oligomers instead of mature fibrils CITATION CITATION, and the interaction between lipid bilayer and these soluble oligomer CITATION CITATION.
	For example, channel-like annular structures of oligomers of several amyloidogenic peptides have been observed on the lipid membrane CITATION, CITATION, and have been studied by molecular dynamics simulations as well CITATION, CITATION.
	Moreover, up to 10 percent components in amyloid deposits from patient tissues were lipid molecules, indicating that the lipids can be uptaken from membranes and then wrapped into fibrillar amyloid CITATION CITATION.
	Most studies so far treated the lipid bilayer as a template to exert its influences on the conformation and aggregation properties of peptides CITATION CITATION.
	There is, however, missing information about how individual lipid molecule involving in the peptide aggregation process.
	It will then be beneficial to understand the molecular details of how single lipid molecule influences the assembly process of amyloidogenic peptides which is the main focus of the current study.
	Besides the external factors, such as lipid bilayer, pH value, the sequences of peptide themselves have great effects on the aggregation behaviors.
	Several other species such as non-human primates CITATION, cats CITATION, raccoons CITATION, and rodent species can produce IAPP, but the primary sequence of IAPP varies slightly among species.
	Importantly, IAPP from rodent species, such as rat/mouse IAPP lose capacities of aggregating into amyloid fibrils CITATION, but transgenic mouse models that express human IAPP develop islet deposits CITATION.
	The rIAPP differs from hIAPP in six amino acids and five of them are clustered in a short decapeptide, which is considered to be strongly amyloidogenic and forms similar unbranched fibrils itself to the full-length hIAPP CITATION, CITATION.
	The three proline substitutions in rIAPP20 29 are believed to be highly responsible for the lacking of the amyloidogenic property of the segment or full-length peptide CITATION.
	Although rIAPP has been intensively applied in experimental research acting as a potential peptide inhibitor for peptide aggregation CITATION, CITATION, the molecular mechanism of its resistance to amyloid is still not crystal clear.
	Here, the aggregation of rIAPP20 29 segments is subjected to the same simulation condition as hIAPP20 29 to explore the non-amyloidogenic properties of the peptide and meanwhile to evaluate the simulation results as a negative control.
	Due to the metastable and short-lived nature of soluble pre-fibril oligomers at the early steps of fibril formation, experimental data are usually difficult to obtain CITATION, CITATION.
	Thus, the computational approaches have been employed to complement experimental investigations to gain the insight into the aggregation mechanisms CITATION CITATION.
	Considering multiple copies of peptides needed due to the self-assembly nature of amyloid formation, various simplified representations of molecular systems using implicit solvent models were preferred rather than all-atom models.
	Santini et al. performed ART-OPEP simulations on trimer of A 16 22 by treating side chains as a bead and solvent implicitly CITATION.
	A novel mechanism for single -strand to surmount unnatural registry without dissociation, referred to as reptation was proposed before experimental characterization CITATION.
	Cheon et al. used ProFASi package to reduce the bonded potential energy to include torsional angles only and treated hydrogen bonds explicitly CITATION.
	They were able to carry out two series of 100 Monte Carlo simulations on 20 copies of two fragments A 16 22 and A 25 35.
	They observed early-stage events and obtained an atomic-detailed description of nucleated conformational conversion CITATION model for amyloid aggregation.
	In these studies, simulations were usually started with randomly oriented, extended or random-coiled peptides which underwent ab initio folding to form -sheet oligomers.
	Albeit simplified models allow studying large-scale systems CITATION or observing more events in limited simulation time CITATION, all-atom explicit solvent models can reproduce amyloid aggregation in aqueous environment more accurately and supply more information on sidechain contacts CITATION.
	Nguyen et al. prolonged a series of conventional MD simulations to 300 ns on A 16 22 of 3 6 oligomer size with explicit solvent CITATION.
	The extensive simulations were able to probe the interpeptide sidechain contacts and large conformational fluctuations upon monomer addition to preformed -sheet oligomers in a dock-lock mechanism.
	In our studies, an enhanced-sampling method, replica exchange molecular dynamics CITATION was implemented CITATION, and all water and peptide atoms are treated explicitly by applying OPLS-AA force field CITATION.
	The four copies of amyloidogenic segment hIAPP20 29 and an extra dioleoylphosphatidylcholine lipid molecule were initially set in extended conformation and dispersed in simulation boxes.
	The formation of -sheet containing tetramers, was observed within 100 ns ab initio REMD folding simulations.
	The acquirement of abundant intermediate states suggested two possible -sheet transition pathways.
	Simulation of four hIAPP peptides without lipid molecule was also performed.
	Nonamyloidogenic rat IAPP segments were studied as a negative control with the aim of understanding the inhibitory effect of three proline substitutions.
 this study was designed to assess sex-related differences in the selection of an appropriate strategy when facing novelty
 a simple visuo-spatial task was used to investigate exploratory behavior as a specific response to novelty
 the exploration task was followed by a visual discrimination task  and the responses were analyzed using signal detection theory
 during exploration women selected a local searching strategy in which the metric distance between what is already known and what is unknown was reduced  whereas men adopted a global strategy based on an approximately uniform distribution of choices
 women's exploratory behavior gives rise to a notion of a secure base warranting a sense of safety while men's behavior does not appear to be influenced by risk
 this sex-related difference was interpreted as a difference in beliefs concerning the likelihood of uncertain events influencing risk evaluation
 males and females seem to differ in spatial abilities and styles  CITATION
 generally  studies involving navigational problems showed that female cognitive style relies more on detailed information  while male style relies more on global information  CITATION
 evolutionary mechanisms could potentially account for sex differences in spatial behavior
 for example  these behavioral differences may be due to mating patterns that induced a selection of large-range navigation in males  CITATION
 mating patterns or mating strategies are linked to the dynamics of reproduction and sexual selection  CITATION   and sexual selection is restricted to characteristics that influence mate choice and competition for mates
 typically  males have to compete through extensive ranging for access to mates while females have to choose mating partners according to reproductive success  CITATION
 another proposition  but exclusively directed at humans  suggested that the division of labor game hunting and plant gathering would have put greater selection pressure on females' spatial memory because females sustained gathering duties  CITATION
 however  as argued by ecuyer-dab and robert  CITATION    the selection of male characteristics depends on females' choice for mates
 in females  however  spatial cognition would have been primarily shaped by the natural selection of a strong concern for survival both of self and of offspring
 this concern would have compelled them to favor low-risk strategies  like concentrating on proximal spatial cues  when coping with space-related problems
 such focusing would have enabled secure navigation based on detailed landmark encoding  as well as  in certain species  regular feeding based on remembering the exact locations of potential resources  p  NUMBER 
 thus  the hypothesis of labor division would be a by-product of sexual selection and not the cause of sex differences in spatial behavior
 taken together  the literature seems to indicate that the key to understanding the evolution of behavioral sex differences relies on the relative costs and benefits of producing offspring  CITATION
 in that context spatial skills play a crucial role since they increase reproductive success and the accessibility to food resource but  at the same time  multiply the risks of getting lost  being killed or consumed by other animals predation
 hence  the survival of mobile species depends on their ability to balance costs and benefits induced by locomotion and this balancing should differ according to sex
 experimental investigations of sex differences in spatial abilities yield apparently disparate results  CITATION
 this might be partly due to the complexity of contemporary experimental designs  but also to a lack of investigations concerning decision-making processes involved in the selection of strategies
 the current study investigates sex differences in basic behaviors like exploration  detection and discrimination involving the selection of strategies when coping with uncertainty
 following the above quotation from ecuyer-dab and robert's  the hypothesis is that women  compared to men  should favor low-risk strategies when coping with space-related problems
 to test this  i used a simple spontaneous two-dimensional exploratory task
 this choice relies on the fact that exploration is a natural behavior and that it is fundamental in acquiring spatial knowledge
 it seems to be based on driving factors such as curiosity  comfort or mastery over one's environment
 moreover  it is commonly defined as serving to reduce uncertainty and thus allow coping with fear  CITATION
 exploration is mainly characterized by a succession of progressions and stops  CITATION   and the selection of exploration could rely on its capacity to act as a regulator of uncertainty
 indeed  progressions are based on decisions taken during stops  and stops correspond to choice points allowing decisions
 voss  CITATION  refers to the exploration process as the generation and testing of hypotheses concerning the object's meaning and potential use
 in order to assess risk-taking  a classical visual discrimination task based on the stimuli observed during the exploration task was used
 the results were analyzed with signal detection theory
 in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its perceived disutility relative to not testing is greater than the utility associated with prevention of possible disease
 the prospect theory editing operation, by which a decision maker's reference point is determined, can have important effects on the disutility of the test
 on the basis of the prospect theory value function, this paper develops two approaches to reducing disutility by directing the decision maker's attention to either actual past or expected future losses that result in shifted reference points
 after providing a graphical description of the approaches and a mathematical proof of the direction of their effect on judgment, we briefly illustrate the potential value of these approaches with examples from qualitative research on prostate cancer treatment decisions
 in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its disutility relative to not testing is greater than the utility associated with prevention of possible disease
 for example, people may feel that the anticipated disutility of a colonoscopy for colorectal cancer screening is great enough relative to the expected utilty of prevention of possible colorectal cancer to dissuade them from seeking colonoscopy
 the prospect theory editing operation  CITATION , by which a decision maker's reference point is determined, can have important impacts on the perceived disutility of the test
 the work of rothman, salovey, and colleagues on message framing has tested prospect theory predictions of how the description of test outcomes as gains or losses as well as the conceptualization of the purpose of the test as preventative vs diagnostic and the consequent perception of whether the test is "safe" or "risky" can affect test rates  CITATION
 specifically, message framing theories predict that when a procedure is perceived as risky e g , cancer screening tests may cause a patient to find out that they have cancer, loss-framed messages will promote testing more strongly than gain-framed messages, because people favor risky prospects over sure prospects in the domain of losses
 on the other hand, when a procedure is perceived as safe e g , sunscreen prevents sunburn and skin cancer, gain-framed messages are predicted to be more effective because people prefer sure prospects to risky prospects in the domain of gains
 several public health intervention studies have examined message framing and generally found evidence favoring the predictions  CITATION
 on the basis of the prospect theory value function, this paper develops two approaches to reducing perceived disutility by directing the decision maker's attention to either actual past or expected future losses that can serve as reference points and are not consequences of the test itself
 these approaches thus differ from message framing, which focuses on how the test outcomes are described and manipulates gain and loss framing
 we instead derive the potential impact of directly refocusing the decision maker's reference point
	although the internet as level topology has been extensively studied over the past few years  little is known about the details of the as taxonomy
	an as  node  can represent a wide variety of organizations  e g   large isp  or small private business  university  with vastly different network characteristics  external connectivity patterns  network growth tendencies  and other properties that we can hardly neglect while working on veracious internet representations in simulation environments
	in this paper  we introduce a radically new approach based on machine learning techniques to map all the ases in the internet into a natural as taxonomy
	we successfully classify  NUMBER   NUMBER  percent  of ases with expected accuracy of  NUMBER   NUMBER  percent 
	we release to the community the as level topology dataset augmented with   NUMBER   the as taxonomy information and  NUMBER   the set of as attributes we used to classify ases
	we believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the internet
	the rapid expansion of the internet in the last two decades has produced a large scale system of thousands of diverse  independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments
	from  NUMBER  to  NUMBER  the number of globally routable as identifiers has increased from less than  NUMBER   NUMBER  to more than  NUMBER   NUMBER   exerting significant pressure on interdomain routing as well as other functional and structural parts of the internet
	this impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the internet infrastructure
	in particular  the as level topology is an intermix of networks owned and operated by many different organizations  e g   backbone providers  regional providers  access providers  universities and private companies
	statistical information that faithfully characterizes different as types is on the critical path toward understanding the structure of the internet  as well as for modeling its topology and growth
	in topology modeling  knowledge of as types is mandatory for augmenting synthetically constructed or measured as topologies with realistic intra as and inter as router level topologies
	for example  we expect the network of a dual homed university to be drastically different from that of a dual homed small company
	the university will likely contain dozens of internal routers  thousands of hosts  and many other network elements  switches  servers  firewalls 
	on the other hand  the small company will most probably have a single router and a simple network topology
	since there is such a diversity among different network types  we cannot accurately augment the as level topology with appropriate router level topologies if we cannot 	characterize the composing ases
	moreover  annotating the ases in the as topology with their types is a prerequisite for modeling the evolution of the internet  since different types of ases exhibit different growth patterns
	for example  internet service providers  isp  grow by attracting new customers and by engaging in business agreements with other isps
	on the other hand  small companies that connect to the internet through one or few isps do not grow significantly over time
	thus  categorizing different types of ases in the internet is necessary to identify network evolution patterns and develop accurate evolution models
	an as taxonomy is also necessary for mapping ip addresses to different types of users
	for example  in traffic analysis studies its often required to distinguish between packets that come from home and business users
	given an as taxonomy  its possible to realize this goal by checking the type of as that originates the prefix in which an ip address lies
	in this work  we introduce a radically new approach based on machine learning to construct a representative as taxonomy
	we develop an algorithm to classify ases based on empirically observed differences between as characteristics
	we use a large set of data from the internet routing registries  irr   CITATION  and from routeviews  CITATION  to identify intrinsic differences between ases of different types
	then  we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ases into six representative classes that reflect ases with different network properties and infrastructures
	we derive macroscopic statistics on the different types of ases in the internet and validate our results using a sample of  NUMBER  manually identified as types
	our validation demonstrates that our classification algorithm achieves high accuracy   NUMBER   NUMBER  percent  of the examined classifications were correct
	finally  we make our results and our classifier publicly available to promote further research and understanding of the internet s structure and evolution
	in section  we start with a brief discussion of related work
	section  describes the data we used  and in section  we specify the set of as classes we use in our experiments
	section  introduces our classification approach and results
	we validate them in section  and conclude in section 
 We introduce a new principle for model selection in regression and classification
 Many regression models are controlled by some smoothness or flexibility or complexity parameter  SYMBOL , eg the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials
 Let  SYMBOL  be the (best) regressor of complexity  SYMBOL  on data  SYMBOL
 A more flexible regressor can fit more data  SYMBOL  well than a more rigid one
 If something (here small loss) is easy to achieve it's typically worth less
 We define the loss rank of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL
 We suggest selecting the model complexity  SYMBOL  that has minimal loss rank (LoRP)
 Unlike most penalized maximum likelihood variants (AIC,BIC,MDL), LoRP only depends on the regression functions and the loss function
 It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN
 In this paper we formalize, discuss, and motivate LoRP, study it for specific regression problems, in particular linear ones, and compare it to other model selection schemes
 Consider a regression or classification problem in which we want to determine the functional relationship  SYMBOL  from data  SYMBOL , ie we seek a function  SYMBOL  such that  SYMBOL  is close to the unknown  SYMBOL  for all  SYMBOL
 One may define regressor  SYMBOL  directly, eg `average the  SYMBOL  values of the  SYMBOL  nearest neighbors (kNN) of  SYMBOL  in  SYMBOL ', or select the  SYMBOL  from a class of functions  SYMBOL  that has smallest (training) error on  SYMBOL
 If the class  SYMBOL  is not too large, e g the polynomials of fixed reasonable degree  SYMBOL , this often works well
 What remains is to select the right model complexity  SYMBOL , like  SYMBOL  or  SYMBOL
 This selection cannot be based on the training error, since the more complex the model (large  SYMBOL , small  SYMBOL ) the better the fit on  SYMBOL  (perfect for  SYMBOL  and  SYMBOL )
 This problem is called overfitting, for which various remedies have been suggested:  We will not discuss empirical test set methods like cross-validation, but only training set based methods
 See eg CITATION  for a comparison of cross-validation with Bayesian model selection
 Training set based model selection methods allow using all data  SYMBOL  for regression
 The most popular ones can be regarded as penalized versions of Maximum Likelihood (ML)
 In addition to the function class  SYMBOL , one has to specify a sampling model  SYMBOL , eg that the  SYMBOL  have independent Gaussian distribution with mean  SYMBOL
 ML chooses  SYMBOL , Penalized ML (PML) then chooses  SYMBOL Penalty SYMBOL , where the penalty depends on the used approach (MDL  CITATION , BIC  CITATION , AIC  CITATION )
 In particular, modern MDL  CITATION  has sound exact foundations and works very well in practice
 All PML variants rely on a proper sampling model (which may be difficult to establish), ignore (or at least do not tell how to incorporate) a potentially given loss function, and are typically limited to (semi)parametric models
 The main goal of the paper is to establish a criterion for selecting the ``best'' model complexity  SYMBOL based on regressors  SYMBOL  given as a black box without insight into the origin or inner structure of  SYMBOL , that does not depend on things often not given (like a stochastic noise model),  and that exploits what is given (like the loss function)
 The key observation we exploit is that large classes  SYMBOL  or more flexible regressors  SYMBOL  can fit more data  SYMBOL  well than more rigid ones, eg many  SYMBOL  can be fit well with high order polynomials
 We define the  loss rank  of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL , as measured by some loss function
 The loss rank is large for regressors fitting  SYMBOL  not well  and  for too flexible regressors (in both cases the regressor fits many other  SYMBOL  better)
 The loss rank has a minimum for not too flexible regressors which fit  SYMBOL  not too bad
 We claim that minimizing the loss rank is a suitable model selection criterion, since it trades off the quality of fit with the flexibility of the model
 Unlike PML, our new Loss Rank Principle (LoRP) works without a noise (stochastic sampling) model, and is directly applicable to any non-parametric regressor, like kNN
 In Section , after giving a brief introduction to regression, we formally state LoRP for model selection
 To make it applicable to real problems, we have to generalize it to continuous spaces and regularize infinite loss ranks
 In Section  we derive explicit expressions for the loss rank for the important class of linear regressors, which includes kNN, polynomial, linear basis function (LBFR), Kernel, and projective regression
 In Section  we compare linear LoRP to Bayesian model selection for linear regression with Gaussian noise and prior, and in Section  to PML, in particular MDL, BIC, AIC, and MacKay's  CITATION  and Hastie's et al  CITATION  trace formulas for the effective dimension
 In this paper we just scratch at the surface of LoRP
 Section  contains further considerations, to be elaborated on in the future
 previous research on anchoring has shown this heuristic to be a very robust psychological phenomenon ubiquitous across many domains of human judgment and decision-making
 despite the prevalence of anchoring effects  researchers have only recently begun to investigate the underlying factors responsible for how and in what ways a person is susceptible to them
 this paper examines how one such factor  the big-five personality trait of openness-to-experience  influences the effect of previously presented anchors on participants' judgments
 our findings indicate that participants high in openness-to-experience were significantly more influenced by anchoring cues relative to participants low in this trait
 these findings were consistent across two different types of anchoring tasks providing convergent evidence for our hypothesis
 the anchoring effect  CITATION  refers to the adjustment of one's assessment  higher or lower  based upon previously presented external information or an  anchor
 the anchoring heuristic appears to be prevalent throughout human decision processes and has been shown to reliably influence judgments in a variety of domains including probability estimates  CITATION   negotiation  CITATION   legal judgments  CITATION   and general knowledge  CITATION
 further  anchoring effects appear viable across most situations for both novices and experts  CITATION  and seem to be effective under conditions of monetary incentives  CITATION  and in real-world settings  CITATION
 anchoring thus appears to be a very robust psychological phenomenon
 however  not all individuals may be equally influenced by anchoring cues
 identification of factors that influence how and in what ways a person is susceptible to this heuristic should further the understanding of the process
 one avenue of approach is to investigate the role of individual difference factors
 tversky and kahneman  CITATION  pointed to the important role of  personal characteristics  of the decision maker in risky choice situations
 later work by stanovich and west  CITATION  suggested that intellectual traits influence decision making and consequential choice preference
 recently  individual differences have been found in numerical reliance  CITATION   ambiguity  CITATION   preference for actions or inactions  CITATION  and the optimistic bias  CITATION
 the big-five personality traits  CITATION  have proven to be important individual difference factors for understanding decision choices
 further  attesting to the importance of individual differences  levin and hart  CITATION  demonstrated that individual differences in preference appear to originate at a very early age
 taken together  these findings suggest that the impact of individual difference factors on decision-making is both profound and pervasive
 the purpose of the current study is to investigate how one individual difference factor may influence the strength of the anchoring effect
 specifically  we are interested in how individual differences in the personality trait of openness-to-experience influences anchoring effects
 in the last couple of decades the five-factor model of personality has become the most widely tested and well-regarded personality trait model
 a great deal of research has supported this model's validity and reliability  CITATION
 while most research has agreed on the nature of the first four factors  the nature of the fifth factor has been controversial  a controversy predominately based upon whether a lexical approach  derived from language frequency within the lexicon of a particular language  CITATION   or a questionnaire approach  CITATION  should be used to measure it
 the fifth factor is often labeled openness-to-experience  which refers to a propensity to adjust beliefs and behaviors when exposed to new types of information or ideas  CITATION
 individuals scoring high on this dimension are more open to new ideas  CITATION  and motivated to seek variety and external experience
 individuals scoring low tend to be less inclined to consider alternative opinions and are more steadfast in their own beliefs  CITATION  making them more likely to rely upon information that is familiar and conventional  CITATION
 a fundamental aspect of the anchoring effect is that individuals are sensitive to information which they have experienced
 this change in judgment  which is based upon external cues  seems particularly relevant and related to the openness-to-experience personality trait
 specifically  as research has shown  the openness trait reflects individual propensities to  adjust  one's beliefs  CITATION  and to consider external information  CITATION
 therefore  based upon the nature of the openness-to-experience trait and the processes involved in the anchoring effect  we hypothesize that individual differences in openness-to-experience will influence susceptibility to anchoring effects
 specifically  we hypothesize that the judgments of those individuals high in this trait will be more influenced by previously presented anchors whereas those individuals low in this trait will be less influenced by the anchor
 to test this hypothesis  we first measured individual levels of the personality trait of openness-to-experience
 we then provided participants with an anchoring task involving either the mississippi river study  NUMBER  or african nations in the un study  NUMBER 
  we test in the context of a dictator game the proposition that individuals may experience a self-control conflict between the temptation to act selfishly and the better judgment to act pro-socially
  we manipulated the likelihood that individuals would identify self-control conflict, and we measured their trait ability to implement self-control strategies
  our analysis reveals a positive and significant correlation between trait self-control and pro-social behavior in the treatment where we expected a relatively high likelihood of conflict identification-but not in the treatment where we expected a low likelihood
  the magnitude of the effect is of economic significance
  we conclude that subtle cues might prove sufficient to alter individuals' perception of allocation opportunities, thereby prompting individuals to draw on their own cognitive resources to act pro-socially
  lured by temptation, individuals may find themselves acting against their better judgment
  self-control failure, famously termed akrasia in plato's protagoras  CITATION , persists throughout domains of daily life and represents a central issue of both philosophy and modern-day social sciences
  for example, the dieter faced with the opportunity to indulge in a delicious creamy cake may perceive a conflict between indulging and maintaining a good figure
  the student may feel conflicted between the desire to go to the cinema and her better judgment to stay home and study
  and, similarly, the fashionista might feel conflicted between the temptation to purchase new boots and her better judgment to maintain a responsible budget
  perhaps less intuitively, but no less importantly, the question of pro-social versus selfish behavior may be understood in similar terms
  this conceptualization may help reconcile conflicting notions in economics of selfish and pro-social motivations
  that individuals should care much about their own self-interest seems almost tautological and requires little further exposition, but that individuals also should care about the interest of others-at the expense of that of their own-has attracted considerable attention  CITATION
  for example, many individuals voluntarily contribute to charity or to public goods e g , recycling, and they pay their taxes despite low likelihood of punishment for failing to do so
  nonetheless, one could imagine that even individuals of generally pro-social inclination on occasion may feel tempted to act selfishly and hence underreport income to the authorities
  that is, pro-social preferences potentially fly in the face of basic urges for personal gain-or greed-and the individual may thus experience a self-control conflict between better judgment to act pro-socially and the temptation to act selfishly
  self-control-our capacity to overrule temptation-is no less complex than it is important
  a multitude of conceptualizations exist, many of which are complementary
  typically, and in line with classic ideas of the conflict between reason and passion, authors view self-control as a "cold" executive function that guides behavior in the face of "hot" impulses to act against better judgment  CITATION
  willpower, then, represents the combined resources that the executive function-or the planner, in the parlance of thaler and shefrin  CITATION -brings to bear in a deliberate struggle against temptation  CITATION
  such resources may include cognitive strategies to divert attention away from temptation  CITATION , strategies of pre-commitment  CITATION , or possibly the sheer strength of mind to hold back from the song of the sirens
  our conceptualization of self-control mirrors these
  only recently has the psychological literature started to explore how the question of pro-social versus selfish behavior relates to that of self-control
  loewenstein  CITATION  suggests that selfish behavior may be motivated by visceral urges or drive-states, resembling cravings for relief from hunger, pain, and sexual deprivation
  o'donoghue and loewenstein  CITATION  argue that such selfish urges may conflict with the "colder", more abstract preferences for altruism, as visceral urges for sweets may conflict with more abstract preferences for a fine figure or good health
  at present, there is but indirect evidence for this idea
  for example, pronin et al CITATION  show that decisions about others resemble decisions about "future selves", both classes of which contrast to decisions about less abstract "present selves"
  albrecht et al CITATION  report consistent results; individuals who choose between immediate and delayed rewards for themselves exhibit less patience and more affective involvement activation in the dopaminergic reward system than do individuals who make such choices for others-or for themselves in the future
  moreover, curry et al CITATION  find in a standard public goods game that individuals' discount rates are negatively associated with their contributions to the public good
  that is, more "impatient" individuals contributed less to the public good than did "patient" ones
  arriving at similar results, fehr and leibbrandt  CITATION  report that patient vs impatient fishermen, whose time preferences were elicited in the lab, exhibited more cooperative behavior in a common resource problem and were in the field less likely to over-exploit the common pool resource
  furthermore, burks et al CITATION  find that "short-term" patience-the  beta   in the  beta  -&#x3b4;  model-is positively associated with cooperative behavior in a sequential prisoner's dilemma
  however, duffy and smith  CITATION  report no effect of cognitive load-a manipulation intended to deplete cognitive resources and thereby impair self-control-on outcomes across treatments in a repeated multi-player prisoner's dilemma
  an emerging literature on the "default" response in games of trust and reciprocity lends further credence to the notion that altruistic responses require self-control
  achtziger et al CITATION  subjected players in an ultimatum game to cognitive resource depletion, and show that depleted proposers made lower offers-they became less altruistic
  moreover, depleted responders were more likely to reject offers that were unfair to themselves-they exhibited "altruistic punishment"
  halali et al CITATION  report the same for responders, but with a different depletion task
  crockett et al CITATION  subjected responders to acute tryptophan depletion-a procedure that temporarily reduces serotonin levels in the brain and thereby impairs self-control  CITATION ; reduced serotonin levels raised rejection rates and this reduction is positively correlated with impulsive choice in a delay-discounting task  CITATION
  using a trust game, knoch et al CITATION  subjected trustees' right lateral prefrontal cortex to transcranial magnetic stimulation, which reduces functioning in the targeted brain region
  trustees, though cognizant that returning a share of the investments was both strategic and norm-compliant, were unable to do so under impaired executive functioning; self-control seems necessary to act on the better judgment to resist the temptation to keep the received investment entirely for oneself
  closest to our domain of inquiry, piovesan and wengstrom  CITATION  measured response times of participants in a repeated dictator game, lasting 24 periods
  they find both across and within participants that lower response times are associated with more selfish choices
  one interpretation of their results is that the default behavior is to act selfishly and that pro-social behavior requires the successful resolution of a self-control conflict, which slows the response time
  such successful resolution of conflict would require cognitive resources, but hauge et al CITATION  report no effect of cognitive load on players in one-shot dictator games
  in this paper we attempt a more direct test of the hypothesis that pro-social versus selfish behavior may represent a self-control problem
  we employ a standard measure of pure pro-social behavior, the one-shot dictator game, which invokes neither concerns for strategy nor for reciprocity; and a well-grounded psychometric measure of self-control, the rosenbaum self-control schedule  CITATION
  further, we explore the conditions under which we expect an association between self-control and pro-social behavior
  in so doing, we rely on two conditions necessary for successfully exercising restraint in the face of temptation; myrseth and fishbach  CITATION  propose a two-stage model of self-control, which postulates that an individual in the face of temptation first identifies conflict or not between indulging and pursuing a higher-order goal and, second, that the individual next employs self-control strategies if and only if conflict was identified at the first stage see figure 1
  critically, self-control strategies are relevant to the decision to indulge only when the individual has identified self-control conflict
  therefore, one strategy for investigating whether the problem of pro-social versus selfish behavior resembles one of self-control is to test whether the tendency to apply self-control strategies is positively associated with pro-social behavior when individuals have identified self-control conflict, but less so or not at all when individuals have not
  determinants of conflict identification in the face of temptation have been explored only recently
  in some contexts, the question is almost trivial and identification of conflict virtually obvious
  for example, the diabetic dieter probably knows that having even a single, tempting chocolate may incur major costs
  however, the question of self-control conflict is more ambiguous for the non-diabetic dieter, who faces the same chocolate
  having this one chocolate alone will not incur major costs, but doing so regularly might
  similarly, the good citizen may find that a general failure to act generously would represent a major threat to his self-image, but being stingy on just a couple of occasions is a more ambiguous matter
  myrseth and fishbach  CITATION  use the term epsilon cost temptation to denote tempting opportunities that incur nothing but trivial costs when consumed in small amounts, but potentially serious costs when consumed extensively
  they argue that individuals identify self-control conflict in the face of epsilon cost temptation if and only if two conditions are met: a the focal consumption opportunity must be viewed in relation to multiple additional opportunities, and b the decision maker must assume that similar choices are made for each opportunity  CITATION
  that is, considering the question of whether or not to have a delicious creamy cake will evoke self-control conflict in the dieter if the dining opportunity is viewed in relation to future opportunities for dessert consumption, but not if the dining opportunity is viewed in isolation, as a singular episode
  similarly, the question of whether or not to be generous-to donate to a charitable organization-may elicit self-control conflict if the decision is viewed in relation to future decisions, but not if the decision is viewed in isolation
  if viewed in relation to future decisions, the question of how much to donate on a single occasion may have bearing on the decision maker's self-image; donating now-and in the future-indicates a generous character, whereas keeping the money for oneself does not
  however, if viewed in isolation, the question of how much to donate has little bearing on self-image; the present decision of how much to donate is considered only in light of immediate consequences, leaving self-image out of the equation
  because a consistent self-image represents an important motivator for pro-social behavior  CITATION , we expect that individuals are more likely to identify self-control conflict between selfish and pro-social behavior if the allocation decision is seen in relation to future opportunities than if it is seen in isolation
  myrseth and fishbach  CITATION  show that subtle framing manipulations are sufficient to influence identification of self-control conflict in the face of epsilon cost temptation
  they find that presenting a calendar displaying the current month, with a grid separating the dates, raised participants' subsequent consumption of potato chips relative to that of participants whom were presented a calendar without a grid
  they argue that the gridded calendar activated an isolated versus interrelated frame of the choice opportunity; it made participants more likely to isolate the date in question and thus less likely to see the decision task in relation to similar future opportunities
  consequently, the grid reduced the likelihood that participants would identify a conflict between the temptation to have chips and the better judgment to maintain a fine figure and good health
  indeed, participants who viewed the gridded calendar reported experiencing less conflict during their decision to have chips or not than did those who viewed the non-gridded calendar
  furthermore, participants' trait ability to implement self-control strategies, measured by rosenbaum's  CITATION  psychometric scale, was positively associated with chips consumption for those who viewed the calendar without the grid and who were more likely to identify conflict, but not for others who viewed the calendar with and who were less likely to identify conflict
  that is, participants who viewed the calendar without the grid, more likely than those who viewed the calendar with, identified self-control conflict and therefore leveraged their self-control strategies to resist the tempting chips
  to explore our hypothesis that the problem of pro-social versus selfish behavior may represent one of self-control, we have applied the empirical strategy from myrseth and fishbach  CITATION  in the dictator game-a participant is granted an endowment and asked to split it between herself and a recipient  CITATION , and in our case the red cross featured as recipient  CITATION
  the game thus pits pro-social motivations against self-interest
  if pro-social versus selfish behavior could represent a self-control conflict, we would expect participants' trait self-control, as measured by rosenbaum's  CITATION  scale, to correlate positively with pro-social behavior for participants who have just previously viewed a calendar without a grid, but less so or not at all for participants who have viewed a calendar with
  the graph in figure 2 displays donation, as a function of level of self-control, for two different levels of identification likelihood
  in the case of low likelihood, the slope is expected to be weakly positive
  in the case of the higher likelihood, however, the slope is expected to be strictly greater than that in the case of low likelihood
  this means that for a given level of self-control, one might observe substantially different donation behavior depending on whether conflict was identified or not
	Regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems
	Bundle methods for regularized risk minimization (BMRM) and the closely related SVMStruct are considered the best general purpose solvers to tackle this problem
	It was recently shown that BMRM requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution
	In the first part of the paper we use the Hadamard matrix to construct a regularized risk minimization problem and show that these rates cannot be improved
	We then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an  SYMBOL  accurate solution in  SYMBOL  iterations
	Let  SYMBOL  denote samples and  SYMBOL  be the corresponding labels
	Given a training set of  SYMBOL  sample label pairs  SYMBOL , drawn i i d from a joint probability distribution on  SYMBOL , many machine learning algorithms solve the following regularized risk minimization problem:     
	Here  SYMBOL  denotes the loss on instance  SYMBOL  using the current model  SYMBOL  and  SYMBOL , the empirical risk, is the average loss on the training set
	The regularizer  SYMBOL  acts as a penalty on the complexity of the classifier and prevents overfitting
	Usually the loss is convex in  SYMBOL  but can be nonsmooth while the regularizer is usually a smooth strongly convex function
	Binary Support Vector Machines (SVMs) are a prototypical example of such regularized risk minimization problems where  SYMBOL  and the loss considered is the binary hinge loss:    
	Recently, a number of solvers have been proposed for the regularized risk minimization problem
	The first and perhaps the best known solver is SVMStruct  CITATION , which was shown to converge in  SYMBOL  iterations to an  SYMBOL  accurate solution
	The convergence analysis of SVMStruct was improved to  SYMBOL  iterations by  CITATION
	In fact,  CITATION  showed that their convergence analysis holds for a more general solver than SVMStruct namely BMRM (Bundle method for regularized risk minimization)
	At every iteration BMRM replaces  SYMBOL  by a piecewise linear lower bound  SYMBOL  and optimizes    to obtain the next iterate  SYMBOL
	Here  SYMBOL  denotes an arbitrary subgradient of  SYMBOL  at  SYMBOL  (see Section ) and  SYMBOL
	The piecewise linear lower bound is successively tightened until the gap     falls below a predefined tolerance  SYMBOL
	Even though BMRM solves an expensive optimization problem at every iteration, the convergence analysis only uses a simple one-dimensional line search to bound the decrease in  SYMBOL
	Furthermore, the empirical convergence behavior of BMRM is much better than the theoretically predicted rates on a number of real life problems
	It was therefore conjectured that the rates of convergence of BMRM could be improved
	In this paper we answer this question in the negative by explicitly constructing a regularized risk minimization problem for which BMRM takes at least  SYMBOL  iterations
	One possible way to circumvent the  SYMBOL  lower bound is to solve the problem in the dual
	Using a very old result of Nesterov  CITATION  we obtain an algorithm for SVMs which only requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution; each iteration of the algorithm requires  SYMBOL  work
	Although we primarily focus on the regularized risk minimization with the binary hinge loss, our algorithm can also be used whenever the empirical risk is piecewise linear and contains a small number of pieces
	Examples of this include multiclass, multi-label, and ordinal regression hinge loss and other related losses### abstract ###
 similar to research on risky choice, the traditional analysis of intertemporal choice takes the view that an individual behaves so as to maximize the discounted sum of all future utilities
 the well-known allais paradox contradicts the fundamental postulates of maximizing the expected value or utility of a risky option
 we describe a violation of the law of diminishing marginal utility as well as an intertemporal version of the allais paradox
 in the field of intertemporal choice, the discounted-utility du theory proposed by paul samuelson in 1937 was presented not only as a valid normative standard but also as a descriptive theory of actual intertemporal choice behavior  CITATION
 in its general form, the du theory proposes that the value of an option, x; t, is the product of its present utility, ux, and an exponential temporal discounting function, ft, where t is the time at which x is acquired
 the overall value of a mixed option, a = {x, t, x, t, }, denoted va, is simply the sum of these products
 that is, va = sigma  ux ft
 an option a will be preferred to an option b if and only if va  greater than  vb
 however, a large body of empirical evidence demonstrates that people systematically violate this theory
 this includes the common difference effect, the magnitude effect, the gain-loss asymmetry, the delay-speedup asymmetry, and so on  CITATION
 this situation has led researchers to consider extensions and modifications of the du theory to reconcile it with the experimental data
 the most prominent idea to account for these anomalies is the hyperbolic discounting model  CITATION
 this model suggests that the discount rate is not dynamically consistent but that the rate is higher between the present and near future and lower between the near and far distant future
 numerous theories have been developed by transforming the discount function to other forms, from one-parameter hyperbolic discounting  CITATION  to generalized hyperbolic discounting  CITATION , to proportional discounting  CITATION , and to quasi-hyperbolic discounting  CITATION
 however, these models focus on intertemporal choice between pairs of single-dated outcomes represented as pure gains or losses
 when these models are applied to intertemporal choice between pairs of multiple-dated outcomes in mixed contexts, there is general agreement on the additive assumption and the independence assumption
 with an apt transformation of the discounting rate, the additive assumption means that preferences for outcome sequences are based on a simple aggregation of their individual components within intertemporal choice  CITATION
 the independence assumption means that the value or utility of an outcome in one period is independent of outcomes in other periods  CITATION
 because risk and delay might be psychologically equivalent, or at least analogous, and because similar psychological processes might underlie risk and intertemporal choice  CITATION , theoretical development in intertemporal choice has progressed steadily along a similar route as that of risky choice  CITATION
 both lines of research have spawned a large number of variant models
 although the functional forms differ, most theories assume a maximization principle; that is, people calculate the mathematical expectation of each outcome and add them together before choosing the option that maximizes overall value or utility
 a minor difference is that the existing models of intertemporal choice are relatively underdeveloped and are less flexible in dealing with empirical challenges
 for example, research on risky decision making does not treat risky choice as limited to pure gains or pure losses but has been extended to include mixed outcomes involving both gains and losses
 examples include the sign-dependent utility model  CITATION , the rank- and sign-dependent utility model  CITATION , and the transfer of attention exchange model  CITATION
 the well-known allais paradox  CITATION  contradicts the fundamental postulates of maximizing the expected utility of a risky option
 the paradox presents a violation of the cancellation axiom, which asserts that, if two options have a common consequence under a particular event, the preference order of the options should be independent of the value of that consequence  CITATION
 since then, many new descriptive theories of risky choice have abandoned the maximization assumption  CITATION
 most models of intertemporal choice have not yet abandoned the additive assumption and the independence assumption
 these two assumptions would lead to the cancellation axiom, which indicates that a preference between two sequences with elements in common does not depend on the nature of the common elements
 table 1 illustrates an example of the multiple-dated outcomes problem, which would be used to test the cancellation axiom
 in problem i, the additive models predict that adding a common element x at time 2 to both option a and option b would not change the preference orderings
 the violation of cancellation would be observed if the preference orderings were different between problem i and problem i
 however, if allais's proposition applies to intertemporal choice, we will eventually encounter an intertemporal version of the allais paradox
 we first illustrate our point with a paradox that is an intertemporal-type violation of the cancellation axiom
	We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors
	To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent
	We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis
	Factor analysis is the task of explaining data by means of a set of  latent factors
	Factor  regression  couples this analysis with a prediction task, where the predictions are made solely on the basis of the factor representation
	The latent factor representation achieves two-fold benefits: (1) discovering the latent  process  underlying the data; (2) simpler predictive modeling through a compact data representation
	In particular, (2) is motivated by the problem of prediction in the  ``large P small N''  paradigm  CITATION , where the number of features  SYMBOL  greatly exceeds the number of examples  SYMBOL , potentially resulting in overfitting
	We address three fundamental shortcomings of standard factor analysis approaches  CITATION : (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis
	Our motivation for this work stems from the task of reconstructing regulatory structure from gene-expression data
	In this context, factors correspond to regulatory pathways
	Our contributions thus parallel the needs of gene pathway modeling
	In addition, we couple predictive modeling (for factor regression) within the factor analysis framework itself, instead of having to model it separately
	Our factor regression model is fundamentally nonparametric
	In particular, we treat the gene-to-factor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process (IBP)  CITATION , designed to account for the sparsity of relevant genes (features)
	We  couple  this IBP with a hierarchical prior over the factors
	This prior explains the fact that pathways are fundamentally related: some are involved in transcription, some in signaling, some in synthesis
	The nonparametric nature of our sparse IBP requires that the hierarchical prior  also  be nonparametric
	A natural choice is Kingman's coalescent  CITATION , a popular distribution over infinite binary trees
	Since our motivation is an application in bioinformatics, our notation and terminology will be drawn from that area
	In particular,  genes  are  features ,  samples  are  examples , and  pathways  are  factors
	However, our model is more general
	An alternative application might be to a collaborative filtering problem, in which case our genes might correspond to movies, our samples might correspond to users and our pathways might correspond to genres
	In this context, all three contributions of our model still make sense: we do not know how many movie genres there are; some genres are closely related (romance to comedy versus to action); many movies may be spurious### abstract ###
	similar to research on risky choice, the traditional analysis of intertemporal choice takes the view that an individual behaves so as to maximize the discounted sum of all future utilities
	the well-known allais paradox contradicts the fundamental postulates of maximizing the expected value or utility of a risky option
	we describe a violation of the law of diminishing marginal utility as well as an intertemporal version of the allais paradox
	in the field of intertemporal choice, the discounted-utility du theory proposed by paul samuelson in 1937 was presented not only as a valid normative standard but also as a descriptive theory of actual intertemporal choice behavior  CITATION
	in its general form, the du theory proposes that the value of an option, x; t, is the product of its present utility, ux, and an exponential temporal discounting function, ft, where t is the time at which x is acquired
	the overall value of a mixed option, a = {x, t, x, t, }, denoted va, is simply the sum of these products
	that is, va = sigma  ux ft
	an option a will be preferred to an option b if and only if va  greater than  vb
	however, a large body of empirical evidence demonstrates that people systematically violate this theory
	this includes the common difference effect, the magnitude effect, the gain-loss asymmetry, the delay-speedup asymmetry, and so on  CITATION
	this situation has led researchers to consider extensions and modifications of the du theory to reconcile it with the experimental data
	the most prominent idea to account for these anomalies is the hyperbolic discounting model  CITATION
	this model suggests that the discount rate is not dynamically consistent but that the rate is higher between the present and near future and lower between the near and far distant future
	numerous theories have been developed by transforming the discount function to other forms, from one-parameter hyperbolic discounting  CITATION  to generalized hyperbolic discounting  CITATION , to proportional discounting  CITATION , and to quasi-hyperbolic discounting  CITATION
	however, these models focus on intertemporal choice between pairs of single-dated outcomes represented as pure gains or losses
	when these models are applied to intertemporal choice between pairs of multiple-dated outcomes in mixed contexts, there is general agreement on the additive assumption and the independence assumption
	with an apt transformation of the discounting rate, the additive assumption means that preferences for outcome sequences are based on a simple aggregation of their individual components within intertemporal choice  CITATION
	the independence assumption means that the value or utility of an outcome in one period is independent of outcomes in other periods  CITATION
	because risk and delay might be psychologically equivalent, or at least analogous, and because similar psychological processes might underlie risk and intertemporal choice  CITATION , theoretical development in intertemporal choice has progressed steadily along a similar route as that of risky choice  CITATION
	both lines of research have spawned a large number of variant models
	although the functional forms differ, most theories assume a maximization principle; that is, people calculate the mathematical expectation of each outcome and add them together before choosing the option that maximizes overall value or utility
	a minor difference is that the existing models of intertemporal choice are relatively underdeveloped and are less flexible in dealing with empirical challenges
	for example, research on risky decision making does not treat risky choice as limited to pure gains or pure losses but has been extended to include mixed outcomes involving both gains and losses
	examples include the sign-dependent utility model  CITATION , the rank- and sign-dependent utility model  CITATION , and the transfer of attention exchange model  CITATION
	the well-known allais paradox  CITATION  contradicts the fundamental postulates of maximizing the expected utility of a risky option
	the paradox presents a violation of the cancellation axiom, which asserts that, if two options have a common consequence under a particular event, the preference order of the options should be independent of the value of that consequence  CITATION
	since then, many new descriptive theories of risky choice have abandoned the maximization assumption  CITATION
	most models of intertemporal choice have not yet abandoned the additive assumption and the independence assumption
	these two assumptions would lead to the cancellation axiom, which indicates that a preference between two sequences with elements in common does not depend on the nature of the common elements
	table 1 illustrates an example of the multiple-dated outcomes problem, which would be used to test the cancellation axiom
	in problem i, the additive models predict that adding a common element x at time 2 to both option a and option b would not change the preference orderings
	the violation of cancellation would be observed if the preference orderings were different between problem i and problem i
	however, if allais's proposition applies to intertemporal choice, we will eventually encounter an intertemporal version of the allais paradox
	we first illustrate our point with a paradox that is an intertemporal-type violation of the cancellation axiom
	In the constraint satisfaction problem ( SYMBOL ), the aim is to find an assignment of values to a set of variables subject to specified constraints
	In the minimum cost homomorphism problem ( SYMBOL ), one is additionally given weights  SYMBOL  for every variable  SYMBOL  and value  SYMBOL , and the aim is to find an assignment  SYMBOL  to the variables that minimizes  SYMBOL
	Let  SYMBOL  denote the  SYMBOL  problem parameterized by the set of predicates allowed for constraints
	SYMBOL  is related to many well-studied combinatorial optimization problems, and concrete applications can be found in, for instance, defence logistics and machine learning
	We show that  SYMBOL  can be studied by using algebraic methods similar to those used for CSPs
	With the aid of algebraic techniques, we classify the computational complexity of  SYMBOL  for all choices of  SYMBOL
	Our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs CITATION
	Constraint satisfaction problems ( SYMBOL ) are a natural way of formalizing a large number of computational problems arising in combinatorial optimization, artificial intelligence, and database theory
	This problem has the following two equivalent formulations: (1) to find an assignment of values to a given set of variables, subject to constraints on the values that can be assigned simultaneously to specified subsets of variables, and (2) to find a homomorphism between two finite relational structures  SYMBOL  and  SYMBOL
	Applications of  SYMBOL s arise in the propositional logic, database and graph theory, scheduling and many other areas
	During the past 30 years,  SYMBOL  and its subproblems has been intensively studied by computer scientists and mathematicians
	Considerable attention has been given to the case where the constraints are restricted to a given finite set of relations  SYMBOL , called a constraint language  CITATION
	For example, when  SYMBOL  is a constraint language over the boolean set  SYMBOL  with four ternary predicates  SYMBOL ,  SYMBOL ,  SYMBOL ,  SYMBOL  we obtain 3-SAT
	This direction of research has been mainly concerned with the computational complexity of  SYMBOL  as a function of  SYMBOL
	It has been shown that the complexity of  SYMBOL  is highly connected with relational clones of universal algebra  CITATION
	For every constraint language  SYMBOL , it has been conjectured that  SYMBOL  is either in P or NP-complete  CITATION
	In the minimum cost homomorphism problem ( SYMBOL ), we are given variables subject to constraints and, additionally, costs on variable/value pairs
	Now, the task is not just to find any satisfying assignment to the variables, but one that minimizes the total cost
	SYMBOL  was introduced in  CITATION  where it was motivated by a real-world problem in defence logistics
	The question for which directed graphs  SYMBOL  the problem  SYMBOL  is polynomial-time solvable was considered in  CITATION
	In this paper, we approach the problem in its most general form by algebraic methods and give a complete algebraic characterization of tractable constraint languages
	From this characterization, we obtain a dichotomy for  SYMBOL , i e , if  SYMBOL  is not polynomial-time solvable, then it is NP-hard
	Of course, this dichotomy implies the dichotomy for directed graphs
	In Section 2, we present some preliminaries together with results connecting the complexity of  SYMBOL  with conservative algebras
	The main dichotomy theorem is stated in Section 3 and its proof is divided into several parts which can be found in Sections 4-8
	The NP-hardness results are collected in Section 4 followed by the building blocks for the tractability result: existence of majority polymorphisms (Section 5) and connections with optimization in perfect graphs (Section 6)
	Section 7 introduces the concept of  arithmetical deadlocks  which lay the foundation for the final proof in Section 8
	In Section 9 we reformulate our main result in terms of relational clones
	Finally, in Section 10 we explain the relation of our results to previous research and present directions for future research
	we introduce a new principle for model selection in regression and classification
	many regression models are controlled by some smoothness or flexibility or complexity parameter    e g   the number of neighbors to be averaged over in k nearest neighbor  knn  regression or the polynomial degree in regression with polynomials
	let   be the  best  regressor of complexity   on data
	a more flexible regressor can fit more data   well than a more rigid one
	if something  here small loss  is easy to achieve it s typically worth less
	we define the loss rank of   as the number of other  fictitious  data   that are fitted better by   than   is fitted by
	we suggest selecting the model complexity   that has minimal loss rank  lorp 
	unlike most penalized maximum likelihood variants  aic bic mdl   lorp only depends on the regression functions and the loss function
	it works without a stochastic noise model  and is directly applicable to any non parametric regressor  like knn
	in this paper we formalize  discuss  and motivate lorp  study it for specific regression problems  in particular linear ones  and compare it to other model selection schemes
	consider a regression or classification problem in which we want to determine the functional relationship   from data    i e   we seek a function   such that   is close to the unknown   for all
	one may define regressor   directly  e g    average the   values of the   nearest neighbors  knn  of   in     or select the   from a class of functions   that has smallest  training  error on
	if the class   is not too large  e g the polynomials of fixed reasonable degree    this often works well
	what remains is to select the right model complexity    like   or
	this selection cannot be based on the training error  since the more complex the model  large    small    the better the fit on    perfect for   and   
	this problem is called overfitting  for which various remedies have been suggested   we will not discuss empirical test set methods like cross validation  but only training set based methods
	see e g    CITATION  for a comparison of cross validation with bayesian model selection
	training set based model selection methods allow using all data   for regression
	the most popular ones can be regarded as penalized versions of maximum likelihood  ml 
	in addition to the function class    one has to specify a sampling model    e g   that the   have independent gaussian distribution with mean
	ml chooses    penalized ml  pml  then chooses  penalty   where the penalty depends on the used approach  mdl  CITATION   bic  CITATION   aic  CITATION  
	in particular  modern mdl  CITATION  has sound exact foundations and works very well in practice
	all pml variants rely on a proper sampling model  which may be difficult to establish   ignore  or at least do not tell how to incorporate  a potentially given loss function  and are typically limited to  semi parametric models
	the main goal of the paper is to establish a criterion for selecting the   best   model complexity   % based on regressors   given as a black box without insight into the origin or inner structure of    % that does not depend on things often not given  like a stochastic noise model   % and that exploits what is given  like the loss function 
	the key observation we exploit is that large classes   or more flexible regressors   can fit more data   well than more rigid ones  e g   many   can be fit well with high order polynomials
	we define the  loss rank  of   as the number of other  fictitious  data   that are fitted better by   than   is fitted by    as measured by some loss function
	the loss rank is large for regressors fitting   not well  and  for too flexible regressors  in both cases the regressor fits many other   better 
	the loss rank has a minimum for not too flexible regressors which fit   not too bad
	we claim that minimizing the loss rank is a suitable model selection criterion  since it trades off the quality of fit with the flexibility of the model
	unlike pml  our new loss rank principle  lorp  works without a noise  stochastic sampling  model  and is directly applicable to any non parametric regressor  like knn
	in section   after giving a brief introduction to regression  we formally state lorp for model selection
	to make it applicable to real problems  we have to generalize it to continuous spaces and regularize infinite loss ranks
	in section  we derive explicit expressions for the loss rank for the important class of linear regressors  which includes knn  polynomial  linear basis function  lbfr   kernel  and projective regression
	in section  we compare linear lorp to bayesian model selection for linear regression with gaussian noise and prior  and in section  to pml  in particular mdl  bic  aic  and mackay s  CITATION  and hastie s et al    CITATION  trace formulas for the effective dimension
	in this paper we just scratch at the surface of lorp
	section  contains further considerations  to be elaborated on in the future
	third-party punishment has recently received attention as an explanation for human altruism
	feelings of anger in response to norm violations are assumed to motivate third-party sanctions, yet there is only sparse and indirect support for this idea
	we investigated the impact of both anger and guilt feelings on third-party sanctions
	in two studies both emotions were independently manipulated
	results show that anger and guilt independently constitute sufficient but not necessary causes of punishment
	low levels of punishment are observed only when neither emotion is elicited
	we discuss the implications of these findings for the functions of altruistic sanctions
	people often defend the interests of others
	they stand up for their friends if someone speaks ill about them in their absence
	they do not tolerate a colleague being bullied at work
	they boycott consumer products that are produced using child labor
	some even come to the aid of a stranger who is being physically harassed, in spite of obvious personal danger
	in general, people retaliate against injustice even if they are not directly victimized
	sanctioning of norm-violations is vital for prosocial behavior to be sustained  CITATION
	however, punishing norm-violations is costly in terms of time and energy
	it may even impose physical risks
	punishing injustice is therefore considered to be a moral act, particularly when it is performed on behalf of others  CITATION
	this begs the question of what incites third-party sanctions, as they usually oppose self-interest